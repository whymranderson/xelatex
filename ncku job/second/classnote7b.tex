
\documentclass{article}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{accents}
\usepackage[ignoreall,a4paper]{geometry}
\usepackage{fancyhdr}

\setcounter{MaxMatrixCols}{10}
%TCIDATA{OutputFilter=LATEX.DLL}
%TCIDATA{Version=5.00.0.2606}
%TCIDATA{<META NAME="SaveForMode" CONTENT="1">}
%TCIDATA{BibliographyScheme=Manual}
%TCIDATA{Created=Wednesday, November 25, 2015 15:33:37}
%TCIDATA{LastRevised=Wednesday, June 22, 2016 16:26:10}
%TCIDATA{<META NAME="GraphicsSave" CONTENT="32">}
%TCIDATA{<META NAME="DocumentShell" CONTENT="Standard LaTeX\Blank - Standard LaTeX Article">}
%TCIDATA{CSTFile=40 LaTeX article.cst}
%TCIDATA{ComputeDefs=
%$W=\left( 1-\sigma \right) I$
%}


\newtheorem{theorem}{Theorem}
\newtheorem{acknowledgement}[theorem]{Acknowledgement}
\newtheorem{algorithm}[theorem]{Algorithm}
\newtheorem{axiom}[theorem]{Axiom}
\newtheorem{case}[theorem]{Case}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{conclusion}[theorem]{Conclusion}
\newtheorem{condition}[theorem]{Condition}
\newtheorem{conjecture}[theorem]{Conjecture}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{criterion}[theorem]{Criterion}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}
\newtheorem{exercise}[theorem]{Exercise}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{notation}[theorem]{Notation}
\newtheorem{problem}[theorem]{Problem}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{solution}[theorem]{Solution}
\newtheorem{summary}[theorem]{Summary}
\newenvironment{proof}[1][Proof]{\noindent\textbf{#1.} }{\ \rule{0.5em}{0.5em}}
\input{../../tcilatex}
\DeclareMathAccent{\wtilde}{\mathord}{largesymbols}{"65}
\pagestyle{fancy}
\fancyfoot[C]{\thepage}


\begin{document}


\section{Non-full-Rank Models}

\begin{enumerate}
\item linear combination of the parameters

\item reparameterization

\item $^{\bigstar }$constraints
\end{enumerate}

\bigskip

\begin{example}
\begin{enumerate}
\item One\_Way model%
\begin{equation*}
y_{ij}=\mu +\alpha _{i}+\varepsilon _{ij}\quad i=1,2\quad j=1,2,3
\end{equation*}%
\begin{equation*}
\left[ 
\begin{array}{c}
y_{11} \\ 
y_{12} \\ 
y_{13} \\ 
y_{21} \\ 
y_{22} \\ 
y_{23}%
\end{array}%
\right] =\left[ 
\begin{array}{ccc}
1 & 1 & 0 \\ 
1 & 1 & 0 \\ 
1 & 1 & 0 \\ 
1 & 0 & 1 \\ 
1 & 0 & 1 \\ 
1 & 0 & 1%
\end{array}%
\right] \left[ 
\begin{array}{c}
\mu \\ 
\alpha _{1} \\ 
\alpha _{2}%
\end{array}%
\right] +\left[ 
\begin{array}{c}
\epsilon _{11} \\ 
\epsilon _{12} \\ 
\epsilon _{13} \\ 
\epsilon _{21} \\ 
\epsilon _{22} \\ 
\epsilon _{23}%
\end{array}%
\right]
\end{equation*}%
\newline
\newline
\begin{equation*}
\mathbf{Y=}\underset{6\times 3}{\mathbf{X}}\mathbf{\beta +\epsilon },\quad
r\left( \mathbf{X}\right) =2<3
\end{equation*}

\item Two\_Way model%
\begin{equation*}
y_{ij}=\mu +\alpha _{i}+\beta _{j}+\varepsilon _{ij}\quad 
\begin{tabular}{l}
$i=1,2$ \\ 
$j=1,2$%
\end{tabular}%
\end{equation*}%
\begin{equation*}
\left[ 
\begin{array}{c}
y_{11} \\ 
y_{12} \\ 
y_{21} \\ 
y_{22}%
\end{array}%
\right] =\left[ 
\begin{array}{ccccc}
1 & 1 & 0 & 1 & 0 \\ 
1 & 1 & 0 & 0 & 1 \\ 
1 & 0 & 1 & 1 & 0 \\ 
1 & 0 & 1 & 0 & 1%
\end{array}%
\right] \left[ 
\begin{array}{c}
\mu \\ 
\alpha _{1} \\ 
\alpha _{2} \\ 
\beta _{1} \\ 
\beta _{2}%
\end{array}%
\right] +\left[ 
\begin{array}{c}
\varepsilon _{11} \\ 
\varepsilon _{12} \\ 
\varepsilon _{21} \\ 
\varepsilon _{22}%
\end{array}%
\right]
\end{equation*}%
\newline
\newline
\begin{equation*}
\mathbf{Y=}\underset{4\times 5}{\mathbf{X}}\mathbf{\beta +\epsilon },\quad
r\left( \mathbf{X}\right) =3<5
\end{equation*}
\end{enumerate}
\end{example}

\bigskip

Consider the model%
\begin{equation*}
\mathbf{Y=X\beta +\epsilon }
\end{equation*}%
where $E\left( \mathbf{\epsilon }\right) =\mathbf{0}$, and $Var\left( 
\mathbf{\epsilon }\right) =\sigma ^{2}I_{n}$ and $\mathbf{X}$ is a $n\times
p $ matrix with $r\left( \mathbf{X}\right) <p\leq n$.

Then the least square estimator of $\mathbf{\beta }$ is%
\begin{equation*}
\mathbf{\hat{\beta}}=\left( \mathbf{X}^{\dagger }\mathbf{X}\right) ^{-}%
\mathbf{X}^{\dagger }\mathbf{Y\quad }\text{(it's not square)}
\end{equation*}%
\begin{eqnarray*}
E\left( \mathbf{\hat{\beta}}\right) &=&E\left[ \left( \mathbf{X}^{\dagger }%
\mathbf{X}\right) ^{-}\mathbf{X}^{\dagger }\mathbf{Y}\right] =\left( \mathbf{%
X}^{\dagger }\mathbf{X}\right) ^{-}\mathbf{X}^{\dagger }E\left( \mathbf{Y}%
\right) \\
&=&\left( \mathbf{X}^{\dagger }\mathbf{X}\right) ^{-}\mathbf{X}^{\dagger }%
\mathbf{X\beta \neq \beta }
\end{eqnarray*}%
\begin{eqnarray*}
Var\left( \mathbf{\hat{\beta}}\right) &=&Var\left( \left( \mathbf{X}%
^{\dagger }\mathbf{X}\right) ^{-}\mathbf{X}^{\dagger }\mathbf{Y}\right)
=\left( \mathbf{X}^{\dagger }\mathbf{X}\right) ^{-}\mathbf{X}^{\dagger
}Var\left( \mathbf{Y}\right) \left[ \left( \mathbf{X}^{\dagger }\mathbf{X}%
\right) ^{-}\mathbf{X}^{\dagger }\right] \\
&=&\sigma ^{2}\left( \mathbf{X}^{\dagger }\mathbf{X}\right) ^{-}\mathbf{X}%
^{\dagger }\mathbf{X}\left( \mathbf{X}^{\dagger }\mathbf{X}\right) ^{-} \\
&&\text{(it's not invariant to the choice of }\left( \mathbf{X}^{\dagger }%
\mathbf{X}\right) ^{-}\text{)}
\end{eqnarray*}

\bigskip

\begin{theorem}
In the model $\mathbf{Y=X\beta +\epsilon }$, where $E\left( \mathbf{\epsilon 
}\right) =\mathbf{0}$\quad $Var\left( \mathbf{\epsilon }\right) =\sigma
^{2}I $ and $\mathbf{X}$ is $n\times p$ matrix with $r\left( \mathbf{X}%
\right) =r<p\leq n$, the $\mathbf{\lambda }^{\dagger }\mathbf{\beta }$ is
estimable iff one of the following conditions holds:

\begin{enumerate}
\item $\mathbf{\lambda }^{\dagger }\in R\left( \mathbf{X}\right) \quad
\left( \exists \ \mathbf{a}\quad \text{s.t.}\quad \mathbf{a}^{\dagger }%
\mathbf{X=\lambda }^{\dagger }\right) $

\item $\mathbf{\lambda }^{\dagger }\in R\left( \mathbf{X^{\dagger }X}\right)
\quad \left( \exists \ \mathbf{r}\quad \text{s.t.}\quad \mathbf{\gamma
^{\dagger }X}^{\dagger }\mathbf{X=\lambda }^{\dagger }\right) $

\item $\left( \mathbf{X^{\dagger }X}\right) \left( \mathbf{X^{\dagger }X}%
\right) ^{-}\mathbf{\lambda =\lambda }$ or $\mathbf{\lambda }^{\dagger
}\left( \mathbf{X^{\dagger }X}\right) ^{-}\mathbf{X^{\dagger }X=\lambda }%
^{\dagger }$
\end{enumerate}

\begin{proof}
\begin{enumerate}
\item[2.] let $\mathbf{\gamma ^{\dagger }X}^{\dagger }\mathbf{X=\lambda }%
\quad \left( \mathbf{a=X\gamma }\right) $\newline
$E\left( \mathbf{a}^{\dagger }\mathbf{Y}\right) =E\left( \mathbf{\gamma
^{\dagger }X}^{\dagger }\mathbf{Y}\right) =\mathbf{\gamma ^{\dagger }X}%
^{\dagger }\mathbf{X\beta =\lambda }^{\dagger }\mathbf{\beta }$\newline
\fbox{$E\left( \mathbf{a}^{\dagger }\mathbf{Y}\right) =\mathbf{\lambda }%
^{\dagger }\mathbf{\beta \Rightarrow a}^{\dagger }\mathbf{X\beta =\lambda }%
^{\dagger }\mathbf{\beta \Rightarrow a}^{\dagger }\mathbf{X=\lambda }%
^{\dagger }$}

\item[3.] If $\mathbf{X}^{\dagger }\mathbf{X}\left( \mathbf{X}^{\dagger }%
\mathbf{X}\right) ^{-}\mathbf{\lambda =\lambda }$\newline
then $\left( \mathbf{X}^{\dagger }\mathbf{X}\right) ^{-}\mathbf{\lambda }$
is a solution to $\mathbf{X}^{\dagger }\mathbf{X\gamma =\lambda }$
\end{enumerate}
\end{proof}
\end{theorem}

\begin{example}
\begin{equation*}
y_{ij}=\mu +\alpha _{i}+\varepsilon _{ij}\quad 
\begin{tabular}{l}
$i=1,2$ \\ 
$j=1,2,3$%
\end{tabular}%
\end{equation*}%
\begin{equation*}
\mathbf{X}=\left[ 
\begin{array}{ccc}
1 & 1 & 0 \\ 
1 & 1 & 0 \\ 
1 & 1 & 0 \\ 
1 & 0 & 1 \\ 
1 & 0 & 1 \\ 
1 & 0 & 1%
\end{array}%
\right] \quad ,\quad \mathbf{\beta }=\left[ 
\begin{array}{c}
\mu \\ 
\alpha _{1} \\ 
\alpha _{2}%
\end{array}%
\right]
\end{equation*}%
\newline
\newline
let $\mathbf{\lambda }^{\dagger }=\left[ 
\begin{array}{ccc}
0 & 1 & -1%
\end{array}%
\right] $

\begin{enumerate}
\item $\mathbf{\lambda }^{\dagger }\in R\left( \mathbf{X}\right) \quad $%
(i.e. $\mathbf{\lambda }^{\dagger }=\left[ \quad \right] -\left[ \quad %
\right] $)

\item $\mathbf{X}^{\dagger }\mathbf{X=}\left[ 
\begin{array}{ccc}
6 & 3 & 3 \\ 
3 & 3 & 0 \\ 
3 & 0 & 3%
\end{array}%
\right] $\newline
\newline
if take $\left( \mathbf{X}^{\dagger }\mathbf{X}\right) ^{-}=\left[ 
\begin{array}{ccc}
0 & 0 & 0 \\ 
0 & \frac{1}{3} & 0 \\ 
0 & 0 & \frac{1}{3}%
\end{array}%
\right] $\newline
\newline
$\mathbf{\gamma =}\left( \mathbf{X}^{\dagger }\mathbf{X}\right) ^{-}\mathbf{%
\lambda =}\left[ 
\begin{array}{ccc}
0 & 0 & 0 \\ 
0 & \frac{1}{3} & 0 \\ 
0 & 0 & \frac{1}{3}%
\end{array}%
\right] \left[ 
\begin{array}{c}
0 \\ 
1 \\ 
-1%
\end{array}%
\right] =\left[ 
\begin{array}{c}
0 \\ 
\frac{1}{3} \\ 
-\frac{1}{3}%
\end{array}%
\right] $\newline
\newline

\item $\left( \mathbf{X}^{\dagger }\mathbf{X}\right) \left( \mathbf{X}%
^{\dagger }\mathbf{X}\right) ^{-}=\left[ 
\begin{array}{ccc}
0 & 1 & 1 \\ 
0 & 1 & 0 \\ 
0 & 0 & 1%
\end{array}%
\right] $\newline
\newline
$\left( \mathbf{X}^{\dagger }\mathbf{X}\right) \left( \mathbf{X}^{\dagger }%
\mathbf{X}\right) ^{-}\mathbf{\lambda =}\left[ 
\begin{array}{ccc}
0 & 1 & 1 \\ 
0 & 1 & 0 \\ 
1 & 0 & 1%
\end{array}%
\right] \left[ 
\begin{array}{c}
0 \\ 
1 \\ 
-1%
\end{array}%
\right] =\left[ 
\begin{array}{c}
0 \\ 
1 \\ 
-1%
\end{array}%
\right] $
\end{enumerate}
\end{example}

\bigskip

\begin{theorem}
In the non-full-rank model $\mathbf{Y=X\beta +\epsilon }$, the number of
linearly independent estimable functions of $\mathbf{\beta }$ is the rank of 
$\mathbf{X}$.
\end{theorem}

\bigskip

note:

\begin{enumerate}
\item each row of $\mathbf{X\beta }$ is estimable

\item each row of $\mathbf{X}^{\dagger }\mathbf{X\beta }$ is estimable
\end{enumerate}

\bigskip

\begin{theorem}
Let $\mathbf{\lambda }^{\dagger }\mathbf{\beta }$ be an estimable function
of $\mathbf{\beta }$ in the model $\mathbf{Y=X\beta +\epsilon }$, $E\left( 
\mathbf{\epsilon }\right) =\mathbf{0}$, $Var\left( \mathbf{\epsilon }\right)
=\sigma ^{2}I$ and $\mathbf{X}_{n\times p}$ with $r\left( \mathbf{X}\right)
=r<p\leq n$.
\end{theorem}

\bigskip

Let $\mathbf{\hat{\beta}}$ be any solution to the normal equations $\mathbf{X%
}^{\dagger }\mathbf{X\hat{\beta}=X}^{\dagger }\mathbf{Y}$, and let $\mathbf{%
\gamma }$ be any solution to $\mathbf{X}^{\dagger }\mathbf{X\gamma =\lambda }
$, then

\begin{enumerate}
\item $E\left( \mathbf{\lambda }^{\dagger }\mathbf{\hat{\beta}}\right)
=E\left( \mathbf{\gamma }^{\dagger }\mathbf{X}^{\dagger }\mathbf{Y}\right) =%
\mathbf{\lambda }^{\dagger }\mathbf{\beta }$

\item $\mathbf{\lambda }^{\dagger }\mathbf{\hat{\beta}}$ is equal to $%
\mathbf{\gamma }^{\dagger }\mathbf{X}^{\dagger }\mathbf{Y\quad \forall \ 
\hat{\beta}}$ or $\mathbf{\gamma }$

\item $\mathbf{\lambda }^{\dagger }\mathbf{\hat{\beta}}$ and $\mathbf{\gamma 
}^{\dagger }\mathbf{X}^{\dagger }\mathbf{Y}$ are invariant to the choice of $%
\mathbf{\hat{\beta}}$ or $\mathbf{\gamma }$

\item $Var\left( \mathbf{\lambda }^{\dagger }\mathbf{\hat{\beta}}\right)
=\sigma ^{2}\mathbf{\lambda }^{\dagger }\left( \mathbf{X}^{\dagger }\mathbf{X%
}\right) ^{-}\mathbf{\lambda }$\newline
$Var\left( \mathbf{\gamma }^{\dagger }\mathbf{X}^{\dagger }\mathbf{Y}\right)
=\sigma ^{2}\mathbf{\gamma }^{\dagger }\mathbf{X}^{\dagger }\mathbf{X\gamma =%
}\sigma ^{2}\mathbf{\gamma \lambda }$

\item $^{\bigstar \bigstar }Var\left( \mathbf{\lambda }^{\dagger }\mathbf{%
\hat{\beta}}\right) $ is unique (invariant to the choice of $\mathbf{\gamma }
$ or $\left( \mathbf{X}^{\dagger }\mathbf{X}\right) ^{-}$)
\end{enumerate}

\begin{proof}
\begin{enumerate}
\item 
\begin{eqnarray*}
E\left( \mathbf{\lambda }^{\dagger }\mathbf{\hat{\beta}}\right) &=&E\left( 
\mathbf{\lambda }^{\dagger }\left( \mathbf{X}^{\dagger }\mathbf{X}\right)
^{-}\mathbf{X}^{\dagger }\mathbf{Y}\right) \\
&=&\mathbf{\lambda }^{\dagger }\left( \mathbf{X}^{\dagger }\mathbf{X}\right)
^{-}\mathbf{X}^{\dagger }\mathbf{X\beta \quad }\text{(i.e. }\mathbf{\lambda }%
^{\dagger }\left( \mathbf{X}^{\dagger }\mathbf{X}\right) ^{-}\mathbf{X}%
^{\dagger }\mathbf{X=\lambda }^{\dagger }\text{)} \\
&=&\mathbf{\lambda }^{\dagger }\mathbf{\beta }
\end{eqnarray*}%
\begin{eqnarray*}
E\left( \mathbf{\gamma }^{\dagger }\mathbf{X}^{\dagger }\mathbf{Y}\right) &=&%
\mathbf{\gamma }^{\dagger }\mathbf{X}^{\dagger }\mathbf{X\beta =}\left( 
\mathbf{X}^{\dagger }\mathbf{X\gamma }\right) ^{\dagger }\mathbf{\beta } \\
&=&\mathbf{\lambda }^{\dagger }\mathbf{\beta \quad }\text{(}\mathbf{X}%
^{\dagger }\mathbf{X\gamma =\lambda }\text{)}
\end{eqnarray*}

\item[3.] let $\mathbf{\gamma }_{1}$ and $\mathbf{\gamma }_{2}$ such that $%
\mathbf{X}^{\dagger }\mathbf{X\gamma }_{1}=\mathbf{X}^{\dagger }\mathbf{%
X\gamma }_{2}=\mathbf{\lambda }$\newline
$\Rightarrow \mathbf{\gamma }_{1}^{\dagger }\mathbf{X}^{\dagger }\mathbf{X%
\hat{\beta}}=\mathbf{\gamma }_{1}^{\dagger }\mathbf{X^{\dagger }Y}$ and $%
\mathbf{\gamma }_{2}^{\dagger }\mathbf{X}^{\dagger }\mathbf{X\hat{\beta}}=%
\mathbf{\gamma }_{2}^{\dagger }\mathbf{X}^{\dagger }\mathbf{Y}$\newline
$\Rightarrow \mathbf{\gamma }_{1}^{\dagger }\mathbf{X}^{\dagger }\mathbf{Y}=%
\mathbf{\gamma }_{2}^{\dagger }\mathbf{X}^{\dagger }\mathbf{Y}$

\item[5.] $\because \mathbf{X}\left( \mathbf{X}^{\dagger }\mathbf{X}\right)
^{-}\mathbf{X}^{\dagger }$ isinvariant to the choice of $\left( \mathbf{X}%
^{\dagger }\mathbf{X}\right) ^{-}$ (why!)\newline
hints:$\left\{ 
\begin{tabular}{l}
1. $\mathbf{X}^{\dagger }\mathbf{X}\left( \mathbf{X}^{\dagger }\mathbf{X}%
\right) ^{-}\mathbf{X}^{\dagger }\mathbf{X=X}^{\dagger }\mathbf{X}$ \\ 
2. $\mathbf{X}\left( \mathbf{X}^{\dagger }\mathbf{X}\right) ^{-}\mathbf{X}%
^{\dagger }\mathbf{X=X}$ \\ 
3. $\mathbf{X}G_{1}\mathbf{X}^{\dagger }\mathbf{X=X}G_{2}\mathbf{X}^{\dagger
}\mathbf{X}$ \\ 
$\vdots $ \\ 
$G_{1}=G_{2}$%
\end{tabular}%
\right. $\newline
\newline
Let $G_{1}$ and $G_{2}$ be two generalized inverse of $\left( \mathbf{X}%
^{\dagger }\mathbf{X}\right) $ then%
\begin{eqnarray*}
\mathbf{X}G_{1}\mathbf{X}^{\dagger } &\mathbf{=}&\mathbf{X}G_{2}\mathbf{X}%
^{\dagger }\quad \text{(}\mathbf{X}\left( \mathbf{X}^{\dagger }\mathbf{X}%
\right) ^{-}\mathbf{X}^{\dagger }\text{ is invariant)} \\
&\Rightarrow &\mathbf{a}^{\dagger }\mathbf{X}G_{1}\mathbf{X}^{\dagger }%
\mathbf{a=\mathbf{a}^{\dagger }X}G_{2}\mathbf{X}^{\dagger }\mathbf{a,}\quad 
\text{where }\mathbf{a}^{\dagger }\mathbf{X=\lambda } \\
&&\text{(}\mathbf{\lambda }^{\dagger }\mathbf{\beta }\text{ is estimable)} \\
&\Rightarrow &\mathbf{\lambda }G_{1}\mathbf{\lambda }^{\dagger }=\mathbf{%
\lambda }G_{2}\mathbf{\lambda }^{\dagger } \\
&\therefore &G_{1}=G_{2}
\end{eqnarray*}
\end{enumerate}
\end{proof}

\bigskip

\begin{theorem}
If $\mathbf{\lambda }^{\dagger }\mathbf{\beta }$ is an estimable function in
the linear model $\mathbf{Y=X\beta +\epsilon }$, where $r\left( \mathbf{X}%
\right) =r<p\leq n$, then $\mathbf{\lambda }^{\dagger }\mathbf{\beta }$ and $%
\mathbf{\gamma }^{\dagger }\mathbf{X}^{\dagger }\mathbf{Y}$ is BLUE.

\begin{proof}
\begin{eqnarray*}
&&\mathbf{\lambda }^{\dagger }\mathbf{\beta }\text{ is estimable} \\
&\Rightarrow &\exists \ \mathbf{a\quad }\text{s.t.}\mathbf{\quad a}^{\dagger
}\mathbf{X=\lambda }^{\dagger } \\
&\Rightarrow &\exists \ \mathbf{\gamma \quad }\text{s.t.}\mathbf{\quad
\gamma ^{\dagger }X}^{\dagger }\mathbf{X=\lambda }^{\dagger }
\end{eqnarray*}%
\newline
\newline
let $\mathbf{a}^{\dagger }\mathbf{Y}$ be a linear estimator of $\mathbf{%
\lambda }^{\dagger }\mathbf{\beta }$, WLOG, let $\mathbf{a}^{\dagger }=%
\mathbf{\gamma }^{\dagger }\mathbf{X}^{\dagger }\mathbf{+C}^{\dagger }$
where $\mathbf{\gamma }^{\dagger }$ is a solution to $\mathbf{\lambda
^{\dagger }=\gamma ^{\dagger }X}^{\dagger }\mathbf{X}$%
\begin{eqnarray*}
E\left( \mathbf{a}^{\dagger }\mathbf{Y}\right) &=&E\left( \left( \mathbf{%
\gamma }^{\dagger }\mathbf{X}^{\dagger }\mathbf{+C}^{\dagger }\right) 
\mathbf{Y}\right) \\
&=&\left( \mathbf{\gamma }^{\dagger }\mathbf{X}^{\dagger }\mathbf{+C}%
^{\dagger }\right) \mathbf{X\beta } \\
&=&\left( \mathbf{\gamma }^{\dagger }\mathbf{X}^{\dagger }\mathbf{X+C}%
^{\dagger }\mathbf{X}\right) \mathbf{\beta } \\
&=&\mathbf{\lambda }^{\dagger }\mathbf{\beta }
\end{eqnarray*}%
\begin{eqnarray*}
&\Rightarrow &\mathbf{\gamma }^{\dagger }\mathbf{X}^{\dagger }\mathbf{X+C}%
^{\dagger }\mathbf{X=\lambda }^{\dagger } \\
&\Rightarrow &\mathbf{C}^{\dagger }\mathbf{X=0\quad }\text{(}\because 
\mathbf{\lambda ^{\dagger }=\gamma ^{\dagger }X}^{\dagger }\mathbf{X}\text{)}
\end{eqnarray*}%
\begin{eqnarray*}
Var\left( \mathbf{a}^{\dagger }\mathbf{Y}\right) &=&\mathbf{a}^{\dagger
}\sigma ^{2}I\left( \mathbf{a}^{\dagger }\right) ^{\dagger } \\
&=&\sigma ^{2}\mathbf{a}^{\dagger }\mathbf{a}=\sigma ^{2}\left[ \left( 
\mathbf{\gamma }^{\dagger }\mathbf{X}^{\dagger }\mathbf{+C}^{\dagger
}\right) \left( \mathbf{\gamma X+C}\right) \right] \\
&=&\sigma ^{2}\left[ \mathbf{\gamma }^{\dagger }\mathbf{X}^{\dagger }\mathbf{%
X+\mathbf{\gamma }^{\dagger }\mathbf{X}^{\dagger }C+C}^{\dagger }\mathbf{%
X\gamma +C}^{\dagger }\mathbf{C}\right] \\
&=&\sigma ^{2}\left[ \mathbf{\gamma }^{\dagger }\mathbf{X}^{\dagger }\mathbf{%
X+}\underset{\text{min=0}}{\underbrace{\mathbf{C}^{\dagger }\mathbf{C}}}%
\right]
\end{eqnarray*}%
\begin{eqnarray*}
\mathbf{\hat{\beta}} &=&\left( \mathbf{X}^{\dagger }\mathbf{X}\right) ^{-}%
\mathbf{X}^{\dagger }\mathbf{Y} \\
\underset{\quad \hookrightarrow \text{BLUE}}{\mathbf{\lambda }^{\dagger }%
\mathbf{\hat{\beta}}} &\mathbf{\leadsto }&\mathbf{\lambda }^{\dagger }%
\mathbf{\beta }
\end{eqnarray*}
\end{proof}

\begin{proof}

\begin{enumerate}
\item 
\begin{eqnarray*}
E\left( \text{SSE}\right) &=&E\left( \mathbf{Y}^{\dagger }\left( I-P_{%
\mathbf{X}}\right) \mathbf{Y}\right) ,\quad P_{\mathbf{X}}=\mathbf{X}\left( 
\mathbf{X}^{\dagger }\mathbf{X}\right) ^{-}\mathbf{X}^{\dagger } \\
&=&tr\left( \left( I_{n}-P_{\mathbf{X}}\right) \sigma ^{2}I\right) +\left( 
\mathbf{X\beta }\right) ^{\dagger }\left( I-P_{\mathbf{X}}\right) \left( 
\mathbf{X\beta }\right) \\
&=&\sigma ^{2}tr\left( I_{n}-P_{\mathbf{X}}\right) +\mathbf{\beta }^{\dagger
}\mathbf{X}^{\dagger }\left( I-P_{\mathbf{X}}\right) \mathbf{X\beta } \\
&=&\sigma ^{2}tr\left( I_{n}-\mathbf{X}\left( \mathbf{X}^{\dagger }\mathbf{X}%
\right) ^{-}\mathbf{X}^{\dagger }\right) +0 \\
&=&\sigma ^{2}\left[ n-tr\left( \mathbf{X}\left( \mathbf{X}^{\dagger }%
\mathbf{X}\right) ^{-}\mathbf{X}^{\dagger }\right) \right] \\
&=&\sigma ^{2}\left[ n-\underset{r\left( \mathbf{X}\right) =r}{\underbrace{%
r\left( \mathbf{X}^{\dagger }\mathbf{X}\right) }}\right] \\
&=&\sigma ^{2}\left[ n-r\right]
\end{eqnarray*}%
\begin{equation*}
E\left( S^{2}\right) =E\left( \frac{\left( \mathbf{Y}-\mathbf{X\hat{\beta}}%
\right) ^{\dagger }\left( \mathbf{Y}-\mathbf{X\hat{\beta}}\right) }{n-r}%
\right) =\sigma ^{2}
\end{equation*}

\item $\because $ SSE $=\mathbf{Y}^{\dagger }\left( I-P_{\mathbf{X}}\right) 
\mathbf{Y}$, $P_{\mathbf{X}}=\mathbf{X}\left( \mathbf{X}^{\dagger }\mathbf{X}%
\right) ^{-}\mathbf{X}^{\dagger }$\newline
$\because \mathbf{X}\left( \mathbf{X}^{\dagger }\mathbf{X}\right) ^{-}%
\mathbf{X}^{\dagger }$ is invariant to choice of $\left( \mathbf{X}^{\dagger
}\mathbf{X}\right) ^{-}$\newline
$\therefore $SSE is invariant to choice of $\left( \mathbf{X}^{\dagger }%
\mathbf{X}\right) ^{-}$
\end{enumerate}
\end{proof}
\end{theorem}

\bigskip

\subsubsection{Reparmeterization}

\begin{eqnarray*}
\mathbf{Y} &=&\mathbf{X\beta +\epsilon \quad }\text{non-full-rank model} \\
&=&\mathbf{ZU\beta +\epsilon } \\
&=&\mathbf{Z\gamma +\epsilon \quad }\text{full-rank model}
\end{eqnarray*}

\begin{enumerate}
\item $\underset{r\times p}{\mathbf{U}}\underset{p\times 1}{\mathbf{\beta }}%
\mathbf{=}\underset{r\times 1}{\mathbf{\gamma }}$\newline
$\mathbf{\gamma }$ linearly indep. estimable functions of $\mathbf{\beta }$

\item $\mathbf{ZU=X}$\newline
$\Rightarrow \mathbf{ZUU}^{\dagger }=\mathbf{XU}^{\dagger }$\newline
$\Rightarrow \mathbf{Z=XU}^{\dagger }\left( \mathbf{UU}^{\dagger }\right)
^{-1}$ where $\mathbf{Z}$ is full-rank with $r\left( \mathbf{Z}\right) =r$

\item $\mathbf{\hat{\gamma}=}\left( \mathbf{Z^{\dagger }Z}\right) ^{-1}%
\mathbf{Z^{\dagger }Y}$%
\begin{eqnarray*}
S^{2} &=&\frac{\left( \mathbf{Y-Z\hat{\gamma}}\right) ^{\dagger }\left( 
\mathbf{Y-Z\hat{\gamma}}\right) }{n-r} \\
&=&\frac{\left( \mathbf{Y}-\mathbf{X\hat{\beta}}\right) ^{\dagger }\left( 
\mathbf{Y}-\mathbf{X\hat{\beta}}\right) }{n-r},\quad \mathbf{Z\gamma =X\beta 
}\Leftrightarrow \mathbf{Z\hat{\gamma}=X\hat{\beta}}
\end{eqnarray*}
\end{enumerate}

\bigskip

\begin{example}
Consider the model%
\begin{equation*}
y_{ij}=\mu +\alpha _{i}+\varepsilon _{ij},\quad i=1,2,\quad j=1,2
\end{equation*}%
\begin{equation*}
\mathbf{Y=X\beta +\epsilon =}\left[ 
\begin{array}{ccc}
1 & 1 & 0 \\ 
1 & 1 & 0 \\ 
1 & 0 & 1 \\ 
1 & 0 & 1%
\end{array}%
\right] \left[ 
\begin{array}{c}
\mu \\ 
\alpha _{1} \\ 
\alpha _{2}%
\end{array}%
\right] +\epsilon ,\quad r\left( \mathbf{X}\right) =2
\end{equation*}%
\newline
let%
\begin{eqnarray*}
\mathbf{\gamma } &\mathbf{=}&\left[ 
\begin{array}{c}
\gamma _{1} \\ 
\gamma _{2}%
\end{array}%
\right] =\left[ 
\begin{array}{c}
\mu +\alpha _{1} \\ 
\mu +\alpha _{2}%
\end{array}%
\right] =\left[ 
\begin{array}{ccc}
1 & 1 & 0 \\ 
1 & 0 & 1%
\end{array}%
\right] \left[ 
\begin{array}{c}
\mu \\ 
\alpha _{1} \\ 
\alpha _{2}%
\end{array}%
\right] \\
&=&\mathbf{U\beta }
\end{eqnarray*}%
\begin{equation*}
\mathbf{Z}=\mathbf{XU}^{\dagger }\left( \mathbf{UU}^{\dagger }\right) ^{-1}=%
\left[ 
\begin{array}{cc}
1 & 0 \\ 
1 & 0 \\ 
0 & 1 \\ 
0 & 1%
\end{array}%
\right] ,\quad r\left( \mathbf{Z}\right) =2
\end{equation*}%
\begin{equation*}
\mathbf{Z}^{\dagger }\mathbf{Y=}\left[ 
\begin{array}{cccc}
1 & 1 & 0 & 0 \\ 
0 & 0 & 1 & 1%
\end{array}%
\right] \left[ 
\begin{array}{c}
Y_{11} \\ 
Y_{12} \\ 
Y_{21} \\ 
Y_{22}%
\end{array}%
\right] =\left[ 
\begin{array}{c}
y_{1} \\ 
y_{2}%
\end{array}%
\right]
\end{equation*}%
\begin{equation*}
\mathbf{Z}^{\dagger }\mathbf{Z=}\left[ 
\begin{array}{cccc}
1 & 1 & 0 & 0 \\ 
0 & 0 & 1 & 1%
\end{array}%
\right] \left[ 
\begin{array}{cc}
1 & 0 \\ 
1 & 0 \\ 
0 & 1 \\ 
0 & 1%
\end{array}%
\right] =\left[ 
\begin{array}{cc}
2 & 0 \\ 
0 & 2%
\end{array}%
\right]
\end{equation*}%
\begin{equation*}
\mathbf{Z}^{\dagger }\mathbf{Z\hat{\gamma}=Z}^{\dagger }\mathbf{Y}
\end{equation*}%
\begin{equation*}
\left[ 
\begin{array}{cc}
2 & 0 \\ 
0 & 2%
\end{array}%
\right] \left[ 
\begin{array}{c}
\hat{r}_{1} \\ 
\hat{r}_{2}%
\end{array}%
\right] =\left[ 
\begin{array}{c}
y_{1} \\ 
y_{2}%
\end{array}%
\right] \quad 
\begin{tabular}{l}
$\hat{r}_{1}=\frac{y_{1}}{2}$ \\ 
$\hat{r}_{2}=\frac{y_{2}}{2}$%
\end{tabular}%
\end{equation*}
\end{example}

\bigskip

\paragraph{Side Conditions}

\begin{theorem}
If $\mathbf{Y}=\mathbf{X\beta +\epsilon }$ where $\mathbf{X}$ is $n\times p$
matrix with $r\left( \mathbf{X}\right) =r<p\leq n$, and if $\Pi _{\left(
p-r\right) \times p}$ matrix with $r\left( \Pi \right) =p-r$ such that $\Pi 
\mathbf{\beta }$ is a set of nonestimable functions, then there is a unique
vector $\hat{\beta}$ that satisfies both

\begin{enumerate}
\item $\mathbf{X}^{\dagger }\mathbf{X}\hat{\beta}=\mathbf{X^{\dagger }Y}$,
and

\item $\Pi \mathbf{\beta =0}$
\end{enumerate}
\end{theorem}

\bigskip

\begin{example}
Consider%
\begin{equation*}
y_{ij}=\mu +\alpha _{i}+\varepsilon _{ij},\quad i=1,2,\quad j=1,2,3
\end{equation*}%
\begin{equation*}
\left[ 
\begin{array}{c}
y_{11} \\ 
y_{12} \\ 
y_{13} \\ 
y_{21} \\ 
y_{22} \\ 
y_{23}%
\end{array}%
\right] =\left[ 
\begin{array}{ccc}
1 & 1 & 0 \\ 
1 & 1 & 0 \\ 
1 & 1 & 0 \\ 
1 & 0 & 1 \\ 
1 & 0 & 1 \\ 
1 & 0 & 1%
\end{array}%
\right] _{6\times 3\curvearrowright r\left( \mathbf{X}\right) =\_\_\_\_}%
\left[ 
\begin{array}{c}
\mu \\ 
\alpha _{1} \\ 
\alpha _{2}%
\end{array}%
\right] +\varepsilon
\end{equation*}%
\newline
let $\alpha _{1}+\alpha _{2}=0\Leftrightarrow \alpha _{1}=-\alpha _{2}$%
\newline
\newline
side condition $\left[ 
\begin{array}{ccc}
0 & 1 & 1%
\end{array}%
\right] \left[ 
\begin{array}{c}
\mu \\ 
\alpha _{1} \\ 
\alpha _{2}%
\end{array}%
\right] =\Pi \mathbf{\beta }$%
\begin{eqnarray*}
y_{1j} &=&\mu ^{\ast }+\alpha _{1}+\varepsilon _{1j} \\
y_{2j} &=&\mu ^{\ast }-\alpha _{1}+\varepsilon _{2j}
\end{eqnarray*}%
\newline
Thus, we have%
\begin{equation*}
\left[ 
\begin{array}{c}
y_{11} \\ 
y_{12} \\ 
y_{13} \\ 
y_{21} \\ 
y_{22} \\ 
y_{23}%
\end{array}%
\right] =\left[ 
\begin{array}{cc}
1 & 1 \\ 
1 & 1 \\ 
1 & 1 \\ 
1 & -1 \\ 
1 & -1 \\ 
1 & -1%
\end{array}%
\right] \left[ 
\begin{array}{c}
\mu ^{\ast } \\ 
\alpha _{1}^{\ast }%
\end{array}%
\right] +\varepsilon
\end{equation*}%
\begin{equation*}
\mathbf{Y=X}^{\ast }\mathbf{\gamma +\epsilon ,\quad }\text{where }\mathbf{X}%
^{\ast }\text{is full-rank}
\end{equation*}%
\newline
$\therefore $ the parameter $\mu ^{\ast }$ and $\alpha _{1}^{\ast }$ can be
estimable
\end{example}

\bigskip

\begin{example}
\begin{proof}
the two sets of equation%
\begin{eqnarray*}
\mathbf{Y} &=&\mathbf{X\beta +\epsilon } \\
\mathbf{0} &=&\Pi \mathbf{\beta +0}
\end{eqnarray*}%
\begin{equation*}
\Rightarrow \left[ 
\begin{array}{c}
\mathbf{Y} \\ 
\mathbf{0}%
\end{array}%
\right] =\left[ 
\begin{array}{c}
\mathbf{X} \\ 
\Pi%
\end{array}%
\right] \mathbf{\beta +}\left[ 
\begin{array}{c}
\mathbf{\epsilon } \\ 
0%
\end{array}%
\right]
\end{equation*}%
$\because $ the rows of $\Pi $ are linearly independent and $\underset{\text{%
(nonestimable)}}{\underline{\text{are not functions of the rows of \textbf{X}%
}}}$\newline
\newline
the $\left[ 
\begin{array}{c}
\mathbf{X} \\ 
\Pi%
\end{array}%
\right] $ is $\left( n+p-r\right) \times p$ of rank $p$\newline
\newline
$\Rightarrow \left[ 
\begin{array}{c}
\mathbf{X} \\ 
\Pi%
\end{array}%
\right] ^{\dagger }\left[ 
\begin{array}{c}
\mathbf{X} \\ 
\Pi%
\end{array}%
\right] $ is $p\times p$ of rank $p$ (rull rank)\newline
\newline
the system of equations%
\begin{equation*}
\left[ 
\begin{array}{c}
\mathbf{X} \\ 
\Pi%
\end{array}%
\right] ^{\dagger }\left[ 
\begin{array}{c}
\mathbf{X} \\ 
\Pi%
\end{array}%
\right] \hat{\beta}=\left[ 
\begin{array}{c}
\mathbf{X} \\ 
\Pi%
\end{array}%
\right] \left[ 
\begin{array}{c}
\mathbf{Y} \\ 
\mathbf{0}%
\end{array}%
\right]
\end{equation*}%
\begin{equation*}
\Leftrightarrow \left( \mathbf{X}^{\dagger }\mathbf{X+}\Pi ^{\dagger }\Pi
\right) \hat{\beta}=\mathbf{X}^{\dagger }\mathbf{Y+}\Pi ^{\dagger }\mathbf{%
0\quad (\ast )}
\end{equation*}%
\begin{eqnarray*}
\hat{\beta} &=&\left( \left[ 
\begin{array}{c}
\mathbf{X} \\ 
\Pi%
\end{array}%
\right] ^{\dagger }\left[ 
\begin{array}{c}
\mathbf{X} \\ 
\Pi%
\end{array}%
\right] \right) ^{-1}\left[ 
\begin{array}{c}
\mathbf{X} \\ 
\Pi%
\end{array}%
\right] ^{-1}\left[ 
\begin{array}{c}
\mathbf{Y} \\ 
\mathbf{0}%
\end{array}%
\right] \\
&=&\left( \mathbf{X}^{\dagger }\mathbf{X+}\Pi ^{\dagger }\Pi \right)
^{-1}\left( \mathbf{X}^{\dagger }\mathbf{Y+}\Pi ^{\dagger }\mathbf{0}\right)
\\
&=&\left( \mathbf{X}^{\dagger }\mathbf{X+}\Pi ^{\dagger }\Pi \right) ^{-1}%
\mathbf{X}^{\dagger }\mathbf{Y}
\end{eqnarray*}%
\newline
by $\left( \ast \right) $, we have%
\begin{equation*}
\mathbf{X}^{\dagger }\mathbf{X\hat{\beta}+}\Pi ^{\dagger }\Pi \hat{\beta}=%
\mathbf{X}^{\dagger }\mathbf{Y}
\end{equation*}%
\begin{equation*}
\Leftrightarrow \mathbf{X}^{\dagger }\mathbf{X\hat{\beta}+}\Pi ^{\dagger
}\Pi \hat{\beta}=\mathbf{X}^{\dagger }\mathbf{Y}
\end{equation*}
\end{proof}
\end{example}

\begin{example}
Consider%
\begin{equation*}
\mathbf{X}=\left[ 
\begin{array}{ccc}
1 & 1 & 0 \\ 
1 & 1 & 0 \\ 
1 & 1 & 0 \\ 
1 & 0 & 1 \\ 
1 & 0 & 1 \\ 
1 & 0 & 1%
\end{array}%
\right] \text{ and }\Pi =\left[ 
\begin{array}{ccc}
0 & 1 & 1%
\end{array}%
\right]
\end{equation*}%
\begin{equation*}
\left( \mathbf{X}^{\dagger }\mathbf{X+}\Pi ^{\dagger }\Pi \right) ^{-1}=%
\frac{1}{4}\left[ 
\begin{array}{ccc}
2 & -1 & -1 \\ 
-1 & 2 & 0 \\ 
-1 & - & 2%
\end{array}%
\right] ,\text{\quad }\mathbf{X}^{\dagger }\mathbf{Y=}\left[ 
\begin{array}{c}
y_{\cdot \cdot } \\ 
y_{1\cdot } \\ 
y_{2\cdot }%
\end{array}%
\right]
\end{equation*}%
Thus,%
\begin{eqnarray*}
\mathbf{\hat{\beta}} &\mathbf{=}&\frac{1}{4}\left[ 
\begin{array}{ccc}
2 & -1 & -1 \\ 
-1 & 2 & 0 \\ 
-1 & - & 2%
\end{array}%
\right] \left[ 
\begin{array}{c}
y_{\cdot \cdot } \\ 
y_{1\cdot } \\ 
y_{2\cdot }%
\end{array}%
\right] \\
&=&\left[ 
\begin{array}{c}
\frac{2y_{\cdot \cdot }-y_{1\cdot }-y_{2\cdot }}{4} \\ 
\frac{-y_{\cdot \cdot }+2y_{1\cdot }}{4} \\ 
\frac{-y_{\cdot \cdot }+2y_{2\cdot }}{2}%
\end{array}%
\right] =\left[ 
\begin{array}{c}
\frac{2\left( y_{1\cdot }+y_{2\cdot }\right) -y_{1\cdot }-y_{2\cdot }}{4} \\ 
\frac{-\left( y_{1\cdot }+y_{2\cdot }\right) +2y_{1\cdot }}{4} \\ 
\frac{-\left( y_{1\cdot }+y_{2\cdot }\right) +2y_{2\cdot }}{2}%
\end{array}%
\right] \\
&=&\left[ 
\begin{array}{c}
\bar{y}_{\cdot \cdot } \\ 
\bar{y}_{1\cdot }-\bar{y}_{\cdot \cdot } \\ 
\bar{y}_{2\cdot }-\bar{y}_{\cdot \cdot }%
\end{array}%
\right]
\end{eqnarray*}%
\begin{equation*}
\Pi \mathbf{\beta }=0\Leftrightarrow \alpha _{1}+\alpha _{2}=0\quad \text{%
side condition}
\end{equation*}
\end{example}

\bigskip

\section{Testing Hypothesis}

\begin{definition}
A hypothesis $H_{0}:\beta _{1}=\cdots =\beta _{q}$ is said to be testable if
there exists a set of linearly independent estimable $\mathbf{\lambda }%
_{1}^{\dagger }\mathbf{\beta }$, $\mathbf{\lambda }_{2}^{\dagger }\mathbf{%
\beta }$, $\cdots $, $\mathbf{\lambda }_{t}^{\dagger }\mathbf{\beta }$ s.t.%
\newline
\begin{equation*}
H_{0}\text{ is true if and only if }\mathbf{\lambda }_{1}^{\dagger }\mathbf{%
\beta }=\mathbf{\lambda }_{2}^{\dagger }\mathbf{\beta }=\cdots =\mathbf{%
\lambda }_{t}^{\dagger }\mathbf{\beta }
\end{equation*}
\end{definition}

\bigskip

note:%
\begin{equation*}
\beta _{1}=\cdots =\beta _{q}\Leftrightarrow \left[ 
\begin{array}{c}
\mathbf{\lambda }_{1}^{\dagger }\mathbf{\beta } \\ 
\vdots \\ 
\mathbf{\lambda }_{q-1}^{\dagger }\mathbf{\beta }%
\end{array}%
\right] =\left[ 
\begin{array}{c}
0 \\ 
\vdots \\ 
0%
\end{array}%
\right]
\end{equation*}%
where $\mathbf{\lambda }_{1}^{\dagger }\mathbf{\beta }$, $\mathbf{\lambda }%
_{2}^{\dagger }\mathbf{\beta }$, $\cdots $, $\mathbf{\lambda }%
_{q-1}^{\dagger }\mathbf{\beta }$ is a set of linearly independent estimable
functions.

\bigskip

\begin{example}
Consider a model%
\begin{equation*}
y_{ij}=\mu +\alpha _{i}+\beta _{j}+\varepsilon _{ij}\quad 
\begin{tabular}{l}
$i=1,2,3$ \\ 
$j=1,2,3$%
\end{tabular}%
\end{equation*}%
the design matrix is 
\begin{equation*}
\mathbf{X}=\left[ 
\begin{array}{ccccccc}
1 & 1 & 0 & 0 & 1 & 0 & 0 \\ 
1 & 1 & 0 & 0 & 0 & 0 & 0 \\ 
1 & 1 & 0 & 0 & 0 & 0 & 1 \\ 
1 & 0 & 1 & 0 & 1 & 0 & 0 \\ 
1 & 0 & 1 & 0 & 0 & 1 & 0 \\ 
1 & 0 & 1 & 0 & 0 & 0 & 1 \\ 
1 & 0 & 0 & 1 & 1 & 0 & 0 \\ 
1 & 0 & 0 & 1 & 0 & 1 & 0 \\ 
1 & 0 & 0 & 1 & 0 & 0 & 1%
\end{array}%
\right] \quad \mathbf{\beta }=\left[ 
\begin{array}{c}
\mu \\ 
\alpha _{1} \\ 
\alpha _{2} \\ 
\beta _{1} \\ 
\beta _{2} \\ 
\beta _{3}%
\end{array}%
\right]
\end{equation*}%
\newline
\newline
$\alpha _{1}-\alpha _{2}$ and $\alpha _{1}+\alpha _{2}-2\alpha _{3}$ are
indep. estimable functions%
\begin{equation*}
\therefore H_{0}:\alpha _{1}=\alpha _{2}=\alpha _{3}\Leftrightarrow 
\begin{tabular}{l}
$\alpha _{1}-\alpha _{2}=0$ \\ 
$\alpha _{1}+\alpha _{2}-2\alpha _{3}=0$%
\end{tabular}%
\end{equation*}%
\begin{equation*}
\Leftrightarrow H_{0}:\left[ 
\begin{array}{ccccccc}
0 & 1 & -1 & 0 & 0 & 0 & 0 \\ 
0 & 1 & 1 & -2 & 0 & 0 & 0%
\end{array}%
\right] \left[ 
\begin{array}{c}
\mu \\ 
\alpha _{1} \\ 
\alpha _{2} \\ 
\alpha _{3} \\ 
\beta _{1} \\ 
\beta _{2} \\ 
\beta _{3}%
\end{array}%
\right] =0
\end{equation*}
\end{example}

\bigskip

\bigskip

\subsubsection{G.L.H}

\begin{theorem}
In the model $\mathbf{Y}=\mathbf{X\beta +\epsilon ,}$ where $\mathbf{%
\epsilon }\sim N_{n}\left( \mathbf{0},\sigma ^{2}I\right) $ and $\mathbf{X}$
is $n\times p$ matrix with $r\left( \mathbf{X}\right) =r<p\leq n$. If $%
\mathbf{C}$ is $m\times p$ matrix with $r\left( \mathbf{C}\right) =m\leq r$
s.t $\mathbf{C\beta }$ is a set of $m$ linearly indep. estimable functions
and if $\hat{\beta}=\left( \mathbf{X}^{\dagger }\mathbf{X}\right) ^{-}%
\mathbf{X}^{\dagger }\mathbf{Y}$, then

\begin{enumerate}
\item $C\mathbf{\hat{\beta}}$ is $N_{m}\left( \mathbf{C\beta },\underset{%
\text{nonsingular}}{\underbrace{\sigma ^{2}\mathbf{C}\left( \mathbf{X}%
^{\dagger }\mathbf{X}\right) ^{-}\mathbf{C}^{\dagger }}}\right) $

\item $\frac{\text{SSH}}{\sigma ^{2}}=\frac{\left( C\hat{\beta}\right)
^{\dagger }\left[ C\left( \mathbf{X}^{\dagger }\mathbf{X}\right)
^{-}C^{\dagger }\right] ^{-1}C\hat{\beta}}{\sigma ^{2}}\sim \chi
_{m}^{2}\left( \frac{\left( C\beta \right) ^{\dagger }\left[ \mathbf{C}%
\left( \mathbf{X}^{\dagger }\mathbf{X}\right) ^{-}\mathbf{C}^{\dagger }%
\right] ^{-1}C\beta }{2\sigma ^{2}}\right) $

\item $\frac{\text{SSE}}{\sigma ^{2}}=\frac{\mathbf{Y}^{\dagger }\left( I-%
\mathbf{X}\left( \mathbf{X}^{\dagger }\mathbf{X}\right) ^{-}\mathbf{X}%
^{\dagger }\right) \mathbf{Y}}{\sigma ^{2}}\sim \chi _{n-r}^{2}$

\item SSH and SSE are indep.

\item $\frac{\text{SSH}/m}{\text{SSE}/n-r}\sim F_{m,n-r}\quad $if $H_{0}:%
\mathbf{C\beta =0}$ is true.
\end{enumerate}

\begin{proof}
\begin{enumerate}
\item $\because $ $\mathbf{C\beta }$ is a set of $m$ linearly indep
estimable functions%
\begin{equation*}
\therefore C\left( \mathbf{X}^{\dagger }\mathbf{X}\right) ^{-}\mathbf{X}%
^{\dagger }\mathbf{X=}C
\end{equation*}%
\begin{equation*}
r\left( C\right) \leq r\left( C\left( \mathbf{X}^{\dagger }\mathbf{X}\right)
^{-}\mathbf{X}^{\dagger }\right) \leq r\left( C\right) 
\end{equation*}%
\begin{equation*}
r\left( C\left( \mathbf{X}^{\dagger }\mathbf{X}\right) ^{-}\mathbf{X}%
^{\dagger }\right) =r\left( C\right) =m
\end{equation*}%
and%
\begin{eqnarray*}
r\left( C\right)  &=&r\left( C\left( \mathbf{X}^{\dagger }\mathbf{X}\right)
^{-}\mathbf{X}^{\dagger }\right)  \\
&=&r\left( C\left( \mathbf{X}^{\dagger }\mathbf{X}\right) ^{-}\mathbf{X}%
^{\dagger }\left( C\left( \mathbf{X}^{\dagger }\mathbf{X}\right) ^{-}\mathbf{%
X}^{\dagger }\right) ^{\dagger }\right)  \\
&=&r\left( C\left( \mathbf{X}^{\dagger }\mathbf{X}\right) ^{-}\mathbf{X}%
^{\dagger }\mathbf{X}\left( \mathbf{X}^{\dagger }\mathbf{X}\right)
^{-}C^{\dagger }\right)  \\
&=&r\left( C\left( \mathbf{X}^{\dagger }\mathbf{X}\right) ^{-}\mathbf{X}%
^{\dagger }\mathbf{C}^{\dagger }\right) =m
\end{eqnarray*}%
\begin{equation*}
C\left( \mathbf{X}^{\dagger }\mathbf{X}\right) ^{-}\mathbf{X}^{\dagger }%
\mathbf{C}^{\dagger }\text{ is nonsingular}
\end{equation*}%
\begin{eqnarray*}
H_{0} &:&\mathbf{Y=Wr+\epsilon }\text{\quad R.M\quad }\left( H_{0}:\beta
_{1}=\cdots =\beta _{q}\right)  \\
H_{1} &:&\mathbf{Y=X\beta +\epsilon }\text{\quad F.M}
\end{eqnarray*}%
\begin{equation*}
F=\frac{\mathbf{Y}^{\dagger }\left( P_{\mathbf{X}}-P_{W}\right) \mathbf{Y}%
/r^{\ast }}{\mathbf{Y}^{\dagger }\left( I-P_{\mathbf{X}}\right) \mathbf{Y}%
/\left( n-r\right) }\sim F_{r^{\ast },n-r}
\end{equation*}
\end{enumerate}
\end{proof}
\end{theorem}

\end{document}
