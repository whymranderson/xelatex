
\documentclass{article}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{accents}
\usepackage[ignoreall,a4paper]{geometry}
\usepackage{fancyhdr}

\setcounter{MaxMatrixCols}{10}
%TCIDATA{OutputFilter=LATEX.DLL}
%TCIDATA{Version=5.00.0.2606}
%TCIDATA{<META NAME="SaveForMode" CONTENT="1">}
%TCIDATA{BibliographyScheme=Manual}
%TCIDATA{Created=Wednesday, November 25, 2015 15:33:37}
%TCIDATA{LastRevised=Thursday, March 31, 2016 18:31:18}
%TCIDATA{<META NAME="GraphicsSave" CONTENT="32">}
%TCIDATA{<META NAME="DocumentShell" CONTENT="Standard LaTeX\Blank - Standard LaTeX Article">}
%TCIDATA{CSTFile=40 LaTeX article.cst}
%TCIDATA{ComputeDefs=
%$W=\left( 1-\sigma \right) I$
%}


\newtheorem{theorem}{Theorem}
\newtheorem{acknowledgement}[theorem]{Acknowledgement}
\newtheorem{algorithm}[theorem]{Algorithm}
\newtheorem{axiom}[theorem]{Axiom}
\newtheorem{case}[theorem]{Case}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{conclusion}[theorem]{Conclusion}
\newtheorem{condition}[theorem]{Condition}
\newtheorem{conjecture}[theorem]{Conjecture}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{criterion}[theorem]{Criterion}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}
\newtheorem{exercise}[theorem]{Exercise}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{notation}[theorem]{Notation}
\newtheorem{problem}[theorem]{Problem}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{solution}[theorem]{Solution}
\newtheorem{summary}[theorem]{Summary}
\newenvironment{proof}[1][Proof]{\noindent\textbf{#1.} }{\ \rule{0.5em}{0.5em}}
\input{../../tcilatex}
\DeclareMathAccent{\wtilde}{\mathord}{largesymbols}{"65}
\pagestyle{fancy}
\fancyfoot[C]{\thepage}

\begin{document}


\setcounter{part}{1} \setcounter{page}{1}

\section{Introduction}

mathematical / statistical models

\begin{equation*}
\overset{x}{\text{input}}\longrightarrow \frame{process}\longrightarrow 
\overset{y}{\text{output}}
\end{equation*}

\begin{eqnarray*}
y &=&g\left( x_{1},x_{2},\cdots ,x_{p}\right) \\
y_{i} &=&\underline{g}\left( x_{1},x_{2},\cdots ,x_{p}\right) +\underline{%
\varepsilon }_{i}
\end{eqnarray*}

i.e. $F=\frac{9}{5}C+32$, $E\left( \varepsilon _{i}\right) =0$

\bigskip

\begin{equation*}
E\left( Y\right) =\underset{\text{mean function}}{\underbrace{g\left(
x_{1},x_{2},\cdots ,x_{p}\right) }}=\mu _{Y}
\end{equation*}%
\begin{eqnarray*}
\mu _{Y} &=&g\left( x_{1},x_{2},\cdots ,x_{p}\right) \\
&=&g\left( x_{1},\beta _{1},x_{2},\beta _{2},\cdots ,x_{p},\beta _{p}\right)
\\
&=&x_{1}\beta _{1}+\cdots +x_{p}\beta _{p}
\end{eqnarray*}

\begin{equation*}
h\left( \mu _{i}\right) =\beta _{1}x_{1}+\cdots +\beta _{p}x_{p}
\end{equation*}

\bigskip

\paragraph{Simple Linear Regression Model}

\begin{equation*}
\underline{y}_{i}=\beta _{0}+\beta _{1}x_{i}+\underline{\varepsilon }%
_{i},\quad i=1,\cdots ,n
\end{equation*}

\paragraph{Multiple Linear Regression Model}

\begin{equation*}
y_{i}=\beta _{0}+\beta _{1}x_{1i}+\cdots +\beta _{p}x_{pi}+\varepsilon _{i}
\end{equation*}

\paragraph{Analysis of Variance Model (ANOVA)}

\begin{enumerate}
\item $y_{ij}=\mu _{i}+\varepsilon _{ij}=\mu +\alpha _{i}+\varepsilon _{ij}$%
, $i=1,2,3$, $j=1,2,\cdots ,n$\newline
($H_{0}:\mu _{1}=\mu _{2}=\mu _{3}\Longleftrightarrow H_{0}=\alpha
_{1}=\alpha _{2}=\alpha _{3}$)

\item 
\begin{eqnarray*}
y_{ijk} &=&\mu _{ijk}+\varepsilon _{ijk} \\
&=&\mu +\alpha _{i}+\beta _{j}+\gamma _{ij}+\varepsilon _{ijk}
\end{eqnarray*}%
\begin{eqnarray*}
i &=&1,\cdots ,3 \\
j &=&1,\cdots ,3 \\
k &=&1,\cdots ,k
\end{eqnarray*}%
\begin{equation*}
\underset{n\times 1}{\mathbf{Y}}\mathbf{=}\underset{n\times p}{\mathbf{X}}%
\underset{p\times 1}{\beta }+\underset{n\times 1}{\varepsilon }
\end{equation*}

\item linear model%
\begin{equation*}
\underset{n\times 1}{\mathbf{Y}}\mathbf{=}\underset{n\times p}{\mathbf{X}}%
\underset{p\times 1}{\mathbf{\beta }}+\underset{n\times 1}{\mathbf{%
\varepsilon }}
\end{equation*}%
where $\mathbf{Y}$ is the n-vector of response, $\mathbf{X}$ is the $n\times
p$ design matrix, $\mathbf{\beta }$ is the p-vector parameters and $\mathbf{%
\varepsilon }$ is the n-vector of errors.
\end{enumerate}

\bigskip

\subsubsection{Simple Linear Regression Model%
\protect\begin{equation*}
y_{i}=g\left( x_{i},\protect\beta _{1}\right) +\protect\varepsilon _{i}\quad
,\quad i=1,2,\cdots ,n 
\protect\end{equation*}%
}

\begin{enumerate}
\item $g\left( x_{i},\beta _{1}\right) =\beta _{0}+\beta _{1}x_{i}$

\item $g\left( x_{i},\beta _{1}\right) =\beta _{0}+\beta _{1}\log \left(
x_{i}\right) =\beta _{0}+\beta _{1}x_{i}^{\prime }$

\item $g\left( x_{i},\beta _{1}\right) =\beta _{0}+\beta _{1}\frac{1}{x_{i}}%
=\beta _{0}+\beta _{1}x"_{i}$
\end{enumerate}

\begin{equation*}
\begin{array}{c}
y_{1}=\beta _{0}+\beta _{1}x_{1}+\varepsilon _{1} \\ 
\vdots \\ 
y_{n}=\beta _{0}+\beta _{1}x_{n}+\varepsilon _{1}%
\end{array}%
\end{equation*}%
\begin{equation*}
\Rightarrow \left[ 
\begin{array}{c}
y_{1} \\ 
\vdots \\ 
y_{n}%
\end{array}%
\right] =\left[ 
\begin{array}{cc}
1 & x_{1} \\ 
\vdots & \vdots \\ 
1 & x_{n}%
\end{array}%
\right] _{n\times 2}\left[ 
\begin{array}{c}
\beta _{0} \\ 
\beta _{1}%
\end{array}%
\right] _{2\times 1}+\left[ 
\begin{array}{c}
\varepsilon _{1} \\ 
\vdots \\ 
\varepsilon _{n}%
\end{array}%
\right]
\end{equation*}%
\begin{equation*}
\left[ \mathbf{Y=X\beta +\varepsilon }\right]
\end{equation*}%
\begin{equation*}
y_{i}=\beta _{0}+\beta _{1}x_{i}\quad \Rightarrow y_{i}-\left( \beta
_{0}+\beta _{1}x_{i}\right) =0
\end{equation*}

\bigskip

\paragraph{Least Squares Estimation}

\begin{equation*}
Q\left( \beta _{0},\beta _{1}\right) =\tsum\limits_{i=1}^{n}\left[
y_{i}-\left( \beta _{0}+\beta _{1}x_{i}\right) \right] ^{2}
\end{equation*}%
\begin{equation*}
\left\{ 
\begin{array}{c}
\frac{\partial Q}{\partial \beta _{0}}=0 \\ 
\frac{\partial Q}{\partial \beta _{1}}=0%
\end{array}%
\right.
\end{equation*}%
\begin{equation*}
\Rightarrow \left\{ 
\begin{array}{c}
-2\tsum\limits_{i=1}^{n}\left[ y_{i}-\hat{\beta}_{0}-\hat{\beta}_{1}x_{i}%
\right] =0 \\ 
-2\tsum\limits_{i=1}^{n}\left[ y_{i}-\hat{\beta}_{0}-\hat{\beta}_{1}x_{i}%
\right] x_{i}=0%
\end{array}%
\right. \text{normal equation}
\end{equation*}

\begin{eqnarray*}
&\Rightarrow &\hat{\beta}_{1}=\frac{\tsum\limits_{i=1}^{n}\left( x_{i}-\bar{x%
}\right) \left( y_{i}-\bar{y}\right) }{\tsum\limits_{i=1}^{n}\left( x_{i}-%
\bar{x}\right) ^{2}} \\
&=&\frac{\tsum\limits_{i=1}^{n}x_{i}^{\prime }y_{i}^{\prime }}{%
\tsum\limits_{i=1}^{n}x_{i}^{\prime 2}}=\frac{S_{xy}}{S_{xx}}
\end{eqnarray*}%
and%
\begin{equation*}
\hat{\beta}_{0}=\bar{y}-\hat{\beta}_{1}\bar{x}
\end{equation*}

the fitted equation $Y=\beta _{0}+\beta _{1}x_{i}$%
\begin{equation*}
\hat{y}_{i}=\hat{\beta}_{0}+\hat{\beta}_{1}x_{i}=\left( \bar{y}-\hat{\beta}%
\bar{x}\right) +\hat{\beta}_{1}\bar{x}=\bar{y}+\hat{\beta}_{1}\left( x_{i}-%
\bar{x}\right)
\end{equation*}

the residual%
\begin{eqnarray*}
r_{i} &=&y_{i}-\hat{y}_{i}=y_{i}-\left( \bar{y}+\hat{\beta}_{1}\left( x_{i}-%
\bar{x}\right) \right) \\
&=&\left( y_{i}-\bar{y}\right) +\hat{\beta}_{1}\left( x_{i}-\bar{x}\right)
\end{eqnarray*}

\paragraph{$Y_{i}=\protect\beta _{0}+\protect\beta _{1}x_{i}+\protect%
\varepsilon _{i}\quad ,\quad i=1,\cdots ,n$}

make the following additional assumptions

\begin{enumerate}
\item $E\left( \varepsilon _{i}\right) =0\quad ,\quad \forall i\qquad $(i.e. 
$E\left( Y_{i}\right) =\beta _{0}+\beta _{1}x_{i}$\quad $\mu _{y}=\beta
_{0}+\beta _{1}x_{i}$)

\item $Var\left( \varepsilon _{i}\right) =\sigma ^{2}\quad ,\quad \forall
i\qquad $(i.e. $Var\left( Y_{i}\right) =\sigma ^{2}$)

\item $Cov\left( \varepsilon _{i},\varepsilon _{j}\right) =0\quad ,\quad
\forall i\neq j\qquad $(i.e. $Cov\left( Y_{i},Y_{j}\right) =0$)
\end{enumerate}

\bigskip

$k_{i}=\frac{\left( x_{i}-\bar{x}\right) }{\tsum \left( x_{i}-\bar{x}\right)
^{2}}$

\begin{eqnarray*}
E\left( \hat{\beta}_{1}\right) &=&E\left( \frac{\tsum \left( x_{i}-\bar{x}%
\right) \left( y_{i}-\bar{y}\right) }{\tsum \left( x_{i}-\bar{x}\right) ^{2}}%
\right) =E\left( \tsum\limits_{i=1}^{n}k_{i}Y_{i}\right) \\
&=&\tsum\limits_{i=1}^{n}k_{i}E\left( Y_{i}\right)
=\tsum\limits_{i=1}^{n}k_{i}\left( \beta _{0}+\beta _{1}x_{i}\right) \\
&=&\beta _{0}\underset{0}{\underbrace{\tsum k_{i}}}+\beta _{1}\underset{1}{%
\underbrace{\tsum k_{i}x_{i}}}=\beta _{1}
\end{eqnarray*}

\bigskip

\begin{eqnarray*}
E\left( \hat{\beta}_{0}\right) &=&E\left( \bar{Y}-\hat{\beta}_{1}\bar{X}%
\right) =E\left( \bar{Y}\right) -\bar{X}E\left( \hat{\beta}_{1}\right) \\
&=&\left( \beta _{0}+\beta _{1}\bar{x}-\bar{x}\beta _{1}\right) =\beta _{0}
\end{eqnarray*}

\begin{eqnarray*}
Var\left( \hat{\beta}_{1}\right) &=&Var\left(
\tsum\limits_{i=1}^{n}k_{i}Y_{i}\right)
=\tsum\limits_{i=1}^{n}k_{i}^{2}Var\left( Y_{i}\right) \\
&=&\sigma ^{2}\tsum\limits_{i=1}^{n}k_{i}^{2}=\frac{\sigma ^{2}}{\tsum
\left( x_{i}-\bar{x}\right) ^{2}}
\end{eqnarray*}

\bigskip

\begin{eqnarray*}
Var\left( \hat{\beta}_{0}\right) &=&Var\left( \bar{Y}-\hat{\beta}_{1}\bar{X}%
\right) \\
&=&Var\left( \bar{Y}\right) -\bar{X}^{2}Var\left( \hat{\beta}_{1}\right) -2%
\bar{X}Cov\left( \bar{Y},\hat{\beta}_{1}\right) \\
&&\left[ Cov\left( \bar{Y},\hat{\beta}_{1}\right) =0\right] \\
&=&\sigma ^{2}\left[ \frac{1}{n}+\frac{\bar{x}^{2}}{\tsum \left( x_{i}-\bar{x%
}\right) ^{2}}\right]
\end{eqnarray*}

\bigskip

\paragraph{Let}

\begin{equation*}
\varepsilon _{i}\overset{\text{iid}}{\sim }N\left( o,\sigma ^{2}\right)
\end{equation*}

\begin{eqnarray*}
Y_{i} &=&\beta _{0}+\beta _{1}x_{i}+\varepsilon _{i} \\
&\Rightarrow &Y_{i}\sim N\left( \beta _{0}+\beta _{1}x_{i},\sigma ^{2}\right)
\end{eqnarray*}%
\begin{equation*}
\hat{\beta}_{1}=\tsum\limits_{i=1}^{n}k_{i}Y_{i}\sim N\left( \beta _{1},%
\frac{\sigma ^{2}}{\tsum \left( x_{i}-\bar{x}\right) ^{2}}\right)
\end{equation*}%
\begin{equation*}
\hat{\beta}_{0}=\bar{Y}-\hat{\beta}_{1}\bar{X}\sim N\left( \beta _{0},\sigma
^{2}\left( \frac{1}{n}+\frac{\bar{x}^{2}}{\tsum \left( x_{i}-\bar{x}\right)
^{2}}\right) \right)
\end{equation*}

\bigskip

the MLE

the likelihood function%
\begin{equation*}
L\left( \beta _{0},\beta _{1},\sigma ^{2}\right)
=\tprod\limits_{i=1}^{n}\left( \frac{1}{\sqrt{2\pi \sigma ^{2}}}e^{-\frac{%
\left( y_{i}-\beta _{0}-\beta _{1}x_{i}\right) ^{2}}{2\sigma ^{2}}}\right)
\end{equation*}

\bigskip

the $\log $-likelihood function%
\begin{eqnarray*}
l\left( \beta _{0},\beta _{1},\sigma ^{2}\right) &=&-\frac{n}{2}\ln \left(
2\pi \right) -\frac{n}{2}\ln \sigma ^{2} \\
&&-\frac{1}{2\sigma ^{2}}\tsum \left( y_{i}-\beta _{0}-\beta
_{1}x_{i}\right) ^{2}
\end{eqnarray*}

\begin{equation*}
\left\{ 
\begin{array}{c}
\frac{\partial l}{\partial \beta _{0}}=0 \\ 
\frac{\partial l}{\partial \beta _{1}}=0 \\ 
\frac{\partial l}{\partial \sigma ^{2}}=0%
\end{array}%
\right.
\end{equation*}

\begin{equation*}
\Rightarrow \left\{ 
\begin{array}{c}
-2\tsum_{i=1}^{n}\left( y_{i}-\hat{\beta}_{0}-\hat{\beta}_{1}x_{i}\right) =0
\\ 
-2\tsum_{i=1}^{n}\left( y_{i}-\hat{\beta}_{0}-\hat{\beta}_{1}x_{i}\right)
x_{i}=0 \\ 
-\frac{n}{2\hat{\sigma}^{2}}+\frac{1}{2\left( \hat{\sigma}^{2}\right) ^{2}}%
\tsum_{i=1}^{n}\left( y_{i}-\hat{\beta}_{0}-\hat{\beta}_{1}x_{i}\right)
^{2}=0%
\end{array}%
\right.
\end{equation*}

\begin{equation*}
\Rightarrow \left. 
\begin{array}{c}
\hat{\beta}_{0}=\bar{Y}-\hat{\beta}_{1}\bar{X} \\ 
\hat{\beta}_{1}=\frac{S_{xy}}{S_{xx}} \\ 
\hat{\sigma}^{2}=\frac{\tsum \left( y_{i}-\hat{\beta}_{0}-\hat{\beta}%
_{1}x_{i}\right) ^{2}}{n} \\ 
E\left( \hat{\sigma}^{2}\right) \neq \sigma ^{2}%
\end{array}%
\right.
\end{equation*}

\bigskip

\paragraph{Inference}

\begin{enumerate}
\item $\beta _{1}$

\begin{enumerate}
\item $\hat{\beta}_{1}\sim N\left( \beta _{1},\frac{\sigma ^{2}}{\tsum
\left( x_{i}-\bar{x}\right) ^{2}}\right) $

\item $\frac{\left( n-2\right) S^{2}}{\sigma ^{2}}\sim x_{n-1}^{2}$, where $%
S^{2}=\frac{\tsum \left( y_{i}-\hat{\beta}_{0}-\hat{\beta}_{1}x_{i}\right)
^{2}}{n-2}$

\item $\hat{\beta}_{1}$ and $S^{2}$ are indep

\item $\frac{\hat{\beta}_{1}}{\sqrt{\frac{S^{2}}{\tsum \left( x_{i}-\bar{x}%
\right) ^{2}}}}\sim t_{n-2}\qquad \left( \frac{\beta _{1}}{\sqrt{\frac{%
\sigma ^{2}}{\tsum \left( x_{i}-\bar{x}\right) ^{2}}}}\right) $ ,
noncentralizing parameter $\sqrt{\frac{\sigma ^{2}}{\tsum \left( x_{i}-\bar{x%
}\right) ^{2}}}$

\item $H_{0}:\beta _{1}=0$ v.s. $H_{1}:\beta _{1}\neq 0$\newline
$t=\frac{\hat{\beta}_{1}}{S\left( \hat{\beta}_{1}\right) }$, reject $H_{0}$
if $\left\vert t\right\vert >t_{\frac{\alpha }{2}}\left( n-2\right) $

\item A 100\%$\left( 1-\alpha \right) $ C.I.%
\begin{equation*}
\hat{\beta}_{1}\pm t_{\frac{\alpha }{2}}\left( n-2\right) S\left( \hat{\beta}%
_{1}\right)
\end{equation*}%
\begin{equation*}
\hat{\beta}_{0}\pm t_{\frac{\alpha }{2}}\left( n-2\right) S\left( \hat{\beta}%
_{0}\right)
\end{equation*}%
\newline
$H_{0}:\beta _{1}=0$ vs $H_{1}:\beta _{1}\neq 0$%
\begin{equation*}
Y_{i}=\beta _{0}+\beta _{1}x_{i}+\varepsilon _{i}
\end{equation*}%
\begin{equation*}
\hat{\beta}_{0}=\bar{y}+\hat{\beta}_{1}\bar{x}\quad \hat{Y}=\bar{y}
\end{equation*}
\end{enumerate}

\item $\beta _{0}$

\begin{enumerate}
\item $\hat{\beta}_{0}\sim N\left( \beta _{0},\sigma ^{2}\left[ \frac{1}{n}+%
\frac{\bar{x}^{2}}{\tsum \left( x_{i}-\bar{x}\right) ^{2}}\right] \right) $

\item $\hat{\beta}_{0}$ and $S^{2}$ are indep

\item $\frac{\hat{\beta}_{0}}{S\left( \hat{\beta}_{0}\right) }\sim
t_{n-2}\left( \frac{\beta _{0}}{\sigma \diagup \sqrt{\frac{1}{n}+\frac{\bar{x%
}^{2}}{\tsum \left( x_{i}-\bar{x}\right) ^{2}}}}\right) $

\item $H_{0}:\beta _{0}=0$ v.s. $H_{1}:\beta _{0}\neq 0$\newline
$t=\frac{\hat{\beta}_{0}}{S\left( \hat{\beta}_{0}\right) }$, reject $H_{0}$
if $\left\vert t\right\vert >t_{\frac{\alpha }{2}}\left( n-2\right) $
\end{enumerate}
\end{enumerate}

\bigskip

\paragraph{ANOVA Approach}

Partioning of total sum of squares

\begin{eqnarray*}
\underset{\text{SST}}{\tsum\limits_{i=1}^{n}\left( Y_{i}-\bar{Y}\right) ^{2}}
&=&\tsum\limits_{i=1}^{n}\left( Y_{i}-\hat{Y}_{i}+\hat{Y}_{i}-\bar{Y}\right)
^{2} \\
&=&\underset{\text{SSR}}{\tsum\limits_{i=1}^{n}\left( Y_{i}-\hat{Y}%
_{i}\right) ^{2}}+\underset{\text{SSE}}{\tsum\limits_{i=1}^{n}\left( \hat{Y}%
_{i}-\bar{Y}\right) ^{2}}
\end{eqnarray*}%
SSR: regression sum of squares, SSE: error sum of squares

\bigskip 

the coefficient of determination of $R^{2}$ is defined by 
\begin{equation*}
R^{2}=\frac{\text{SSR}}{\text{SST}}\quad R=\sqrt{R^{2}}
\end{equation*}

\end{document}
