
\documentclass{article}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{accents}
\usepackage[ignoreall,a4paper]{geometry}
\usepackage{fancyhdr}

\setcounter{MaxMatrixCols}{10}
%TCIDATA{OutputFilter=LATEX.DLL}
%TCIDATA{Version=5.00.0.2606}
%TCIDATA{<META NAME="SaveForMode" CONTENT="1">}
%TCIDATA{BibliographyScheme=Manual}
%TCIDATA{Created=Wednesday, November 25, 2015 15:33:37}
%TCIDATA{LastRevised=Tuesday, July 12, 2016 16:20:10}
%TCIDATA{<META NAME="GraphicsSave" CONTENT="32">}
%TCIDATA{<META NAME="DocumentShell" CONTENT="Standard LaTeX\Blank - Standard LaTeX Article">}
%TCIDATA{CSTFile=40 LaTeX article.cst}
%TCIDATA{ComputeDefs=
%$W=\left( 1-\sigma \right) I$
%}


\newtheorem{theorem}{Theorem}
\newtheorem{acknowledgement}[theorem]{Acknowledgement}
\newtheorem{algorithm}[theorem]{Algorithm}
\newtheorem{axiom}[theorem]{Axiom}
\newtheorem{case}[theorem]{Case}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{conclusion}[theorem]{Conclusion}
\newtheorem{condition}[theorem]{Condition}
\newtheorem{conjecture}[theorem]{Conjecture}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{criterion}[theorem]{Criterion}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}
\newtheorem{exercise}[theorem]{Exercise}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{notation}[theorem]{Notation}
\newtheorem{problem}[theorem]{Problem}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{solution}[theorem]{Solution}
\newtheorem{summary}[theorem]{Summary}
\newenvironment{proof}[1][Proof]{\noindent\textbf{#1.} }{\ \rule{0.5em}{0.5em}}
\input{../../tcilatex}
\DeclareMathAccent{\wtilde}{\mathord}{largesymbols}{"65}
\pagestyle{fancy}
\fancyfoot[C]{\thepage}
\input{tcilatex}

\begin{document}


\section{One-Way ANOVA Balanced Case}

The one-way balanced model:%
\begin{eqnarray*}
y_{ij} &=&\mu +\alpha _{i}+\varepsilon _{ij}\quad 
\begin{tabular}{l}
$i=1,2,\cdots ,k$ \\ 
$j=1,\cdots ,n$%
\end{tabular}
\\
&=&\mu _{i}+\varepsilon _{ij}
\end{eqnarray*}

the assumptions

\begin{enumerate}
\item $\varepsilon _{ij}\overset{\text{iid}}{\sim }N\left( 0,\sigma
^{2}\right) $

\item $\sum\limits_{i=1}^{k}\alpha _{i}=0$ (side condition)
\end{enumerate}

\bigskip

the matrix form%
\begin{equation*}
\left[ 
\begin{array}{c}
y_{11} \\ 
\vdots \\ 
y_{1n} \\ 
y_{21} \\ 
\vdots \\ 
y_{2n} \\ 
\vdots \\ 
y_{k1} \\ 
\vdots \\ 
y_{kn}%
\end{array}%
\right] =\left[ 
\begin{array}{ccccc}
1 & 1 & 0 &  & 0 \\ 
1 & 1 & 0 & \cdots & \vdots \\ 
1 & 1 & 0 &  & 0 \\ 
1 & 1 & 0 &  &  \\ 
& 0 & 1 &  &  \\ 
\vdots & 0 & 1 & \cdots & \vdots \\ 
& 0 & 1 &  &  \\ 
&  & \vdots &  &  \\ 
1 & 0 & 0 &  & 1 \\ 
&  & \vdots & \cdots &  \\ 
1 & 0 & 0 &  & 1%
\end{array}%
\right] \left[ 
\begin{array}{c}
\mu \\ 
\alpha _{1} \\ 
\alpha _{2} \\ 
\vdots \\ 
\alpha _{k}%
\end{array}%
\right] +\mathbf{\epsilon }
\end{equation*}

the design matrix is $kn\times \left( k+1\right) $ of rank $k$

\begin{equation*}
\mathbf{X}^{\dagger }\mathbf{X}=\left[ 
\begin{array}{cccccc}
kn & n & n &  & \cdots & n \\ 
n & n & n & 0 & \cdots & 0 \\ 
\vdots &  & \vdots &  &  & \vdots \\ 
n & 0 &  &  &  & n%
\end{array}%
\right] _{\left( k+1\right) \times \left( k+1\right) }\quad \mathbf{X}%
^{\dagger }\mathbf{Y}=\left[ 
\begin{array}{c}
y_{\cdot \cdot } \\ 
y_{1\cdot } \\ 
\vdots \\ 
y_{k\cdot }%
\end{array}%
\right]
\end{equation*}

\bigskip

the normal equations can be expressed as%
\begin{eqnarray*}
kn\hat{\mu}+n\hat{\alpha}_{1}+\cdots +n\hat{\alpha}_{k} &=&y_{\cdot \cdot }
\\
n\hat{\mu}+n\hat{\alpha}_{i} &=&y_{i}\quad ,\quad i=1,2,\cdots ,k
\end{eqnarray*}

Using the side condition $\sum \hat{\alpha}_{i}=0$, the solution to normal
equations is given by%
\begin{eqnarray*}
\hat{\mu} &=&\frac{y_{\cdot \cdot }}{kn}=\bar{y}_{\cdot \cdot } \\
\hat{\alpha}_{i} &=&\frac{y_{i\cdot }}{n}-\hat{\mu}=\frac{y_{i\cdot }}{n}-%
\bar{y}_{\cdot \cdot }\quad ,\quad i=1,2,\cdots ,k
\end{eqnarray*}%
\begin{equation*}
S^{2}=\frac{\text{SSE}}{k\left( n-1\right) }=\frac{\mathbf{Y}^{\dagger
}\left( I-P_{\mathbf{X}}\right) \mathbf{Y}}{k\left( n-1\right) }=\frac{%
\mathbf{Y}\left( I-\mathbf{X}\left( \mathbf{X}^{\dagger }\mathbf{X}\right)
^{-}\mathbf{X}^{\dagger }\right) \mathbf{Y}}{k\left( n-1\right) }
\end{equation*}%
is unbiased estimator of $\sigma ^{2}$

\bigskip

\begin{eqnarray*}
\text{SSE} &=&\mathbf{Y}\left( I-\mathbf{X}\left( \mathbf{X}^{\dagger }%
\mathbf{X}\right) ^{-}\mathbf{X}^{\dagger }\right) \mathbf{Y} \\
&=&\mathbf{Y}^{\dagger }\mathbf{Y-Y}^{\dagger }\mathbf{X}\left( \mathbf{X}%
^{\dagger }\mathbf{X}\right) ^{-}\mathbf{X}^{\dagger }\mathbf{Y} \\
&=&\sum \sum y_{ij}^{2}-\mathbf{\hat{\beta}}^{\dagger }\mathbf{X}^{\dagger }%
\mathbf{Y}
\end{eqnarray*}

\begin{eqnarray*}
\mathbf{\hat{\beta}}^{\dagger }\mathbf{X}^{\dagger }\mathbf{Y} &=&\left[ 
\begin{array}{cccc}
\hat{\mu} & \hat{\alpha}_{1} & \cdots & \hat{\alpha}_{k}%
\end{array}%
\right] \left[ 
\begin{array}{c}
y_{\cdot \cdot } \\ 
y_{1\cdot } \\ 
\vdots \\ 
y_{k\cdot }%
\end{array}%
\right] \\
&=&\left[ 
\begin{array}{cccc}
\bar{y}_{\cdot \cdot } & \frac{y_{1\cdot }}{n}-\bar{y}_{\cdot \cdot } & 
\cdots & \frac{y_{k\cdot }}{n}-\bar{y}_{\cdot \cdot }%
\end{array}%
\right] \left[ 
\begin{array}{c}
y_{\cdot \cdot } \\ 
y_{1\cdot } \\ 
\vdots \\ 
y_{k\cdot }%
\end{array}%
\right] \\
&=&\bar{y}_{\cdot \cdot }y_{\cdot \cdot }+\left( \frac{y_{1\cdot }}{n}-\bar{y%
}_{\cdot \cdot }\right) y_{1\cdot }+\cdots +\left( \frac{y_{k\cdot }}{n}-%
\bar{y}_{\cdot \cdot }\right) y_{k\cdot } \\
&=&\sum\limits_{i=1}^{k}\frac{y_{i\cdot }^{2}}{n}
\end{eqnarray*}%
\begin{equation*}
\therefore \text{SSE}=\sum \sum y_{ij}^{2}-\sum_{i=1}^{k}\frac{y_{i\cdot
}^{2}}{n}=\sum_{i=1}^{k}\sum_{j=1}^{k}\left( y_{ij}-\bar{y}_{i\cdot }\right)
^{2}
\end{equation*}

\bigskip

\subsubsection{Testing the hypothesis $H_{0}:\protect\mu _{1}=\cdots =%
\protect\mu _{k}\quad \left( \text{i.e. }H_{0}:\protect\alpha _{1}=\cdots =%
\protect\alpha _{k}\right) $}

\paragraph{The Full-Reduced-Model Approach}

\begin{equation*}
\begin{tabular}{l}
the full model is $y_{ij}=\mu +\alpha _{i}+\varepsilon _{ij}$ \\ 
the reduced model is $y_{ij}=\mu +\varepsilon _{ij}$%
\end{tabular}%
\end{equation*}%
\begin{eqnarray*}
i &=&1,\cdots ,k \\
j &=&1,\cdots ,n
\end{eqnarray*}

\bigskip

the matrix form of R.M. is%
\begin{equation*}
\mathbf{Y}=\mu \underset{\mathbf{X}_{i}}{\fbox{$\mathbf{1}_{kn\times 1}$}}+%
\mathbf{\epsilon }
\end{equation*}

for the R.M., the estimator is%
\begin{equation*}
\hat{\mu}=\left( \mathbf{1}^{\dagger }\mathbf{1}\right) ^{-1}\mathbf{1}%
^{\dagger }\mathbf{Y}=\frac{y_{\cdot \cdot }}{kn}=\bar{y}_{\cdot \cdot }
\end{equation*}%
\begin{equation*}
\hat{\beta}^{\dagger }1^{\dagger }\mathbf{Y=}\left( \bar{y}_{\cdot \cdot
}^{\dagger }\right) y_{\cdot \cdot }=\frac{y_{\cdot \cdot }^{2}}{kn}
\end{equation*}%
\begin{eqnarray*}
&\therefore &\text{SSR}=\mathbf{\hat{\beta}X}^{\dagger }\mathbf{Y-}\hat{\beta%
}^{\dagger }\mathbf{1}^{\dagger }\mathbf{Y=}\sum\limits_{i=1}^{k}\frac{%
y_{i\cdot }^{2}}{n}-\frac{y_{\cdot \cdot }^{2}}{kn} \\
&=&n\sum\limits_{i=1}^{k}\left( \bar{y}_{i\cdot }-\bar{y}_{\cdot \cdot
}\right) ^{2}
\end{eqnarray*}

\bigskip

ANOVA Table

\bigskip

\begin{tabular}{lllll}
S.V. & df & S.S & M.S. & E(M.S.) \\ \hline
treatments & $k-1$ & $\sum\limits_{i=1}^{k}\frac{y_{i\cdot }^{2}}{n}-\frac{%
y_{\cdot \cdot }^{2}}{kn}$ & $\frac{\text{SSR}}{k-1}$ & $\sigma ^{2}+\frac{n%
}{k-1}\sum \alpha _{i}^{2}$ \\ 
Error & $k\left( n-1\right) $ & $\sum \sum y_{ij}^{2}-\sum \frac{y_{i}^{2}}{n%
}$ & $\frac{\text{SSE}}{k\left( n-1\right) }$ & $\sigma ^{2}$ \\ 
Total & $kn-1$ & $\sum \sum y_{ij}^{2}-\frac{y_{\cdot \cdot }^{2}}{n}$ &  & 
\end{tabular}

\bigskip

\begin{equation*}
\text{SSR}=\mathbf{Y}^{\dagger }\left( P_{\mathbf{X}}-P_{\mathbf{W}}\right) 
\mathbf{Y}
\end{equation*}%
\begin{eqnarray*}
P_{\mathbf{X}}-P_{\mathbf{W}} &=&\mathbf{X}\left( \mathbf{X}^{\dagger }%
\mathbf{X}\right) ^{-}\mathbf{X}^{\dagger }-\mathbf{1}_{nk}\left( \mathbf{1}%
^{\dagger }\mathbf{1}\right) ^{-}\mathbf{1}^{\dagger } \\
&=&\frac{1}{n}\left[ 
\begin{array}{cccc}
J_{n\times n} & 0 & \cdots  & 0 \\ 
0 & J &  & 0 \\ 
\vdots  &  & \ddots  & \vdots  \\ 
J &  & \cdots  & J%
\end{array}%
\right] -\frac{1}{kn}\left[ 
\begin{array}{ccc}
J_{n\times n} & \cdots  & J_{n\times n} \\ 
\vdots  & \ddots  & \vdots  \\ 
J_{n\times n} & \cdots  & J_{n\times n}%
\end{array}%
\right]  \\
&=&\frac{1}{kn}\left[ 
\begin{array}{cccc}
\left( k-1\right) J & -J & \cdots  & -J \\ 
-J & \left( k-1\right) J &  & \vdots  \\ 
\vdots  & \vdots  & \ddots  & \vdots  \\ 
-J & -J & \cdots  & \left( k-1\right) J%
\end{array}%
\right] 
\end{eqnarray*}

\begin{eqnarray*}
E\left( \text{SSR}\right)  &=&E\left( \mathbf{Y}^{\dagger }\left( P_{\mathbf{%
X}}-P_{\mathbf{W}}\right) \mathbf{Y}\right)  \\
&=&tr\left( \left( P_{\mathbf{X}}-P_{\mathbf{W}}\right) \sigma ^{2}I\right)
+\left( \mathbf{X\beta }\right) ^{\dagger }\left( P_{\mathbf{X}}-P_{\mathbf{W%
}}\right) \mathbf{X\beta }
\end{eqnarray*}%
\begin{eqnarray*}
tr\left( P_{\mathbf{X}}-P_{\mathbf{W}}\right)  &=&tr\left( \mathbf{X}\left( 
\mathbf{X}^{\dagger }\mathbf{X}\right) ^{-}\mathbf{X}^{\dagger }-\mathbf{1}%
\left( \mathbf{1}^{\dagger }\mathbf{1}\right) ^{-}\mathbf{1}^{\dagger
}\right)  \\
&=&tr\left( \mathbf{X}\left( \mathbf{X}^{\dagger }\mathbf{X}\right) ^{-}%
\mathbf{X}^{\dagger }\right) -tr\left( \frac{1}{kn}J_{nk}\right)  \\
&=&r\left( \mathbf{X}\right) -\frac{1}{kn}tr\left( J_{nk}\right)  \\
&=&k-1
\end{eqnarray*}

\begin{eqnarray*}
&&\left( \mathbf{X\beta }\right) ^{\dagger }\left( P_{\mathbf{X}}-P_{\mathbf{%
W}}\right) \mathbf{X\beta } \\
&=&\left( \mathbf{X\beta }\right) ^{\dagger }P_{\mathbf{X}}\mathbf{X\beta -}%
\left( \mathbf{X\beta }\right) ^{\dagger }P_{\mathbf{W}}\mathbf{X\beta \quad
\_\_\_}\left( \ast \right) 
\end{eqnarray*}%
\begin{eqnarray*}
\left( \mathbf{X\beta }\right) ^{\dagger }P_{\mathbf{X}}\mathbf{X\beta } &=&%
\mathbf{\mathbf{\beta }^{\dagger }\mathbf{X^{\dagger }}X}\left( \mathbf{X}%
^{\dagger }\mathbf{X}\right) ^{-}\mathbf{X}^{\dagger }\mathbf{X\beta } \\
&=&\mathbf{\mathbf{\beta }^{\dagger }}\left( \mathbf{\mathbf{X^{\dagger }}X}%
\right) \mathbf{\beta } \\
&=&\left[ 
\begin{array}{cccc}
\mu  & \alpha _{1} & \cdots  & \alpha _{k}%
\end{array}%
\right] \left[ 
\begin{array}{cccc}
kn & n & \cdots  & n \\ 
n & n &  & 0 \\ 
\vdots  &  & \ddots  &  \\ 
n & 0 &  & n%
\end{array}%
\right] \left[ 
\begin{array}{c}
\mu  \\ 
\alpha _{1} \\ 
\vdots  \\ 
\alpha _{k}%
\end{array}%
\right]  \\
&=&n\left[ 
\begin{array}{cccc}
k\mu +\sum\limits_{i=1}^{k}\alpha _{i} & \mu +\alpha _{1} & \cdots  & \mu
+\alpha _{k}%
\end{array}%
\right] \left[ 
\begin{array}{c}
\mu  \\ 
\alpha _{1} \\ 
\vdots  \\ 
\alpha _{k}%
\end{array}%
\right]  \\
&=&n\left[ k\mu ^{2}+\mu \sum \alpha _{i}+\sum\limits_{i=1}^{k}\left( \mu
+\alpha _{i}\right) \alpha _{i}\right]  \\
&=&kn\mu ^{2}+n\sum_{i=1}^{k}\alpha _{i}^{2}\mathbf{\quad \_\_\_}\left( \ast
\right) 
\end{eqnarray*}%
\begin{eqnarray*}
\left( \mathbf{X\beta }\right) ^{\dagger }P_{\mathbf{W}}\mathbf{X\beta } &=&%
\mathbf{\mathbf{\beta }^{\dagger }\mathbf{X^{\dagger }}}1\left( 1^{\dagger
}1\right) ^{-1}1^{\dagger }\mathbf{X\beta } \\
&=&\mathbf{\mathbf{\beta }^{\dagger }\mathbf{X^{\dagger }}}1\frac{1}{kn}%
1^{\dagger }\mathbf{X\beta } \\
&=&\frac{1}{kn}\left( 1^{\dagger }\mathbf{X\beta }\right) ^{2}
\end{eqnarray*}

\begin{eqnarray*}
\mathbf{1}_{nk\times 1}^{\dagger }\mathbf{X\beta } &\mathbf{=}&\left[ 
\begin{array}{ccc}
\mathbf{1}_{n}^{\dagger } & \mathbf{\cdots } & \mathbf{1}_{n}^{\dagger }%
\end{array}%
\right] \left[ 
\begin{array}{ccccc}
\mathbf{1}_{n} & \mathbf{1}_{n} & 0 & \cdots  & 0 \\ 
&  & \mathbf{1}_{n} &  &  \\ 
\vdots  &  &  & \ddots  & \vdots  \\ 
\mathbf{1}_{n} &  &  & \cdots  & \mathbf{1}_{n}%
\end{array}%
\right] \left[ 
\begin{array}{c}
\mu  \\ 
\alpha _{1} \\ 
\vdots  \\ 
\alpha _{k}%
\end{array}%
\right]  \\
&=&\left[ 
\begin{array}{cccc}
kn & n & \cdots  & n%
\end{array}%
\right] \left[ 
\begin{array}{c}
\mu  \\ 
\alpha _{1} \\ 
\vdots  \\ 
\alpha _{k}%
\end{array}%
\right]  \\
&=&kn\mu +n\sum\limits_{i=1}^{k}\alpha _{i}=kn\mu 
\end{eqnarray*}%
\begin{equation*}
\therefore \left( \mathbf{X\beta }\right) ^{\dagger }P_{\mathbf{W}}\mathbf{%
X\beta }=\frac{1}{kn}\left( 1^{\dagger }\mathbf{X\beta }\right) ^{2}=\frac{1%
}{kn}\left( kn\mu \right) ^{2}=kn\mu ^{2}\mathbf{\_\_\_}\left( \ast \right) 
\end{equation*}

\bigskip 

\begin{equation*}
\left( \mathbf{X\beta }\right) ^{\dagger }\left( P_{\mathbf{X}}-P_{\mathbf{W}%
}\right) \mathbf{X\beta }=kn\mu ^{2}+n\sum_{i=1}^{k}\alpha _{i}^{2}-kn\mu
^{2}=n\sum_{i=1}^{k}\alpha _{i}^{2}
\end{equation*}

\begin{equation*}
\therefore E\left( \text{SSR}\right) =\sigma ^{2}\left( k-1\right)
+n\sum\limits_{i=1}^{k}\alpha _{i}^{2}
\end{equation*}

\end{document}
