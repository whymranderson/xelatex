
\documentclass{article}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{accents}
\usepackage[ignoreall,a4paper]{geometry}
\usepackage{fancyhdr}

\setcounter{MaxMatrixCols}{10}
%TCIDATA{OutputFilter=LATEX.DLL}
%TCIDATA{Version=5.00.0.2606}
%TCIDATA{<META NAME="SaveForMode" CONTENT="1">}
%TCIDATA{BibliographyScheme=Manual}
%TCIDATA{Created=Wednesday, November 25, 2015 15:33:37}
%TCIDATA{LastRevised=Wednesday, May 11, 2016 17:15:32}
%TCIDATA{<META NAME="GraphicsSave" CONTENT="32">}
%TCIDATA{<META NAME="DocumentShell" CONTENT="Standard LaTeX\Blank - Standard LaTeX Article">}
%TCIDATA{CSTFile=40 LaTeX article.cst}
%TCIDATA{ComputeDefs=
%$W=\left( 1-\sigma \right) I$
%}


\newtheorem{theorem}{Theorem}
\newtheorem{acknowledgement}[theorem]{Acknowledgement}
\newtheorem{algorithm}[theorem]{Algorithm}
\newtheorem{axiom}[theorem]{Axiom}
\newtheorem{case}[theorem]{Case}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{conclusion}[theorem]{Conclusion}
\newtheorem{condition}[theorem]{Condition}
\newtheorem{conjecture}[theorem]{Conjecture}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{criterion}[theorem]{Criterion}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}
\newtheorem{exercise}[theorem]{Exercise}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{notation}[theorem]{Notation}
\newtheorem{problem}[theorem]{Problem}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{solution}[theorem]{Solution}
\newtheorem{summary}[theorem]{Summary}
\newenvironment{proof}[1][Proof]{\noindent\textbf{#1.} }{\ \rule{0.5em}{0.5em}}
\input{../../tcilatex}
\DeclareMathAccent{\wtilde}{\mathord}{largesymbols}{"65}
\pagestyle{fancy}
\fancyfoot[C]{\thepage}



\begin{document}


\setcounter{part}{1} \setcounter{page}{1}

\subsubsection{Aitken Model and generalized least squares}

\begin{theorem}
Let $\mathbb{Y}=\mathbb{X}\boldsymbol{\beta }+\boldsymbol{\varepsilon }$,
where $E\left( \boldsymbol{\varepsilon }\right) =\boldsymbol{0}$, and $%
Var\left( \boldsymbol{\varepsilon }\right) =\sigma ^{2}\mathbb{V}$, $\mathbb{%
X}$ is a full-rank matrix and $\mathbb{V}$ is a known positive definite
matrix.

\begin{enumerate}
\item $\hat{\beta}_{GLS}=\left( \mathbb{X}^{\dagger }V^{-1}\mathbb{X}\right)
^{-1}\mathbb{X}^{\dagger }V^{-1}\mathbb{Y}$ is the BLUE of $\boldsymbol{%
\beta }$

\item $Var\left( \hat{\beta}_{GLS}\right) =\sigma ^{2}\left( \mathbb{X}%
^{\dagger }V^{-1}\mathbb{X}\right) ^{-1}$

\item An unbiased estimator of $\sigma ^{2}$ is%
\begin{equation*}
S^{2}=\frac{\left( \mathbb{Y-X}\hat{\beta}_{GLS}\right) ^{\dagger
}V^{-1}\left( \mathbb{Y-X}\hat{\beta}_{GLS}\right) }{n-p}
\end{equation*}%
\newline
\newline
\end{enumerate}

\begin{proof}
\newline
tool: Let $\mathbb{V}$ be a P.D matrix, then $\exists $ $\mathbb{P}_{n\times
n}$ nonsingular matrix s.t \fbox{$\mathbb{PP}^{\dagger }=\mathbb{V}$}

\begin{enumerate}
\item $\mathbb{Y}=\mathbb{X}\boldsymbol{\beta }+\boldsymbol{\varepsilon }$%
\begin{eqnarray*}
&\Rightarrow &\mathbb{P}^{-1}\mathbb{Y}=\mathbb{P}^{-1}\mathbb{X}\boldsymbol{%
\beta }+\mathbb{P}^{-1}\boldsymbol{\varepsilon } \\
&\Rightarrow &\mathbb{Y}^{\ast }=\mathbb{X}^{\ast }\boldsymbol{\beta }+%
\boldsymbol{\varepsilon }^{\ast }
\end{eqnarray*}%
$E\left( \mathbb{P}^{-1}\boldsymbol{\varepsilon }\right) =\boldsymbol{0}$%
\newline
\begin{eqnarray*}
Var\left( \mathbb{P}^{-1}\boldsymbol{\varepsilon }\right) &=&\mathbb{P}%
^{-1}Var\left( \boldsymbol{\varepsilon }\right) \left( \mathbb{P}%
^{-1}\right) ^{\dagger } \\
&=&\sigma ^{2}P^{-1}V\left( P^{-1}\right) ^{\dagger } \\
&=&\sigma ^{2}\mathbb{P}^{-1}\mathbb{PP}^{\dagger }\left( \mathbb{P}%
^{\dagger }\right) ^{-1} \\
&=&\sigma ^{2}I
\end{eqnarray*}%
$\therefore $OLS is%
\begin{eqnarray*}
\boldsymbol{\hat{\beta}} &\boldsymbol{=}&\left( \left( \mathbb{P}^{-1}%
\mathbb{X}\right) ^{\dagger }\left( \mathbb{P}^{-1}\mathbb{X}\right) \right)
^{-1}\left( \mathbb{P}^{-1}\mathbb{X}\right) ^{\dagger }\mathbb{P}^{-1}%
\mathbb{Y} \\
&&\vdots \\
&=&\left( \mathbb{X}^{\dagger }V^{-1}\mathbb{X}\right) ^{-1}\mathbb{X}%
^{\dagger }\mathbb{V}^{-1}\mathbb{Y} \\
&\hookrightarrow &\text{generalized least squares}
\end{eqnarray*}%
\newline
\newline
$Var\left( P^{-1}\varepsilon \right) =Cov\left( P^{-1}\varepsilon
,P^{-1}\varepsilon \right) =P^{-1}\fbox{$\quad $}\left( P^{-1}\right)
^{\dagger }$

\item 
\begin{eqnarray*}
&&\left( \mathbb{Y-X}\hat{\beta}\right) ^{\dagger }V^{-1}\left( \mathbb{Y-X}%
\hat{\beta}\right) \\
&=&\left( \mathbb{Y-X}\left( \mathbb{X}^{\dagger }V^{-1}\mathbb{X}\right)
^{-1}\mathbb{X}^{\dagger }\mathbb{V}^{-1}\mathbb{Y}\right) ^{\dagger }V^{-1}
\\
&&\left( \mathbb{Y-X}\left( \mathbb{X}^{\dagger }V^{-1}\mathbb{X}\right)
^{-1}\mathbb{X}^{\dagger }\mathbb{V}^{-1}\mathbb{Y}\right) \\
&=&\left[ \left( I-\left( \ast \right) \right) \mathbb{Y}\right] ^{\dagger
}V^{-1}\left[ \left( I-\left( \ast \right) \right) \mathbb{Y}\right]
\end{eqnarray*}%
where $(\ast )=\mathbb{X}\left( \mathbb{X}^{\dagger }V^{-1}\mathbb{X}\right)
^{-1}\mathbb{X}^{\dagger }$%
\begin{eqnarray*}
&=&\mathbb{Y}^{\dagger }\left[ V^{-1}-\left( \ast \right) V^{-1}\right] %
\left[ I-\left( \ast \right) \right] \mathbb{Y} \\
&&\vdots \\
&=&\mathbb{Y}^{\dagger }\left[ V^{-1}-V^{-1}\mathbb{X}\left( \mathbb{X}%
^{\dagger }V^{-1}\mathbb{X}\right) ^{-1}\mathbb{X}^{\dagger }V^{-1}\right] 
\mathbb{Y}
\end{eqnarray*}%
\newline
\newline
\begin{eqnarray*}
&\therefore &E\left( \mathbb{Y}^{\dagger }\left[ V^{-1}-V^{-1}\mathbb{X}%
\left( \mathbb{X}^{\dagger }V^{-1}\mathbb{X}\right) ^{-1}\mathbb{X}^{\dagger
}V^{-1}\right] \mathbb{Y}\right) \\
&=&tr\left( \sigma ^{2}V\left[ V^{-1}-V^{-1}\mathbb{X}\left( \mathbb{X}%
^{\dagger }V^{-1}\mathbb{X}\right) ^{-1}\mathbb{X}^{\dagger }V^{-1}\right]
+\right. \\
&&\left( \mathbb{X}\beta \right) ^{\dagger }\left[ V^{-1}-V^{-1}\mathbb{X}%
\left( \mathbb{X}^{\dagger }V^{-1}\mathbb{X}\right) ^{-1}\mathbb{X}^{\dagger
}V^{-1}\right] \left( \mathbb{X}\beta \right) \\
&=&\sigma ^{2}tr\left( V\left[ V^{-1}-V^{-1}\mathbb{X}\left( \mathbb{X}%
^{\dagger }V^{-1}\mathbb{X}\right) ^{-1}\mathbb{X}^{\dagger }V^{-1}\right]
\right) +0 \\
&=&\sigma ^{2}\left[ \underset{tr\left( I_{n}\right) }{\underbrace{n}}%
-tr\left( I_{pp}\right) \right] =\sigma ^{2}\left( n-p\right)
\end{eqnarray*}%
\newline
\newline
\begin{equation*}
E\left( \mathbb{Y}^{\dagger }A\mathbb{Y}\right) =tr\left( A\tsum \right)
+\mu ^{\dagger }A\mu
\end{equation*}
\end{enumerate}
\end{proof}
\end{theorem}

\bigskip

Note:

\begin{enumerate}
\item If $V=diag\left( v_{1},\cdots ,v_{m}\right) $, then%
\begin{eqnarray*}
Q\left( \boldsymbol{\beta }\right) &=&\left( \mathbb{Y-X}\boldsymbol{\beta }%
\right) ^{\dagger }V^{-1}\left( \mathbb{Y-X}\boldsymbol{\beta }\right) \\
&=&\tsum\limits_{i=1}^{n}\mathcal{W}_{i}\left( y_{i}-x_{i}^{\dagger }%
\boldsymbol{\beta }\right) ^{2}
\end{eqnarray*}%
where $\mathcal{W}_{i}=\frac{1}{v_{i}}$ and $x_{i}$ is the ith row of $%
\mathbb{X}$. $\hat{\beta}$ is called \textbf{weighted\ least squares
estimator}.

\item $\star $ If $l^{\dagger }\boldsymbol{\beta }$ is estimator, then $%
l^{\dagger }\boldsymbol{\hat{\beta}}$ is the BLUE for $l^{\dagger }%
\boldsymbol{\beta }$.\newline
\newline
\end{enumerate}

\begin{example}
$\mathbb{Y}=\mathbb{X}\boldsymbol{\beta }+\boldsymbol{\varepsilon }$, where%
\begin{eqnarray*}
Var\left( \boldsymbol{\varepsilon }\right) &=&\sigma ^{2}\left[ 
\begin{array}{ccc}
1 &  & \sigma \\ 
& \ddots &  \\ 
\sigma &  & 1%
\end{array}%
\right] \\
&=&\left[ 
\begin{array}{cc}
\boldsymbol{1} & \mathbb{X}_{c}%
\end{array}%
\right] \left[ 
\begin{array}{c}
\alpha \\ 
\boldsymbol{\beta }_{1}%
\end{array}%
\right] +\boldsymbol{\varepsilon }
\end{eqnarray*}%
\begin{equation*}
V=\sigma ^{2}\left[ 
\begin{array}{ccc}
v_{11} & v_{12} &  \\ 
& \ddots &  \\ 
&  & 
\end{array}%
\right] _{n\times n}
\end{equation*}%
\begin{eqnarray*}
\boldsymbol{\hat{\beta}} &=&\left( \mathbb{X}^{\dagger }V^{-1}\mathbb{X}%
\right) ^{-1}\mathbb{X}^{\dagger }V^{-1}\mathbb{Y} \\
&=&\left[ 
\begin{array}{cc}
bn & \boldsymbol{0}^{\dagger } \\ 
\boldsymbol{0} & a\mathbb{X}_{c}^{\dagger }\mathbb{X}_{c}%
\end{array}%
\right] ^{-1}\left[ 
\begin{array}{c}
bn\bar{y} \\ 
a\mathbb{X}_{c}^{\dagger }\mathbb{Y}%
\end{array}%
\right] \\
&=&\left[ 
\begin{array}{cc}
\frac{1}{bn} & \boldsymbol{0}^{\dagger } \\ 
\boldsymbol{0} & \frac{1}{a}\left( \mathbb{X}_{c}^{\dagger }\mathbb{X}%
_{c}\right) ^{-1}%
\end{array}%
\right] \left[ 
\begin{array}{c}
bn\bar{y} \\ 
a\mathbb{X}_{c}^{\dagger }\mathbb{Y}%
\end{array}%
\right] \\
&=&\left[ 
\begin{array}{c}
\bar{y} \\ 
\left( \mathbb{X}_{c}^{\dagger }\mathbb{X}_{c}\right) ^{-1}\mathbb{X}%
_{c}^{\dagger }\mathbb{Y}%
\end{array}%
\right]
\end{eqnarray*}%
$\therefore $ the usual least squares estimators are BLUE for a covariance
structure with equal variances and correlations.
\end{example}

\bigskip

\paragraph{Misspecification of the Error Structure}

\begin{equation*}
\mathbb{Y}=\mathbb{X}\boldsymbol{\beta }+\boldsymbol{\varepsilon }\quad
,\quad E\left( \boldsymbol{\varepsilon }\right) =\boldsymbol{0}\quad ,\quad
Var\left( \boldsymbol{\varepsilon }\right) =\sigma ^{2}V\Leftrightarrow
\sigma ^{2}I
\end{equation*}

Consider%
\begin{equation*}
\boldsymbol{\hat{\beta}}_{OLS}=\left( \mathbb{X}^{\dagger }\mathbb{X}\right)
^{-1}\mathbb{X}^{\dagger }\mathbb{Y}
\end{equation*}%
\begin{eqnarray*}
E\left( \boldsymbol{\hat{\beta}}_{OLS}\right) &=&E\left( \left( \mathbb{X}%
^{\dagger }\mathbb{X}\right) ^{-1}\mathbb{X}^{\dagger }\mathbb{Y}\right) \\
&=&\left( \mathbb{X}^{\dagger }\mathbb{X}\right) ^{-1}\mathbb{X}^{\dagger
}E\left( \mathbb{Y}\right) \\
&=&\left( \mathbb{X}^{\dagger }\mathbb{X}\right) ^{-1}\mathbb{X}^{\dagger }%
\mathbb{X}\boldsymbol{\beta } \\
&=&\boldsymbol{\beta }
\end{eqnarray*}%
\begin{eqnarray*}
Var\left( \boldsymbol{\hat{\beta}}_{OLS}\right) &=&Var\left( \left( \mathbb{X%
}^{\dagger }\mathbb{X}\right) ^{-1}\mathbb{X}^{\dagger }\mathbb{Y}\right) \\
&=&Var\left( A\mathbb{Y}\right) \\
&=&\left( \mathbb{X}^{\dagger }\mathbb{X}\right) ^{-1}\mathbb{X}^{\dagger
}Var\left( \mathbb{Y}\right) \left[ \left( \mathbb{X}^{\dagger }\mathbb{X}%
\right) ^{-1}\mathbb{X}^{\dagger }\right] ^{\dagger } \\
&=&\fbox{$\left( \mathbb{X}^{\dagger }\mathbb{X}\right) ^{-1}\mathbb{X}%
^{\dagger }\sigma ^{2}V\mathbb{X}\left( \mathbb{X}^{\dagger }\mathbb{X}%
\right) ^{-1}$}\geq Var\left( \boldsymbol{\hat{\beta}}_{OLS}\right)
\end{eqnarray*}

\bigskip

$\frac{\hat{\theta}-E\left( \hat{\theta}\right) }{\sqrt{Var\left( \hat{\theta%
}\right) }}\quad \frac{\hat{\theta}}{\sqrt{Var\left( \hat{\theta}\right) }}$

\bigskip

\paragraph{Model Misspecification}

\begin{equation*}
\mathbb{Y}=\mathbb{X}\boldsymbol{\beta }+\boldsymbol{\varepsilon }\quad
,\quad E\left( \boldsymbol{\varepsilon }\right) =\boldsymbol{0}\quad ,\text{
and }Var\left( \boldsymbol{\varepsilon }\right) =\sigma ^{2}I
\end{equation*}%
\begin{eqnarray*}
\mathbb{Y} &=&\mathbb{X}\boldsymbol{\beta }+\boldsymbol{\varepsilon } \\
&=&\left[ 
\begin{array}{cc}
\mathbb{X}_{1} & \mathbb{X}_{2}%
\end{array}%
\right] \left[ 
\begin{array}{c}
\boldsymbol{\beta }_{1} \\ 
\boldsymbol{\beta }_{2}%
\end{array}%
\right] +\boldsymbol{\varepsilon } \\
&=&\mathbb{X}_{1}\boldsymbol{\beta }_{1}+\mathbb{X}_{2}\boldsymbol{\beta }%
_{2}+\boldsymbol{\varepsilon }\quad \quad \text{full}
\end{eqnarray*}

\bigskip

\begin{theorem}
(underfitting) If we fit the model $\mathbb{Y}=\mathbb{X}_{1}\boldsymbol{%
\beta }_{1}+\boldsymbol{\varepsilon }$ when the correct model is $\mathbb{Y}=%
\mathbb{X}_{1}\boldsymbol{\beta }_{1}+\mathbb{X}_{2}\boldsymbol{\beta }_{2}+%
\boldsymbol{\varepsilon }$, then

\begin{enumerate}
\item $E\left( \boldsymbol{\hat{\beta}}_{1}^{\ast }\right) =\boldsymbol{%
\beta }_{1}+\mathbb{A}\boldsymbol{\beta }_{2}$, where $\mathbb{A}=\left( 
\mathbb{X}_{1}^{\dagger }\mathbb{X}\right) ^{-1}\mathbb{X}_{1}\mathbb{X}_{2}$

\item $Var\left( \boldsymbol{\hat{\beta}}_{1}^{\ast }\right) =\sigma
^{2}\left( \mathbb{X}_{1}^{\dagger }\mathbb{X}\right) ^{-1}$

\item $Var\left( \boldsymbol{\hat{\beta}}_{1}\right) \underset{\geq }{-}%
Var\left( \boldsymbol{\hat{\beta}}_{1}^{\ast }\right) =\sigma ^{2}\mathbb{AB}%
^{-1}\mathbb{A}^{\dagger }$ which is P.D. matrix
\end{enumerate}

\begin{proof}
\begin{enumerate}
\item 
\begin{eqnarray*}
E\left( \boldsymbol{\hat{\beta}}_{1}^{\ast }\right) &=&E\left( \left( 
\mathbb{X}_{1}^{\dagger }\mathbb{X}_{1}\right) ^{-1}\mathbb{X}_{1}^{\dagger }%
\mathbb{Y}\right) \\
&=&\left( \mathbb{X}_{1}^{\dagger }\mathbb{X}_{1}\right) ^{-1}\mathbb{X}%
_{1}^{\dagger }E\left( \mathbb{Y}\right) \\
&=&\left( \mathbb{X}_{1}^{\dagger }\mathbb{X}_{1}\right) ^{-1}\mathbb{X}%
_{1}^{\dagger }\mathbb{X}\boldsymbol{\beta } \\
&=&\left( \mathbb{X}_{1}^{\dagger }\mathbb{X}_{1}\right) ^{-1}\mathbb{X}%
_{1}^{\dagger }\left( \mathbb{X}_{1}\boldsymbol{\beta }_{1}+\mathbb{X}_{2}%
\boldsymbol{\beta }_{2}\right) \\
&=&\boldsymbol{\beta }_{1}+\left( \mathbb{X}_{1}^{\dagger }\mathbb{X}%
_{1}\right) ^{-1}\underset{0}{\underbrace{\mathbb{X}_{1}^{\dagger }\mathbb{X}%
_{2}}}\boldsymbol{\beta }_{2}\quad \quad \text{bias}
\end{eqnarray*}

\item 
\begin{eqnarray*}
Var\left( \boldsymbol{\hat{\beta}}_{1}^{\ast }\right) &=&Var\left( 
\underline{\left( \mathbb{X}_{1}^{\dagger }\mathbb{X}_{1}\right) ^{-1}%
\mathbb{X}_{1}^{\dagger }}\mathbb{Y}\right) \\
&=&\underline{\left( \mathbb{X}_{1}^{\dagger }\mathbb{X}_{1}\right) ^{-1}%
\mathbb{X}_{1}^{\dagger }}\underset{\sigma ^{2}I}{\underbrace{Var\left( 
\mathbb{Y}\right) }}\underline{\left[ \left( \mathbb{X}_{1}^{\dagger }%
\mathbb{X}_{1}\right) ^{-1}\mathbb{X}_{1}^{\dagger }\right] ^{\dagger }} \\
&=&\sigma ^{2}\left( \mathbb{X}_{1}^{\dagger }\mathbb{X}_{1}\right) ^{-1}%
\mathbb{X}_{1}^{\dagger }\mathbb{X}_{1}\left( \mathbb{X}_{1}^{\dagger }%
\mathbb{X}_{1}\right) ^{-1} \\
&=&\sigma ^{2}\left( \mathbb{X}_{1}^{\dagger }\mathbb{X}_{1}\right) ^{-1}
\end{eqnarray*}%
\begin{eqnarray*}
Var\left( \boldsymbol{\hat{\beta}}\right) &=&\sigma ^{2}\left( \mathbb{X}%
^{\dagger }\mathbb{X}\right) ^{-1} \\
&=&\sigma ^{2}\left[ \left[ 
\begin{array}{cc}
\mathbb{X}_{1} & \mathbb{X}_{2}%
\end{array}%
\right] ^{\dagger }\left[ 
\begin{array}{cc}
\mathbb{X}_{1} & \mathbb{X}_{2}%
\end{array}%
\right] \right] ^{-1} \\
&=&\sigma ^{2}\left[ \left[ 
\begin{array}{c}
\mathbb{X}_{1}^{\dagger } \\ 
\mathbb{X}_{2}^{\dagger }%
\end{array}%
\right] \left[ 
\begin{array}{cc}
\mathbb{X}_{1} & \mathbb{X}_{2}%
\end{array}%
\right] \right] ^{-1} \\
&=&\sigma ^{2}\left[ 
\begin{array}{cc}
\mathbb{X}_{1}^{\dagger }\mathbb{X}_{1} & \mathbb{X}_{1}^{\dagger }\mathbb{X}%
_{2} \\ 
\mathbb{X}_{2}^{\dagger }\mathbb{X}_{1} & \mathbb{X}_{2}^{\dagger }\mathbb{X}%
_{2}%
\end{array}%
\right] ^{-1} \\
&=&\sigma ^{2}\left[ 
\begin{array}{cc}
G_{11} & G_{12} \\ 
G_{21} & G_{22}%
\end{array}%
\right] ^{-1} \\
&=&\sigma ^{2}\left[ 
\begin{array}{cc}
G^{11} & G^{12} \\ 
G^{21} & G^{22}%
\end{array}%
\right]
\end{eqnarray*}%
tools:%
\begin{eqnarray*}
&&\left[ 
\begin{array}{cc}
\mathbb{A}_{11} & \mathbb{A}_{12} \\ 
\mathbb{A}_{21} & \mathbb{A}_{22}%
\end{array}%
\right] ^{-1} \\
&=&\left[ 
\begin{array}{cc}
\mathbb{A}_{11}^{-1}+\mathbb{A}_{11}^{-1}\mathbb{A}_{12}\mathbb{D}^{-1}%
\mathbb{A}_{21}\mathbb{A}_{11}^{-1} & -\mathbb{A}_{11}^{-1}\mathbb{A}_{12}%
\mathbb{D}^{-1} \\ 
-\mathbb{D}^{-1}\mathbb{A}_{21}\mathbb{A}_{11}^{-1} & \mathbb{D}^{-1}%
\end{array}%
\right]
\end{eqnarray*}%
where $D=\mathbb{A}_{22}-\mathbb{A}_{21}\mathbb{A}_{11}^{-1}\mathbb{A}_{12}$%
\begin{eqnarray*}
G^{11} &=&\left( \mathbb{X}_{1}^{\dagger }\mathbb{X}_{1}\right) ^{-1}\mathbb{%
+} \\
&&\left( \mathbb{X}_{1}^{\dagger }\mathbb{X}_{1}\right) \mathbb{X}%
_{1}^{\dagger }\mathbb{X}_{2}\left[ \mathbb{X}_{2}^{\dagger }\mathbb{X}_{2}-%
\mathbb{X}_{2}\mathbb{X}_{1}\left( \mathbb{X}_{1}^{\dagger }\mathbb{X}%
\right) ^{-1}\mathbb{X}_{1}\mathbb{X}_{2}\right] ^{-1}\times \\
&&\mathbb{X}_{2}^{\dagger }\mathbb{X}_{1}\left( \mathbb{X}_{1}^{\dagger }%
\mathbb{X}_{1}\right) ^{-1}
\end{eqnarray*}%
\newline
\newline
\begin{equation*}
\left. 
\begin{array}{c}
Var\left( \boldsymbol{\hat{\beta}}_{1}\right) =\sigma ^{2}G^{11} \\ 
Var\left( \boldsymbol{\hat{\beta}}_{1}^{\ast }\right) =\sigma ^{2}\left( 
\mathbb{X}_{1}^{\dagger }\mathbb{X}_{1}\right) ^{-1}%
\end{array}%
\right]
\end{equation*}%
\newline
\newline
\begin{eqnarray*}
&\therefore &Var\left( \boldsymbol{\hat{\beta}}_{1}\right) -Var\left( 
\boldsymbol{\hat{\beta}}_{1}^{\ast }\right) =\sigma ^{2}\left[ \left( 
\mathbb{X}_{1}^{\dagger }\mathbb{X}\right) ^{-1}+\mathbb{A}B^{-1}\mathbb{A}%
^{\dagger }\right] \\
&&-\sigma ^{2}\left( \mathbb{X}_{1}^{\dagger }\mathbb{X}\right) ^{-1} \\
&=&\sigma ^{2}\underset{\text{P.D. why?}}{\fbox{$\mathbb{AB}^{-1}\mathbb{A}%
^{\dagger }$}}
\end{eqnarray*}

\item 
\begin{eqnarray*}
E\left( S_{1}^{2}\right) &=&\sigma ^{2}+ \\
&&\frac{\boldsymbol{\beta }_{2}^{\dagger }\mathbb{X}_{2}^{\dagger }\left[ I-%
\mathbb{X}_{1}\left( \mathbb{X}_{1}^{\dagger }\mathbb{X}_{1}\right) ^{-1}%
\mathbb{X}_{1}^{\dagger }\right] \mathbb{X}_{2}\boldsymbol{\beta }_{2}}{%
n-P_{1}}
\end{eqnarray*}%
where $S_{1}^{2}=\frac{\left( \mathbb{Y}-\mathbb{X}_{1}\boldsymbol{\hat{\beta%
}}_{1}^{\ast }\right) ^{\dagger }\left( \mathbb{Y}-\mathbb{X}_{1}\boldsymbol{%
\hat{\beta}}_{1}^{\ast }\right) }{n-P_{1}}=\frac{\text{SSE}_{1}}{n-P_{1}}$
and $\gamma \left( \mathbb{X}_{1}\right) =P_{1}$

\begin{proof}
\begin{eqnarray*}
E\left( \text{SSE}_{1}\right) &=&E\left( \mathbb{Y}^{\dagger }\left[ I-%
\mathbb{X}_{1}\left( \mathbb{X}_{1}^{\dagger }\mathbb{X}_{1}\right) ^{-1}%
\mathbb{X}_{1}^{\dagger }\right] \mathbb{Y}\right) \\
&=&tr\left( \left( I-P_{\mathbb{X}_{1}}\right) \sigma ^{2}I\right) +\left( 
\mathbb{X}\beta \right) ^{\dagger }\left[ I-P_{\mathbb{X}_{1}}\right] \left( 
\mathbb{X}\beta \right) \\
&&\left( tr\left( I_{n\times n}\right) =n\quad tr\left( P_{\mathbb{X}%
_{1}}\right) =tr\left( \mathbb{X}_{1}\right) =P_{1}\right) \\
&=&\sigma ^{2}\left( n-P_{1}\right) +\underset{\left( \ast \right) }{%
\underbrace{\beta _{2}^{\dagger }\mathbb{X}_{2}^{\dagger }\left[ I-P_{%
\mathbb{X}_{1}}\right] \mathbb{X}_{2}\beta _{2}}}
\end{eqnarray*}%
\begin{equation*}
\therefore E\left( \frac{\text{SSE}_{1}}{n-P_{1}}\right) =\sigma ^{2}+\frac{%
\left( \ast \right) }{n-P_{1}}\geq 0
\end{equation*}
\end{proof}
\end{enumerate}
\end{proof}
\end{theorem}

\bigskip

\paragraph{Orthogalization}

\begin{equation*}
\mathbb{X}=\left[ 
\begin{array}{cc}
\mathbb{X}_{1} & \mathbb{X}_{2}%
\end{array}%
\right] \quad \boldsymbol{\beta }=\left[ 
\begin{array}{c}
\boldsymbol{\beta }_{1} \\ 
\boldsymbol{\beta }_{2}%
\end{array}%
\right] \quad \mathbb{Y}=\mathbb{X}\boldsymbol{\beta }+\boldsymbol{%
\varepsilon }
\end{equation*}

\begin{theorem}
If $\mathbb{X}_{1}^{\dagger }\mathbb{X}_{2}=\boldsymbol{0}$, then estimator
of $\boldsymbol{\beta }_{1}$ in the full model $\mathbb{Y}=\mathbb{X}_{1}%
\boldsymbol{\beta }_{1}+\mathbb{X}_{2}\boldsymbol{\beta }_{2}+\boldsymbol{%
\varepsilon }$ is same as the estimator of $\boldsymbol{\beta }_{1}^{\ast }$
in the redued model%
\begin{equation*}
\Downarrow \left[ 
\begin{array}{c}
\mathbb{Y}=\mathbb{X}_{1}\boldsymbol{\beta }_{1}+\boldsymbol{\varepsilon }
\\ 
\boldsymbol{\hat{\beta}}_{1}^{\ast }=\left( \mathbb{X}_{1}^{\dagger }\mathbb{%
X}_{1}\right) ^{-1}\mathbb{X}_{1}^{\dagger }\mathbb{Y}%
\end{array}%
\right.
\end{equation*}

\begin{proof}
OLS of $\boldsymbol{\beta }_{1}$ is $\boldsymbol{\hat{\beta}}_{1}^{\ast
}=\left( \mathbb{X}_{1}^{\dagger }\mathbb{X}_{1}\right) ^{-1}\mathbb{X}%
_{1}^{\dagger }\mathbb{Y}$\newline
OLS of $\boldsymbol{\beta }$ is 
\begin{eqnarray*}
\boldsymbol{\hat{\beta}} &=&\left[ 
\begin{array}{c}
\boldsymbol{\hat{\beta}}_{1} \\ 
\boldsymbol{\hat{\beta}}_{2}%
\end{array}%
\right] =\left( \mathbb{X}^{\dagger }\mathbb{X}\right) ^{-1}\mathbb{X}%
^{\dagger }\mathbb{Y} \\
&=&\left[ 
\begin{array}{cc}
\mathbb{X}_{1}^{\dagger }\mathbb{X}_{1} & \mathbb{X}_{1}^{\dagger }\mathbb{X}%
_{2} \\ 
\mathbb{X}_{2}^{\dagger }\mathbb{X}_{1} & \mathbb{X}_{2}^{\dagger }\mathbb{X}%
_{2}%
\end{array}%
\right] ^{-1}\left[ 
\begin{array}{c}
\mathbb{X}_{1}\mathbb{Y} \\ 
\mathbb{X}_{2}\mathbb{Y}%
\end{array}%
\right] \\
&&\left[ \because \mathbb{X}_{1}^{\dagger }\mathbb{X}_{2}=\boldsymbol{0}%
\Downarrow \right] \\
&=&\left[ 
\begin{array}{cc}
\left( \mathbb{X}_{1}^{\dagger }\mathbb{X}_{1}\right) ^{-1} & \boldsymbol{0}%
^{\dagger } \\ 
\boldsymbol{0} & \left( \mathbb{X}_{2}^{\dagger }\mathbb{X}_{2}\right) ^{-1}%
\end{array}%
\right] _{2\times 2}\left[ 
\begin{array}{c}
\mathbb{X}_{1}^{\dagger }\mathbb{Y} \\ 
\mathbb{X}_{2}^{\dagger }\mathbb{Y}%
\end{array}%
\right] _{2\times 1} \\
&=&\left[ 
\begin{array}{c}
\underset{\boldsymbol{\hat{\beta}}_{1}^{\ast }}{\underbrace{\overset{%
\boldsymbol{\hat{\beta}}_{1}}{\overbrace{\left( \mathbb{X}_{1}^{\dagger }%
\mathbb{X}_{1}\right) ^{-1}}}\mathbb{X}_{1}^{\dagger }\mathbb{Y}}} \\ 
\overset{\boldsymbol{\hat{\beta}}_{2}}{\overbrace{\left( \mathbb{X}%
_{2}^{\dagger }\mathbb{X}_{2}\right) ^{-1}}}\mathbb{X}_{2}^{\dagger }\mathbb{%
Y}%
\end{array}%
\right]
\end{eqnarray*}%
\begin{eqnarray*}
&&\left[ 
\begin{array}{cc}
\mathbb{X}_{1}^{\dagger }\mathbb{X}_{1} & \mathbb{X}_{1}^{\dagger }\mathbb{X}%
_{2} \\ 
\mathbb{X}_{2}^{\dagger }\mathbb{X}_{1} & \mathbb{X}_{2}^{\dagger }\mathbb{X}%
_{2}%
\end{array}%
\right] ^{-1} \\
&=&\left[ 
\begin{array}{cc}
\left( \mathbb{X}_{1}^{\dagger }\mathbb{X}_{1}\right) ^{\dagger }+\left( 
\mathbb{X}_{1}^{\dagger }\mathbb{X}_{1}\right) ^{-1}\mathbb{X}_{1}^{\dagger }%
\mathbb{X}_{2}\mathbb{B}^{-1}\mathbb{X}_{2}^{\dagger }\mathbb{X}_{1}\left( 
\mathbb{X}_{1}^{\dagger }\mathbb{X}_{1}\right) ^{-1} & -\left( \mathbb{X}%
_{1}^{\dagger }\mathbb{X}_{1}\right) ^{-1}\mathbb{X}_{1}^{\dagger }\mathbb{X}%
_{2}\mathbb{B}^{-1} \\ 
-\mathbb{B}^{-1}\mathbb{X}_{2}^{-1}\mathbb{X}_{1}\left( \mathbb{X}%
_{1}^{\dagger }\mathbb{X}_{1}\right) ^{-1} & \mathbb{B}^{-1}%
\end{array}%
\right]
\end{eqnarray*}%
where 
\begin{eqnarray*}
\mathbb{B} &=&\mathbb{X}_{2}^{\dagger }\mathbb{X}_{2}-\mathbb{X}%
_{2}^{\dagger }\mathbb{X}_{1}\left( \mathbb{X}_{1}^{\dagger }\mathbb{X}%
_{1}\right) ^{-1}\mathbb{X}_{1}^{\dagger }\mathbb{X}_{2} \\
&=&\mathbb{X}_{2}^{\dagger }\underset{\left( I-P_{\mathbb{X}}\right) \mathbb{%
Y}=M_{\mathbb{X}}\mathbb{Y}}{\underbrace{\left[ I-\mathbb{X}_{1}\left( 
\mathbb{X}_{1}^{\dagger }\mathbb{X}_{1}\right) ^{-1}\mathbb{X}_{1}^{\dagger }%
\right] }}\mathbb{X}_{2} \\
&=&\mathbb{X}_{2}^{\dagger }M_{\mathbb{X}_{1}}\mathbb{X}_{2}
\end{eqnarray*}%
\begin{eqnarray*}
\hat{\beta}_{2} &=&-\left( \mathbb{X}_{2}^{\dagger }M_{\mathbb{X}_{1}}%
\mathbb{X}_{2}\right) ^{-1}\mathbb{X}_{2}^{\dagger }\mathbb{X}_{1}\left( 
\mathbb{X}_{1}^{\dagger }\mathbb{X}_{1}\right) ^{-1}\mathbb{X}_{1}^{\dagger }%
\mathbb{Y} \\
&&+\left( \mathbb{X}_{2}^{\dagger }M_{\mathbb{X}_{1}}\mathbb{X}_{2}\right) 
\mathbb{X}_{2}^{\dagger }\mathbb{Y} \\
&=&\left( \mathbb{X}_{2}^{\dagger }M_{\mathbb{X}_{1}}\mathbb{X}_{2}\right)
^{-1}\mathbb{X}_{2}^{\dagger }M_{\mathbb{X}_{1}}\mathbb{Y} \\
&=&\left( \mathbb{X}_{2}^{\dagger }M_{\mathbb{X}_{1}}M_{\mathbb{X}_{1}}%
\mathbb{X}_{2}\right) ^{-1}\mathbb{X}_{2}^{\dagger }M_{\mathbb{X}_{1}}M_{%
\mathbb{X}_{1}}\mathbb{Y} \\
&=&\left( \left( M_{\mathbb{X}_{1}}\mathbb{X}_{2}\right) ^{\dagger }M_{%
\mathbb{X}_{1}}\mathbb{X}_{2}\right) ^{-1}\left( M_{\mathbb{X}_{1}}\mathbb{X}%
_{2}\right) ^{\dagger }M_{\mathbb{X}_{1}}\mathbb{Y}
\end{eqnarray*}
\end{proof}
\end{theorem}

\bigskip

\begin{example}
Consider $\mathbb{Y}=\mathbb{X}\boldsymbol{\beta }+\boldsymbol{\varepsilon }$%
, where $\boldsymbol{\varepsilon }=N_{n}\left( \boldsymbol{0},\sigma
^{2}I\right) $%
\begin{equation*}
\mathbb{Y}=\mathbb{X}\boldsymbol{\beta }+\boldsymbol{\varepsilon }\sim
N_{n}\left( \mathbb{X}\boldsymbol{\beta },\sigma ^{2}I\right) 
\end{equation*}%
\begin{equation*}
\mathbb{\hat{Y}}=P_{\mathbb{X}}\mathbb{Y}\sim N_{n}\left( \underset{\mathbb{X%
}\boldsymbol{\beta }}{\underbrace{P_{\mathbb{X}}\mathbb{X}\boldsymbol{\beta }%
}},\underset{\sigma ^{2}P_{\mathbb{X}}}{\underbrace{P_{\mathbb{X}}\sigma
^{2}IP_{\mathbb{X}}^{\dagger }}}\right) 
\end{equation*}%
\begin{equation*}
\boldsymbol{\hat{e}}=\left( I-P_{\mathbb{X}}\right) \mathbb{Y}\sim N\left(
0,\sigma ^{2}\left( I-P_{\mathbb{X}}\right) \right) 
\end{equation*}
\end{example}

\bigskip 

\subsubsection{Inference}

\paragraph{Testing models}

\bigskip

Consider the linear model 
\begin{equation*}
\mathbb{Y}=\mathbb{X}\boldsymbol{\beta }+\boldsymbol{\varepsilon \quad }%
\cdots \boldsymbol{\quad }\text{full model}
\end{equation*}%
where $\gamma \left( \mathbb{X}\right) =\gamma \leq p$, $E\left( \boldsymbol{%
\varepsilon }\right) =0$, $Var\left( \boldsymbol{\varepsilon }\right)
=\sigma ^{2}I$

\bigskip

Consider the linear model%
\begin{equation*}
\boldsymbol{y}=\mathbb{W}\boldsymbol{\gamma }+\boldsymbol{\varepsilon \quad }%
\cdots \boldsymbol{\quad }\text{redued model}
\end{equation*}%
where $C\left( \mathbb{W}\right) \subset C\left( \mathbb{X}\right) $, $%
E\left( \boldsymbol{\varepsilon }\right) =0$, $Var\left( \boldsymbol{%
\varepsilon }\right) =\sigma ^{2}I$

\bigskip

note:

\begin{enumerate}
\item the estimation space of R.M. is smaller than in the F.M.

\item The goal is to test whether or not the reduced model is also correct.

\begin{itemize}
\item If R.M. is correct, there is no reason not to use it.

\item Smaller models are easier to interpret.
\end{itemize}

\item Let $P_{\mathbb{X}}$ and $P_{\mathbb{W}}$ denote the ppm onto $C\left( 
\mathbb{X}\right) $ and $C\left( \mathbb{W}\right) $. Because $C\left( 
\mathbb{W}\right) \subset C\left( \mathbb{X}\right) $, we know that $P_{%
\mathbb{X}}-P_{\mathbb{W}}$ is the ppm onto $C\left( P_{\mathbb{X}}-P_{%
\mathbb{W}}\right) =\underset{\maltese }{\underline{C\left( \mathbb{W}%
\right) ^{\perp }C\left( \mathbb{X}\right) }}$
\end{enumerate}

\bigskip

\begin{lemma}
Assume $P_{\mathbb{X}}$ and $P_{\mathbb{W}}$ are projection matries and $%
P_{2}-P_{1}$ is P.D., then

\begin{enumerate}
\item $P_{1}P_{2}=P_{2}P_{1}=P_{2}$

\item $P_{1}-P_{2}$ is a p.m.
\end{enumerate}
\end{lemma}

\bigskip

note:

\begin{enumerate}
\item $P_{1}$ is a p.m onto $C\left( \mathbb{X}\right) $

\item $P_{2}$ is a p.m onto $C\left( \mathbb{W}\right) \subset C\left( 
\mathbb{X}\right) $

\item $P_{1}-P_{2}$ is a p.m onto the orthogonal complement of $\mathbb{W}$
with in $\mathbb{X}$
\end{enumerate}

\bigskip

\frame{figure}

\bigskip

Under F.M., our estimate for $E\left( \boldsymbol{y}\right) =\boldsymbol{%
x\beta }$ is $P_{\mathbb{X}}\mathbb{Y}$

Under R.M., our estimate is $E\left( \boldsymbol{y}\right) =\mathbb{W}%
\boldsymbol{\gamma }$ is $P_{\mathbb{W}}\mathbb{Y}$

\begin{enumerate}
\item If R.M. is correct, then the $P_{\mathbb{X}}$ and $P_{\mathbb{W}}$ are
estimates the same thing. (i.e. $P_{\mathbb{X}}\mathbb{Y}-P_{\mathbb{W}}%
\mathbb{Y=}\left( P_{\mathbb{X}}-P_{\mathbb{W}}\right) \mathbb{Y}$ should be
small.)

\item the size of $\left( P_{\mathbb{X}}-P_{\mathbb{W}}\right) \mathbb{Y}$ is%
\begin{equation*}
\frac{\left[ \left( P_{\mathbb{X}}-P_{\mathbb{W}}\right) \mathbb{Y}\right]
^{\dagger }\left[ \left( P_{\mathbb{X}}-P_{\mathbb{W}}\right) \mathbb{Y}%
\right] }{\gamma \left( P_{\mathbb{X}}-P_{\mathbb{W}}\right) }=\frac{\mathbb{%
Y}^{\dagger }\left( P_{\mathbb{X}}-P_{\mathbb{W}}\right) \mathbb{Y}}{\mathbb{%
\gamma }\left( P_{\mathbb{X}}-P_{\mathbb{W}}\right) }
\end{equation*}
\end{enumerate}

\bigskip

If R.M. is correct, then%
\begin{eqnarray*}
E\left( \frac{\mathbb{Y}^{\dagger }\left( P_{\mathbb{X}}-P_{\mathbb{W}%
}\right) \mathbb{Y}}{\mathbb{\gamma }\left( P_{\mathbb{X}}-P_{\mathbb{W}%
}\right) }\right) &=&\frac{1}{\gamma ^{\ast }}E\left( \mathbb{Y}^{\dagger
}\left( P_{\mathbb{X}}-P_{\mathbb{W}}\right) \mathbb{Y}\right) \\
&=&\frac{1}{\gamma ^{\ast }}\left\{ tr\left[ \left( P_{\mathbb{X}}-P_{%
\mathbb{W}}\right) \sigma ^{2}I\right] +\right. \\
&&\left. \left( W\gamma \right) ^{\dagger }\left( P_{\mathbb{X}}-P_{\mathbb{W%
}}\right) \left( W\gamma \right) \right\} \\
&=&\frac{1}{\gamma ^{\ast }}\left\{ \sigma ^{2}tr\left( P_{\mathbb{X}}-P_{%
\mathbb{W}}\right) +\right. \\
&&\left. \gamma ^{\dagger }W^{\dagger }\left( P_{\mathbb{X}}-P_{\mathbb{W}%
}\right) W\gamma \right\} \\
&=&\frac{1}{\gamma ^{\ast }}\left\{ \sigma ^{2}\gamma \left( P_{\mathbb{X}%
}-P_{\mathbb{W}}\right) \right\} \\
&=&\sigma ^{2}\quad \left( \text{U.E. of }\sigma ^{2}\right)
\end{eqnarray*}

\bigskip

If R.M. is not correct, then%
\begin{eqnarray*}
E\left( \frac{\mathbb{Y}^{\dagger }\left( P_{\mathbb{X}}-P_{\mathbb{W}%
}\right) \mathbb{Y}}{\mathbb{\gamma }^{\ast }}\right) &=&\frac{1}{\gamma
^{\ast }}\left\{ tr\left[ \left( P_{\mathbb{X}}-P_{\mathbb{W}}\right) \sigma
^{2}I\right] \right. \\
&&+\left. \left( \mathbb{X}\beta \right) ^{\dagger }\left( P_{\mathbb{X}}-P_{%
\mathbb{W}}\right) \left( \mathbb{X}\beta \right) \right\} \\
&=&\sigma ^{2}+\left( \mathbb{X}\beta \right) ^{\dagger }\left( P_{\mathbb{X}%
}-P_{\mathbb{W}}\right) \mathbb{X}\beta
\end{eqnarray*}%
\begin{equation*}
MSE=\frac{\mathbb{Y}^{\dagger }\left( I-P_{\mathbb{X}}\right) \mathbb{Y}}{%
\mathbb{\gamma }\left( I-P_{\mathbb{X}}\right) }=\frac{1}{n-\gamma }\mathbb{Y%
}^{\dagger }\left( I-P_{\mathbb{X}}\right) \mathbb{Y}
\end{equation*}%
\begin{equation*}
E\left( MSE\right) =\sigma ^{2}
\end{equation*}

\bigskip

Test Statistic

\bigskip

To test R.M. vs F.M., we use%
\begin{eqnarray*}
F &=&\frac{\mathbb{Y}^{\dagger }\left( P_{\mathbb{X}}-P_{\mathbb{W}}\right) 
\mathbb{Y}/\gamma ^{\ast }}{\mathbb{Y}^{\dagger }\left( I-P_{\mathbb{X}%
}\right) \mathbb{Y}/\left( n-\gamma \right) } \\
&=&\left\{ 
\begin{tabular}{l}
$\simeq 1\quad \text{, R.M. is correct}$ \\ 
$>1\quad \text{, R.M. is not correct}$%
\end{tabular}%
\right.  \\
&=&\frac{\text{MSR}}{\text{MSE}}
\end{eqnarray*}

\end{document}
