
\documentclass{article}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{accents}
\usepackage[ignoreall,a4paper]{geometry}
\usepackage{fancyhdr}

\setcounter{MaxMatrixCols}{10}
%TCIDATA{OutputFilter=LATEX.DLL}
%TCIDATA{Version=5.00.0.2606}
%TCIDATA{<META NAME="SaveForMode" CONTENT="1">}
%TCIDATA{BibliographyScheme=Manual}
%TCIDATA{Created=Wednesday, November 25, 2015 15:33:37}
%TCIDATA{LastRevised=Friday, May 06, 2016 16:47:58}
%TCIDATA{<META NAME="GraphicsSave" CONTENT="32">}
%TCIDATA{<META NAME="DocumentShell" CONTENT="Standard LaTeX\Blank - Standard LaTeX Article">}
%TCIDATA{CSTFile=40 LaTeX article.cst}
%TCIDATA{ComputeDefs=
%$W=\left( 1-\sigma \right) I$
%}


\newtheorem{theorem}{Theorem}
\newtheorem{acknowledgement}[theorem]{Acknowledgement}
\newtheorem{algorithm}[theorem]{Algorithm}
\newtheorem{axiom}[theorem]{Axiom}
\newtheorem{case}[theorem]{Case}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{conclusion}[theorem]{Conclusion}
\newtheorem{condition}[theorem]{Condition}
\newtheorem{conjecture}[theorem]{Conjecture}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{criterion}[theorem]{Criterion}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}
\newtheorem{exercise}[theorem]{Exercise}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{notation}[theorem]{Notation}
\newtheorem{problem}[theorem]{Problem}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{solution}[theorem]{Solution}
\newtheorem{summary}[theorem]{Summary}
\newenvironment{proof}[1][Proof]{\noindent\textbf{#1.} }{\ \rule{0.5em}{0.5em}}
\input{../../tcilatex}
\DeclareMathAccent{\wtilde}{\mathord}{largesymbols}{"65}
\pagestyle{fancy}
\fancyfoot[C]{\thepage}

\input{tcilatex}

\begin{document}


\setcounter{part}{1} \setcounter{page}{1}

\subsubsection{Aitken Model and generalized least squares}

\begin{theorem}
Let $\mathbb{Y}=\mathbb{X}\boldsymbol{\beta }+\boldsymbol{\varepsilon }$,
where $E\left( \boldsymbol{\varepsilon }\right) =\boldsymbol{0}$, and $%
Var\left( \boldsymbol{\varepsilon }\right) =\sigma ^{2}\mathbb{V}$, $\mathbb{%
X}$ is a full-rank matrix and $\mathbb{V}$ is a known positive definite
matrix.

\begin{enumerate}
\item $\hat{\beta}_{GLS}=\left( \mathbb{X}^{\dagger }V^{-1}\mathbb{X}\right)
^{-1}\mathbb{X}^{\dagger }V^{-1}\mathbb{Y}$ is the BLUE of $\boldsymbol{%
\beta }$

\item $Var\left( \hat{\beta}_{GLS}\right) =\sigma ^{2}\left( \mathbb{X}%
^{\dagger }V^{-1}\mathbb{X}\right) ^{-1}$

\item An unbiased estimator of $\sigma ^{2}$ is%
\begin{equation*}
S^{2}=\frac{\left( \mathbb{Y-X}\hat{\beta}_{GLS}\right) ^{\dagger
}V^{-1}\left( \mathbb{Y-X}\hat{\beta}_{GLS}\right) }{n-p}
\end{equation*}%
\newline
\newline
\end{enumerate}

\begin{proof}
\newline
tool: Let $\mathbb{V}$ be a P.D matrix, then $\exists $ $\mathbb{P}_{n\times
n}$ nonsingular matrix s.t \fbox{$\mathbb{PP}^{\dagger }=\mathbb{V}$}

\begin{enumerate}
\item $\mathbb{Y}=\mathbb{X}\boldsymbol{\beta }+\boldsymbol{\varepsilon }$%
\begin{eqnarray*}
&\Rightarrow &\mathbb{P}^{-1}\mathbb{Y}=\mathbb{P}^{-1}\mathbb{X}\boldsymbol{%
\beta }+\mathbb{P}^{-1}\boldsymbol{\varepsilon } \\
&\Rightarrow &\mathbb{Y}^{\ast }=\mathbb{X}^{\ast }\boldsymbol{\beta }+%
\boldsymbol{\varepsilon }^{\ast }
\end{eqnarray*}%
$E\left( \mathbb{P}^{-1}\boldsymbol{\varepsilon }\right) =\boldsymbol{0}$%
\newline
\begin{eqnarray*}
Var\left( \mathbb{P}^{-1}\boldsymbol{\varepsilon }\right) &=&\mathbb{P}%
^{-1}Var\left( \boldsymbol{\varepsilon }\right) \left( \mathbb{P}%
^{-1}\right) ^{\dagger } \\
&=&\sigma ^{2}P^{-1}V\left( P^{-1}\right) ^{\dagger } \\
&=&\sigma ^{2}\mathbb{P}^{-1}\mathbb{PP}^{\dagger }\left( \mathbb{P}%
^{\dagger }\right) ^{-1} \\
&=&\sigma ^{2}I
\end{eqnarray*}%
$\therefore $OLS is%
\begin{eqnarray*}
\boldsymbol{\hat{\beta}} &\boldsymbol{=}&\left( \left( \mathbb{P}^{-1}%
\mathbb{X}\right) ^{\dagger }\left( \mathbb{P}^{-1}\mathbb{X}\right) \right)
^{-1}\left( \mathbb{P}^{-1}\mathbb{X}\right) ^{\dagger }\mathbb{P}^{-1}%
\mathbb{Y} \\
&&\vdots \\
&=&\left( \mathbb{X}^{\dagger }V^{-1}\mathbb{X}\right) ^{-1}\mathbb{X}%
^{\dagger }\mathbb{V}^{-1}\mathbb{Y} \\
&\hookrightarrow &\text{generalized least squares}
\end{eqnarray*}%
\newline
\newline
$Var\left( P^{-1}\varepsilon \right) =Cov\left( P^{-1}\varepsilon
,P^{-1}\varepsilon \right) =P^{-1}\fbox{$\quad $}\left( P^{-1}\right)
^{\dagger }$

\item 
\begin{eqnarray*}
&&\left( \mathbb{Y-X}\hat{\beta}\right) ^{\dagger }V^{-1}\left( \mathbb{Y-X}%
\hat{\beta}\right) \\
&=&\left( \mathbb{Y-X}\left( \mathbb{X}^{\dagger }V^{-1}\mathbb{X}\right)
^{-1}\mathbb{X}^{\dagger }\mathbb{V}^{-1}\mathbb{Y}\right) ^{\dagger }V^{-1}
\\
&&\left( \mathbb{Y-X}\left( \mathbb{X}^{\dagger }V^{-1}\mathbb{X}\right)
^{-1}\mathbb{X}^{\dagger }\mathbb{V}^{-1}\mathbb{Y}\right) \\
&=&\left[ \left( I-\left( \ast \right) \right) \mathbb{Y}\right] ^{\dagger
}V^{-1}\left[ \left( I-\left( \ast \right) \right) \mathbb{Y}\right]
\end{eqnarray*}%
where $(\ast )=\mathbb{X}\left( \mathbb{X}^{\dagger }V^{-1}\mathbb{X}\right)
^{-1}\mathbb{X}^{\dagger }$%
\begin{eqnarray*}
&=&\mathbb{Y}^{\dagger }\left[ V^{-1}-\left( \ast \right) V^{-1}\right] %
\left[ I-\left( \ast \right) \right] \mathbb{Y} \\
&&\vdots \\
&=&\mathbb{Y}^{\dagger }\left[ V^{-1}-V^{-1}\mathbb{X}\left( \mathbb{X}%
^{\dagger }V^{-1}\mathbb{X}\right) ^{-1}\mathbb{X}^{\dagger }V^{-1}\right] 
\mathbb{Y}
\end{eqnarray*}%
\newline
\newline
\begin{eqnarray*}
&\therefore &E\left( \mathbb{Y}^{\dagger }\left[ V^{-1}-V^{-1}\mathbb{X}%
\left( \mathbb{X}^{\dagger }V^{-1}\mathbb{X}\right) ^{-1}\mathbb{X}^{\dagger
}V^{-1}\right] \mathbb{Y}\right) \\
&=&tr\left( \sigma ^{2}V\left[ V^{-1}-V^{-1}\mathbb{X}\left( \mathbb{X}%
^{\dagger }V^{-1}\mathbb{X}\right) ^{-1}\mathbb{X}^{\dagger }V^{-1}\right]
+\right. \\
&&\left( \mathbb{X}\beta \right) ^{\dagger }\left[ V^{-1}-V^{-1}\mathbb{X}%
\left( \mathbb{X}^{\dagger }V^{-1}\mathbb{X}\right) ^{-1}\mathbb{X}^{\dagger
}V^{-1}\right] \left( \mathbb{X}\beta \right) \\
&=&\sigma ^{2}tr\left( V\left[ V^{-1}-V^{-1}\mathbb{X}\left( \mathbb{X}%
^{\dagger }V^{-1}\mathbb{X}\right) ^{-1}\mathbb{X}^{\dagger }V^{-1}\right]
\right) +0 \\
&=&\sigma ^{2}\left[ \underset{tr\left( I_{n}\right) }{\underbrace{n}}%
-tr\left( I_{pp}\right) \right] =\sigma ^{2}\left( n-p\right)
\end{eqnarray*}%
\newline
\newline
\begin{equation*}
E\left( \mathbb{Y}^{\dagger }A\mathbb{Y}\right) =tr\left( A\tsum \right)
+\mu ^{\dagger }A\mu
\end{equation*}
\end{enumerate}
\end{proof}
\end{theorem}

\bigskip

Note:

\begin{enumerate}
\item If $V=diag\left( v_{1},\cdots ,v_{m}\right) $, then%
\begin{eqnarray*}
Q\left( \boldsymbol{\beta }\right) &=&\left( \mathbb{Y-X}\boldsymbol{\beta }%
\right) ^{\dagger }V^{-1}\left( \mathbb{Y-X}\boldsymbol{\beta }\right) \\
&=&\tsum\limits_{i=1}^{n}\mathcal{W}_{i}\left( y_{i}-x_{i}^{\dagger }%
\boldsymbol{\beta }\right) ^{2}
\end{eqnarray*}%
where $\mathcal{W}_{i}=\frac{1}{v_{i}}$ and $x_{i}$ is the ith row of $%
\mathbb{X}$. $\hat{\beta}$ is called \textbf{weighted\ least squares
estimator}.

\item $\star $ If $l^{\dagger }\boldsymbol{\beta }$ is estimator, then $%
l^{\dagger }\boldsymbol{\hat{\beta}}$ is the BLUE for $l^{\dagger }%
\boldsymbol{\beta }$.\newline
\newline
\end{enumerate}

\begin{example}
$\mathbb{Y}=\mathbb{X}\boldsymbol{\beta }+\boldsymbol{\varepsilon }$, where%
\begin{eqnarray*}
Var\left( \boldsymbol{\varepsilon }\right) &=&\sigma ^{2}\left[ 
\begin{array}{ccc}
1 &  & \sigma \\ 
& \ddots &  \\ 
\sigma &  & 1%
\end{array}%
\right] \\
&=&\left[ 
\begin{array}{cc}
\boldsymbol{1} & \mathbb{X}_{c}%
\end{array}%
\right] \left[ 
\begin{array}{c}
\alpha \\ 
\boldsymbol{\beta }_{1}%
\end{array}%
\right] +\boldsymbol{\varepsilon }
\end{eqnarray*}%
\begin{equation*}
V=\sigma ^{2}\left[ 
\begin{array}{ccc}
v_{11} & v_{12} &  \\ 
& \ddots &  \\ 
&  & 
\end{array}%
\right] _{n\times n}
\end{equation*}%
\begin{eqnarray*}
\boldsymbol{\hat{\beta}} &=&\left( \mathbb{X}^{\dagger }V^{-1}\mathbb{X}%
\right) ^{-1}\mathbb{X}^{\dagger }V^{-1}\mathbb{Y} \\
&=&\left[ 
\begin{array}{cc}
bn & \boldsymbol{0}^{\dagger } \\ 
\boldsymbol{0} & a\mathbb{X}_{c}^{\dagger }\mathbb{X}_{c}%
\end{array}%
\right] ^{-1}\left[ 
\begin{array}{c}
bn\bar{y} \\ 
a\mathbb{X}_{c}^{\dagger }\mathbb{Y}%
\end{array}%
\right] \\
&=&\left[ 
\begin{array}{cc}
\frac{1}{bn} & \boldsymbol{0}^{\dagger } \\ 
\boldsymbol{0} & \frac{1}{a}\left( \mathbb{X}_{c}^{\dagger }\mathbb{X}%
_{c}\right) ^{-1}%
\end{array}%
\right] \left[ 
\begin{array}{c}
bn\bar{y} \\ 
a\mathbb{X}_{c}^{\dagger }\mathbb{Y}%
\end{array}%
\right] \\
&=&\left[ 
\begin{array}{c}
\bar{y} \\ 
\left( \mathbb{X}_{c}^{\dagger }\mathbb{X}_{c}\right) ^{-1}\mathbb{X}%
_{c}^{\dagger }\mathbb{Y}%
\end{array}%
\right]
\end{eqnarray*}%
$\therefore $ the usual least squares estimators are BLUE for a covariance
structure with equal variances and correlations.
\end{example}

\bigskip

\paragraph{Misspecification of the Error Structure}

\begin{equation*}
\mathbb{Y}=\mathbb{X}\boldsymbol{\beta }+\boldsymbol{\varepsilon }\quad
,\quad E\left( \boldsymbol{\varepsilon }\right) =\boldsymbol{0}\quad ,\quad
Var\left( \boldsymbol{\varepsilon }\right) =\sigma ^{2}V\Leftrightarrow
\sigma ^{2}I
\end{equation*}

Consider%
\begin{equation*}
\boldsymbol{\hat{\beta}}_{OLS}=\left( \mathbb{X}^{\dagger }\mathbb{X}\right)
^{-1}\mathbb{X}^{\dagger }\mathbb{Y}
\end{equation*}%
\begin{eqnarray*}
E\left( \boldsymbol{\hat{\beta}}_{OLS}\right) &=&E\left( \left( \mathbb{X}%
^{\dagger }\mathbb{X}\right) ^{-1}\mathbb{X}^{\dagger }\mathbb{Y}\right) \\
&=&\left( \mathbb{X}^{\dagger }\mathbb{X}\right) ^{-1}\mathbb{X}^{\dagger
}E\left( \mathbb{Y}\right) \\
&=&\left( \mathbb{X}^{\dagger }\mathbb{X}\right) ^{-1}\mathbb{X}^{\dagger }%
\mathbb{X}\boldsymbol{\beta } \\
&=&\boldsymbol{\beta }
\end{eqnarray*}%
\begin{eqnarray*}
Var\left( \boldsymbol{\hat{\beta}}_{OLS}\right) &=&Var\left( \left( \mathbb{X%
}^{\dagger }\mathbb{X}\right) ^{-1}\mathbb{X}^{\dagger }\mathbb{Y}\right) \\
&=&Var\left( A\mathbb{Y}\right) \\
&=&\left( \mathbb{X}^{\dagger }\mathbb{X}\right) ^{-1}\mathbb{X}^{\dagger
}Var\left( \mathbb{Y}\right) \left[ \left( \mathbb{X}^{\dagger }\mathbb{X}%
\right) ^{-1}\mathbb{X}^{\dagger }\right] ^{\dagger } \\
&=&\fbox{$\left( \mathbb{X}^{\dagger }\mathbb{X}\right) ^{-1}\mathbb{X}%
^{\dagger }\sigma ^{2}V\mathbb{X}\left( \mathbb{X}^{\dagger }\mathbb{X}%
\right) ^{-1}$}\geq Var\left( \boldsymbol{\hat{\beta}}_{OLS}\right)
\end{eqnarray*}

\bigskip

$\frac{\hat{\theta}-E\left( \hat{\theta}\right) }{\sqrt{Var\left( \hat{\theta%
}\right) }}\quad \frac{\hat{\theta}}{\sqrt{Var\left( \hat{\theta}\right) }}$

\bigskip

\paragraph{Model Misspecification}

\begin{equation*}
\mathbb{Y}=\mathbb{X}\boldsymbol{\beta }+\boldsymbol{\varepsilon }\quad
,\quad E\left( \boldsymbol{\varepsilon }\right) =\boldsymbol{0}\quad ,\text{
and }Var\left( \boldsymbol{\varepsilon }\right) =\sigma ^{2}I
\end{equation*}%
\begin{eqnarray*}
\mathbb{Y} &=&\mathbb{X}\boldsymbol{\beta }+\boldsymbol{\varepsilon } \\
&=&\left[ 
\begin{array}{cc}
\mathbb{X}_{1} & \mathbb{X}_{2}%
\end{array}%
\right] \left[ 
\begin{array}{c}
\boldsymbol{\beta }_{1} \\ 
\boldsymbol{\beta }_{2}%
\end{array}%
\right] +\boldsymbol{\varepsilon } \\
&=&\mathbb{X}_{1}\boldsymbol{\beta }_{1}+\mathbb{X}_{2}\boldsymbol{\beta }%
_{2}+\boldsymbol{\varepsilon }\quad \quad \text{full}
\end{eqnarray*}

\bigskip

\begin{theorem}
(underfitting) If we fit the model $\mathbb{Y}=\mathbb{X}_{1}\boldsymbol{%
\beta }_{1}+\boldsymbol{\varepsilon }$ when the correct model is $\mathbb{Y}=%
\mathbb{X}_{1}\boldsymbol{\beta }_{1}+\mathbb{X}_{2}\boldsymbol{\beta }_{2}+%
\boldsymbol{\varepsilon }$, then

\begin{enumerate}
\item $E\left( \boldsymbol{\hat{\beta}}_{1}^{\ast }\right) =\boldsymbol{%
\beta }_{1}+\mathbb{A}\boldsymbol{\beta }_{2}$, where $\mathbb{A}=\left( 
\mathbb{X}_{1}^{\dagger }\mathbb{X}\right) ^{-1}\mathbb{X}_{1}\mathbb{X}_{2}$

\item $Var\left( \boldsymbol{\hat{\beta}}_{1}^{\ast }\right) =\sigma
^{2}\left( \mathbb{X}_{1}^{\dagger }\mathbb{X}\right) ^{-1}$

\item $Var\left( \boldsymbol{\hat{\beta}}_{1}\right) \underset{\geq }{-}%
Var\left( \boldsymbol{\hat{\beta}}_{1}^{\ast }\right) =\sigma ^{2}\mathbb{AB}%
^{-1}\mathbb{A}^{\dagger }$ which is P.D. matrix
\end{enumerate}

\begin{proof}
\begin{enumerate}
\item 
\begin{eqnarray*}
E\left( \boldsymbol{\hat{\beta}}_{1}^{\ast }\right) &=&E\left( \left( 
\mathbb{X}_{1}^{\dagger }\mathbb{X}_{1}\right) ^{-1}\mathbb{X}_{1}^{\dagger }%
\mathbb{Y}\right) \\
&=&\left( \mathbb{X}_{1}^{\dagger }\mathbb{X}_{1}\right) ^{-1}\mathbb{X}%
_{1}^{\dagger }E\left( \mathbb{Y}\right) \\
&=&\left( \mathbb{X}_{1}^{\dagger }\mathbb{X}_{1}\right) ^{-1}\mathbb{X}%
_{1}^{\dagger }\mathbb{X}\boldsymbol{\beta } \\
&=&\left( \mathbb{X}_{1}^{\dagger }\mathbb{X}_{1}\right) ^{-1}\mathbb{X}%
_{1}^{\dagger }\left( \mathbb{X}_{1}\boldsymbol{\beta }_{1}+\mathbb{X}_{2}%
\boldsymbol{\beta }_{2}\right) \\
&=&\boldsymbol{\beta }_{1}+\left( \mathbb{X}_{1}^{\dagger }\mathbb{X}%
_{1}\right) ^{-1}\underset{0}{\underbrace{\mathbb{X}_{1}^{\dagger }\mathbb{X}%
_{2}}}\boldsymbol{\beta }_{2}\quad \quad \text{bias}
\end{eqnarray*}

\item 
\begin{eqnarray*}
Var\left( \boldsymbol{\hat{\beta}}_{1}^{\ast }\right)  &=&Var\left( 
\underline{\left( \mathbb{X}_{1}^{\dagger }\mathbb{X}_{1}\right) ^{-1}%
\mathbb{X}_{1}^{\dagger }}\mathbb{Y}\right)  \\
&=&\underline{\left( \mathbb{X}_{1}^{\dagger }\mathbb{X}_{1}\right) ^{-1}%
\mathbb{X}_{1}^{\dagger }}\underset{\sigma ^{2}I}{\underbrace{Var\left( 
\mathbb{Y}\right) }}\underline{\left[ \left( \mathbb{X}_{1}^{\dagger }%
\mathbb{X}_{1}\right) ^{-1}\mathbb{X}_{1}^{\dagger }\right] ^{\dagger }} \\
&=&\sigma ^{2}\left( \mathbb{X}_{1}^{\dagger }\mathbb{X}_{1}\right) ^{-1}%
\mathbb{X}_{1}^{\dagger }\mathbb{X}_{1}\left( \mathbb{X}_{1}^{\dagger }%
\mathbb{X}_{1}\right) ^{-1} \\
&=&\sigma ^{2}\left( \mathbb{X}_{1}^{\dagger }\mathbb{X}_{1}\right) ^{-1}
\end{eqnarray*}%
\begin{eqnarray*}
Var\left( \boldsymbol{\hat{\beta}}\right)  &=&\sigma ^{2}\left( \mathbb{X}%
^{\dagger }\mathbb{X}\right) ^{-1} \\
&=&\sigma ^{2}\left[ \left[ 
\begin{array}{cc}
\mathbb{X}_{1} & \mathbb{X}_{2}%
\end{array}%
\right] ^{\dagger }\left[ 
\begin{array}{cc}
\mathbb{X}_{1} & \mathbb{X}_{2}%
\end{array}%
\right] \right] ^{-1} \\
&=&\sigma ^{2}\left[ \left[ 
\begin{array}{c}
\mathbb{X}_{1}^{\dagger } \\ 
\mathbb{X}_{2}^{\dagger }%
\end{array}%
\right] \left[ 
\begin{array}{cc}
\mathbb{X}_{1} & \mathbb{X}_{2}%
\end{array}%
\right] \right] ^{-1} \\
&=&\sigma ^{2}\left[ 
\begin{array}{cc}
\mathbb{X}_{1}^{\dagger }\mathbb{X}_{1} & \mathbb{X}_{1}^{\dagger }\mathbb{X}%
_{2} \\ 
\mathbb{X}_{2}^{\dagger }\mathbb{X}_{1} & \mathbb{X}_{2}^{\dagger }\mathbb{X}%
_{2}%
\end{array}%
\right] ^{-1} \\
&=&\sigma ^{2}\left[ 
\begin{array}{cc}
G_{11} & G_{12} \\ 
G_{21} & G_{22}%
\end{array}%
\right] ^{-1} \\
&=&\sigma ^{2}\left[ 
\begin{array}{cc}
G^{11} & G^{12} \\ 
G^{21} & G^{22}%
\end{array}%
\right] 
\end{eqnarray*}%
tools:%
\begin{eqnarray*}
&&\left[ 
\begin{array}{cc}
\mathbb{A}_{11} & \mathbb{A}_{12} \\ 
\mathbb{A}_{21} & \mathbb{A}_{22}%
\end{array}%
\right] ^{-1} \\
&=&\left[ 
\begin{array}{cc}
\mathbb{A}_{11}^{-1}+\mathbb{A}_{11}^{-1}\mathbb{A}_{12}\mathbb{D}^{-1}%
\mathbb{A}_{21}\mathbb{A}_{11}^{-1} & -\mathbb{A}_{11}^{-1}\mathbb{A}_{12}%
\mathbb{D}^{-1} \\ 
-\mathbb{D}^{-1}\mathbb{A}_{21}\mathbb{A}_{11}^{-1} & \mathbb{D}^{-1}%
\end{array}%
\right] 
\end{eqnarray*}%
where $D=\mathbb{A}_{22}-\mathbb{A}_{21}\mathbb{A}_{11}^{-1}\mathbb{A}_{12}$%
\begin{eqnarray*}
G^{11} &=&\left( \mathbb{X}_{1}^{\dagger }\mathbb{X}_{1}\right) ^{-1}\mathbb{%
+} \\
&&\left( \mathbb{X}_{1}^{\dagger }\mathbb{X}_{1}\right) \mathbb{X}%
_{1}^{\dagger }\mathbb{X}_{2}\left[ \mathbb{X}_{2}^{\dagger }\mathbb{X}_{2}-%
\mathbb{X}_{2}\mathbb{X}_{1}\left( \mathbb{X}_{1}^{\dagger }\mathbb{X}%
\right) ^{-1}\mathbb{X}_{1}\mathbb{X}_{2}\right] ^{-1}\times  \\
&&\mathbb{X}_{2}^{\dagger }\mathbb{X}_{1}\left( \mathbb{X}_{1}^{\dagger }%
\mathbb{X}_{1}\right) ^{-1}
\end{eqnarray*}%
\newline
\newline
\begin{equation*}
\left. 
\begin{array}{c}
Var\left( \boldsymbol{\hat{\beta}}_{1}\right) =\sigma ^{2}G^{11} \\ 
Var\left( \boldsymbol{\hat{\beta}}_{1}^{\ast }\right) =\sigma ^{2}\left( 
\mathbb{X}_{1}^{\dagger }\mathbb{X}_{1}\right) ^{-1}%
\end{array}%
\right] 
\end{equation*}%
\newline
\newline
\begin{eqnarray*}
&\therefore &Var\left( \boldsymbol{\hat{\beta}}_{1}\right) -Var\left( 
\boldsymbol{\hat{\beta}}_{1}^{\ast }\right) =\sigma ^{2}\left[ \left( 
\mathbb{X}_{1}^{\dagger }\mathbb{X}\right) ^{-1}+\mathbb{A}B^{-1}\mathbb{A}%
^{\dagger }\right]  \\
&&-\sigma ^{2}\left( \mathbb{X}_{1}^{\dagger }\mathbb{X}\right) ^{-1} \\
&=&\sigma ^{2}\underset{\text{P.D. why?}}{\fbox{$\mathbb{AB}^{-1}\mathbb{A}%
^{\dagger }$}}
\end{eqnarray*}

\item 
\begin{eqnarray*}
E\left( S_{1}^{2}\right)  &=&\sigma ^{2}+ \\
&&\frac{\boldsymbol{\beta }_{2}^{\dagger }\mathbb{X}_{2}^{\dagger }\left[ I-%
\mathbb{X}_{1}\left( \mathbb{X}_{1}^{\dagger }\mathbb{X}_{1}\right) ^{-1}%
\mathbb{X}_{1}^{\dagger }\right] \mathbb{X}_{2}\boldsymbol{\beta }_{2}}{%
n-P_{1}}
\end{eqnarray*}%
where $S_{1}^{2}=\frac{\left( \mathbb{Y}-\mathbb{X}_{1}\boldsymbol{\hat{\beta%
}}_{1}^{\ast }\right) ^{\dagger }\left( \mathbb{Y}-\mathbb{X}_{1}\boldsymbol{%
\hat{\beta}}_{1}^{\ast }\right) }{n-P_{1}}=\frac{\text{SSE}_{1}}{n-P_{1}}$
and $\gamma \left( \mathbb{X}_{1}\right) =P_{1}$
\end{enumerate}
\end{proof}
\end{theorem}

\end{document}
