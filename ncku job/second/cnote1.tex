
\documentclass{article}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{accents}
\usepackage[ignoreall,a4paper]{geometry}
\usepackage{fancyhdr}

\setcounter{MaxMatrixCols}{10}
%TCIDATA{OutputFilter=LATEX.DLL}
%TCIDATA{Version=5.00.0.2606}
%TCIDATA{<META NAME="SaveForMode" CONTENT="1">}
%TCIDATA{BibliographyScheme=Manual}
%TCIDATA{Created=Wednesday, November 25, 2015 15:33:37}
%TCIDATA{LastRevised=Friday, May 20, 2016 11:46:33}
%TCIDATA{<META NAME="GraphicsSave" CONTENT="32">}
%TCIDATA{<META NAME="DocumentShell" CONTENT="Standard LaTeX\Blank - Standard LaTeX Article">}
%TCIDATA{CSTFile=40 LaTeX article.cst}
%TCIDATA{ComputeDefs=
%$W=\left( 1-\sigma \right) I$
%}


\newtheorem{theorem}{Theorem}
\newtheorem{acknowledgement}[theorem]{Acknowledgement}
\newtheorem{algorithm}[theorem]{Algorithm}
\newtheorem{axiom}[theorem]{Axiom}
\newtheorem{case}[theorem]{Case}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{conclusion}[theorem]{Conclusion}
\newtheorem{condition}[theorem]{Condition}
\newtheorem{conjecture}[theorem]{Conjecture}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{criterion}[theorem]{Criterion}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}
\newtheorem{exercise}[theorem]{Exercise}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{notation}[theorem]{Notation}
\newtheorem{problem}[theorem]{Problem}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{solution}[theorem]{Solution}
\newtheorem{summary}[theorem]{Summary}
\newenvironment{proof}[1][Proof]{\noindent\textbf{#1.} }{\ \rule{0.5em}{0.5em}}
\input{../../tcilatex}
\DeclareMathAccent{\wtilde}{\mathord}{largesymbols}{"65}
\pagestyle{fancy}
\fancyfoot[C]{\thepage}

\input{tcilatex}

\begin{document}


\section{Linear Mixed Models}

\subsection{The random effects model}

Consider the model%
\begin{equation*}
Y_{ij}=\mu +a_{i}+\varepsilon _{ij}\qquad 
\begin{array}{c}
i=1,\cdots ,a \\ 
j=1,\cdots ,n%
\end{array}%
\end{equation*}%
where $a_{i}\sim \mathcal{N}\left( 0,\sigma _{a}^{2}\right) $ and $%
\varepsilon _{ij}\sim \mathcal{N}\left( 0,\sigma ^{2}\right) $, and the $%
\varepsilon _{ij}$ and $a_{i}$ are $JI+I$ independent random variables.

\bigskip

\begin{example}
Suppose that we are interested in studying the output in numbers of parts
turned out by the workers in a factory. A large number of workers are
available, and we choose $I$ of them at random, asking each to work $J$
different two-hour time periods.
\end{example}

\begin{example}
Suppose that a pharmaceutical company wishes to test a new experimental
drug. The drug is to be applied to patients with particular disease and the
response is a measure of the improvement in their status. A sample of $I=10$
clinics is selected at random from a large population of clinics and, within
each clinic, a random sample of $J=5$ patients is selected.
\end{example}

\bigskip

The matrix form%
\begin{eqnarray*}
Y_{11} &=&\mu +\alpha _{1}+\varepsilon _{11} \\
Y_{12} &=&\mu +\alpha _{1}+\varepsilon _{12} \\
&&\vdots \\
Y_{1n} &=&\mu +\alpha _{1}+\varepsilon _{1n} \\
&&\vdots \\
Y_{a1} &=&\mu +\alpha _{I}+\varepsilon _{I1} \\
&&\vdots \\
Y_{an} &=&\mu +\alpha _{I}+\varepsilon _{In}
\end{eqnarray*}%
\begin{equation*}
\Leftrightarrow \left[ 
\begin{array}{c}
Y_{11} \\ 
\vdots \\ 
Y_{1n} \\ 
\vdots \\ 
Y_{I1} \\ 
\vdots \\ 
Y_{IJ}%
\end{array}%
\right] =\underset{a+1\text{ colomn}}{\left[ \underbrace{%
\begin{array}{c}
1_{n} \\ 
\\ 
\vdots \\ 
1_{n}%
\end{array}%
}\right. \left. \underbrace{%
\begin{array}{cccc}
1_{n} &  &  &  \\ 
& 1_{n} &  &  \\ 
&  & \ddots &  \\ 
&  &  & 1_{n}%
\end{array}%
}\right] _{a_{n}\times \left( a+1\right) }}\left[ 
\begin{array}{c}
\mu \\ 
\alpha _{1} \\ 
\vdots \\ 
\alpha _{a}%
\end{array}%
\right]
\end{equation*}

\bigskip

\begin{equation*}
\mathbf{Y}=\mu \boldsymbol{1}_{an}+\mathbf{Za+\varepsilon }
\end{equation*}%
where $\mathbf{Z}$ is an $an\times a$ matrix, $\mathbf{a}$ is $a\times 1$
unknown random vector.

\bigskip

\begin{theorem}
Consider the model $\mathbf{Y}=\mathbf{x\beta }+\tsum\limits_{i=1}^{m}%
\mathbf{Z}_{i}\mathbf{a}_{i}+\mathbf{\varepsilon }$, where $\mathbf{x}$ is a
known $n\times p$ matrix, the $\mathbf{Z}_{i}$'s are known $n_{i}\times
r_{i} $ full-rank matrix, $\mathbf{\beta }$ is a $p\times 1$ vector of
unknown parameters, $\mathbf{\varepsilon }$ is an $n\times 1$ unkown random
vector such that $E\left( \mathbf{\varepsilon }\right) =\mathbf{0}$ and $%
V\left( \mathbf{\varepsilon }\right) =\sigma ^{2}I_{n}$, and $\mathbf{a}_{i}$
are $r_{i}\times 1$ unknown random vectors such that $E\left( \mathbf{a}%
_{i}\right) =\mathbf{0}$ and $Var\left( \mathbf{a}_{i}\right) =\sigma
_{i}^{2}I_{r_{i}}$. Furthermore, $Cov\left( \mathbf{a}_{i},\mathbf{a}%
_{j}\right) =\mathbf{0}$ for $i\neq j$ and $Cov\left( \mathbf{a}_{i},\mathbf{%
\varepsilon }\right) =\mathbf{0}\quad \forall i$. Then

\begin{enumerate}
\item $E\left( \mathbf{Y}\right) =\mathbf{x\beta }$

\item $Var\left( \mathbf{Y}\right) =\tsum\limits_{i=1}^{m}\sigma _{i}^{2}%
\mathbf{Z}_{i}\mathbf{Z}_{i}^{\dagger }+\sigma ^{2}I_{n}$
\end{enumerate}

\begin{proof}
\begin{enumerate}
\item 
\begin{eqnarray*}
E\left( \mathbf{Y}\right) &=&E\left( \mathbf{x\beta }+\tsum\limits_{i=1}^{m}%
\mathbf{Z}_{i}\mathbf{a}_{i}+\mathbf{\varepsilon }\right) \\
&=&\mathbf{x\beta }+\tsum\limits_{i=1}^{m}\left[ \mathbf{Z}_{i}E\left( 
\mathbf{a}_{i}\right) +E\left( \mathbf{\varepsilon }\right) \right] \\
&=&\mathbf{x\beta }
\end{eqnarray*}

\item 
\begin{eqnarray*}
Var\left( \mathbf{Y}\right) &=&Var\left( \mathbf{x\beta }+\tsum%
\limits_{i=1}^{m}\mathbf{Z}_{i}\mathbf{a}_{i}+\mathbf{\varepsilon }\right) \\
&=&Var\left( \tsum\limits_{i=1}^{m}\mathbf{Z}_{i}\mathbf{a}_{i}+\mathbf{%
\varepsilon }\right) \\
&=&Var\left( \tsum\limits_{i=1}^{m}\mathbf{Z}_{i}\mathbf{a}_{i}\right)
+Var\left( \mathbf{\varepsilon }\right) +Cov\left( \tsum\limits_{i=1}^{m}%
\mathbf{Z}_{i}\mathbf{a}_{i},\mathbf{\varepsilon }\right) \\
&=&\tsum\limits_{i=1}^{m}Var\left( \mathbf{Z}_{i}\mathbf{a}_{i}\right)
+\tsum\limits_{i\neq j}Cov\left( \mathbf{Z}_{i}\mathbf{a}_{i},\mathbf{Z}_{j}%
\mathbf{a}_{j}\right) +\sigma ^{2}I_{n} \\
&&+\tsum\limits_{i=1}^{m}Cov\left( \mathbf{Z}_{i}\mathbf{a}_{i},\mathbf{%
\varepsilon }\right) +\tsum\limits_{i=1}^{m}Cov\left( \mathbf{\varepsilon ,Z}%
_{i}\mathbf{a}_{i}\right) \\
&=&\tsum\limits_{i=1}^{m}\mathbf{Z}_{i}Var\left( \mathbf{Z}_{i}\mathbf{a}%
_{i}\right) \mathbf{Z}_{i}^{\dagger }+\tsum\limits_{i\neq j}\mathbf{Z}%
_{i}Cov\left( \mathbf{a}_{i},\mathbf{a}_{j}\right) \mathbf{Z}_{j}+\sigma
^{2}I_{n} \\
&&+\tsum\limits_{i=1}^{m}\mathbf{Z}_{i}Cov\left( \mathbf{a}_{i},\mathbf{%
\varepsilon }\right) +\tsum\limits_{i=1}^{m}Cov\left( \mathbf{\varepsilon ,a}%
_{i}\right) \mathbf{Z}_{i}^{\dagger } \\
&=&\tsum\limits_{i=1}^{m}\mathbf{Z}_{i}\sigma _{i}^{2}I_{r_{i}}\mathbf{Z}%
_{i}^{\dagger }+\sigma ^{2}I_{n} \\
&=&\tsum\limits_{i=1}^{m}\sigma _{i}^{2}\mathbf{Z}_{i}\mathbf{Z}%
_{i}^{\dagger }+\sigma ^{2}I_{n}
\end{eqnarray*}
\end{enumerate}
\end{proof}
\end{theorem}

\bigskip

\begin{example}
(Randomized Blocks) An experiment involving three treatments was carried out
by randomly assigning the treatments to experimental units within each of
four blocks of size $3$. The linear model is given by 
\begin{equation*}
y_{ij}=\mu +\tau _{i}+a_{j}+\varepsilon _{ij}\quad 
\begin{array}{c}
i=1,\cdots ,3 \\ 
j=1,\cdots ,4%
\end{array}%
\end{equation*}%
where $a_{j}\sim \mathcal{N}\left( 0,\sigma _{a}^{2}\right) $, $\varepsilon
_{ij}\sim \mathcal{N}\left( 0,\sigma ^{2}\right) $, and $Cov\left(
a_{j},\varepsilon _{ij}\right) =0$.%
\begin{eqnarray*}
Y_{11} &=&\mu +\tau _{1}+a_{1}+\varepsilon _{11} \\
Y_{21} &=&\mu +\tau _{2}+a_{1}+\varepsilon _{12} \\
Y_{31} &=&\mu +\tau _{3}+a_{1}+\varepsilon _{13} \\
&&\vdots \\
Y_{14} &=&\mu +\tau _{1}+a_{4}+\varepsilon _{31} \\
&&\vdots \\
Y_{34} &=&\mu +\tau _{3}+a_{4}+\varepsilon _{34}
\end{eqnarray*}%
\begin{equation*}
\Leftrightarrow \left[ 
\begin{array}{c}
Y_{11} \\ 
Y_{21} \\ 
Y_{31} \\ 
\vdots \\ 
Y_{14} \\ 
\vdots \\ 
Y_{34}%
\end{array}%
\right] =\left[ 
\begin{array}{cc}
\boldsymbol{1}_{3} & I_{3} \\ 
&  \\ 
\boldsymbol{1}_{3} & I_{3} \\ 
\boldsymbol{1}_{3} & I_{3} \\ 
\boldsymbol{1}_{3} & I_{3}%
\end{array}%
\right\vert \left. 
\begin{array}{cccc}
\boldsymbol{1}_{3} & 0 & 0 & 0 \\ 
&  &  &  \\ 
0 & \boldsymbol{1}_{3} & 0 & 0 \\ 
0 & 0 & \boldsymbol{1}_{3} & 0 \\ 
0 & 0 & 0 & \boldsymbol{1}_{3}%
\end{array}%
\right] +\varepsilon
\end{equation*}%
\newline
\newline
\begin{equation*}
\mathbf{Y=x\beta +Za+\varepsilon }
\end{equation*}%
where 
\begin{equation*}
\mathbf{x=}\left[ 
\begin{array}{cc}
\boldsymbol{1}_{3} & I_{3} \\ 
\vdots & \vdots \\ 
\boldsymbol{1}_{3} & I_{3}%
\end{array}%
\right] _{12\times 4},\quad \mathbf{\beta }^{\dagger }=\left[ 
\begin{array}{cccc}
\mu & 1_{1} & 1_{2} & 1_{3}%
\end{array}%
\right] ^{\dagger },
\end{equation*}%
\newline
\begin{equation*}
\mathbf{Z}_{1}=\left[ 
\begin{array}{cccc}
\boldsymbol{1}_{3} &  &  & \mathbf{0} \\ 
& \boldsymbol{1}_{3} &  &  \\ 
&  & \boldsymbol{1}_{3} &  \\ 
\mathbf{0} &  &  & \boldsymbol{1}_{3}%
\end{array}%
\right] _{12\times 4},\quad \mathbf{a}_{1}^{\dagger }=\left[ a_{1},\cdots
,a_{4}\right] _{4\times 1}
\end{equation*}
\end{example}

\bigskip

\begin{itemize}
\item $\tsum =Var\left( \mathbf{Y}\right) =\sigma _{a}^{2}\mathbf{Z}_{1}%
\mathbf{Z}_{1}^{\dagger }+\sigma ^{2}I=\left[ 
\begin{array}{cccc}
\tsum\nolimits_{1} &  &  &  \\ 
& \tsum\nolimits_{1} &  &  \\ 
&  & \tsum\nolimits_{1} &  \\ 
&  &  & \tsum\nolimits_{1}%
\end{array}%
\right] $\newline
where%
\begin{equation*}
\tsum\nolimits_{1}=\left[ 
\begin{array}{ccc}
\sigma _{a}^{2}+\sigma ^{2} & \sigma _{a}^{2} & \sigma _{a}^{2} \\ 
\sigma _{a}^{2} & \sigma _{a}^{2}+\sigma ^{2} & \sigma _{a}^{2} \\ 
\sigma _{a}^{2} & \sigma _{a}^{2} & \sigma _{a}^{2}+\sigma ^{2}%
\end{array}%
\right] 
\end{equation*}
\end{itemize}

\bigskip 

\subsection{Estimation of Variance Components}

Consider the model%
\begin{equation*}
\mathbf{Y}=\mathbf{x\beta }+\tsum\limits_{i=1}^{m}\mathbf{Z}_{i}\mathbf{a}%
_{i}+\mathbf{\varepsilon }
\end{equation*}%
where $\mathbf{x}$ is $n\times p$ matrix, $\mathbf{Z}_{i}$'s are $n\times
r_{i}$ full-rank matrix, $\mathbf{\beta }$ is a $p\times 1$ vector of
unknown parameters, $\mathbf{\varepsilon \sim }\mathcal{N}_{n}\left(
0,\sigma ^{2}I_{n}\right) $ and $\mathbf{a}_{i}\sim \mathcal{N}\left( 
\mathbf{0},\sigma _{i}^{2}I_{r_{i}}\right) $, $Cov\left( \mathbf{a}_{i},%
\mathbf{a}_{j}\right) =\mathbf{0}\quad \forall i\neq j$ and $Cov\left( 
\mathbf{a}_{i},\mathbf{\varepsilon }\right) =\mathbf{0}\quad \forall i$.

\bigskip 

\begin{equation*}
\therefore \mathbf{Y}\sim \mathcal{N}\left( \mathbf{x\beta }%
,\tsum\nolimits_{n\times n}\right) 
\end{equation*}%
where $\tsum =\tsum\limits_{i=1}^{m}\sigma _{i}^{2}\mathbf{Z}_{i}\mathbf{Z}%
_{i}^{\dagger }+\sigma ^{2}I=\tsum\limits_{i=1}^{m}\sigma _{i}^{2}\mathbf{Z}%
_{i}\quad $($\sigma _{0}^{2}=\sigma ^{2}$ and $I_{n}=\mathbf{Z}_{0}$)%
\begin{equation*}
E\left( \mathbf{KY}\right) =\mathbf{kx\beta }=0\Leftrightarrow \mathbf{kx}=%
\mathbf{0}\Leftrightarrow \text{REML}
\end{equation*}%
(residual maximum likelihood)

\bigskip 

\begin{theorem}
A full-rank matrix $\mathbf{K}$ with maximal number of rows such that $%
\mathbf{kx}=\mathbf{0}$, is an $\left( n-r\right) \times n$ matrix. $\mathbf{%
K}$ must be of the form $\mathbf{K}=\mathbf{C}\left( I-P_{\mathbf{x}}\right)
=\mathbf{C}\left[ I-\mathbf{x}\left( \mathbf{x}^{\dagger }\mathbf{x}\right)
^{-1}\mathbf{x}^{\dagger }\right] $ where $\mathbf{0}$ specifies a full-rank
transformation of the rows of $I-P_{\mathbf{x}}$.
\end{theorem}

\bigskip 

Note:

\begin{enumerate}
\item there are infinite number of $\mathbf{K}$

\item $\left( I-P_{\mathbf{x}}\right) \mathbf{Y}$ gives the ordinary
residual vector $\mathbf{\hat{\varepsilon}}$\newline
$\mathbf{KY}=\mathbf{C}\left( I-P_{\mathbf{x}}\right) \mathbf{Y}$ is a
vector of linearcombinations of $\mathbf{\hat{\varepsilon}}$

\item $\mathbf{K}_{\left( n-r\right) \times n}\mathbf{Y}_{n\times 1}\sim 
\mathcal{N}_{n-r}\left( \mathbf{0},\mathbf{K}\tsum \mathbf{K}^{\dagger
}\right) $%
\begin{eqnarray*}
E\left( \mathbf{KY}\right)  &=&\mathbf{kx\beta }=\mathbf{0} \\
Var\left( \mathbf{KY}\right)  &=&\mathbf{K}Var\left( \mathbf{Y}\right) 
\mathbf{K}^{\dagger }=\mathbf{K}\tsum \mathbf{K}^{\dagger }
\end{eqnarray*}
\end{enumerate}

\bigskip 

\begin{theorem}
Consider the model in which 
\end{theorem}

\end{document}
