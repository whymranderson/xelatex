
\documentclass{article}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{accents}
\usepackage[ignoreall,a4paper]{geometry}
\usepackage{fancyhdr}

\setcounter{MaxMatrixCols}{10}
%TCIDATA{OutputFilter=LATEX.DLL}
%TCIDATA{Version=5.00.0.2606}
%TCIDATA{<META NAME="SaveForMode" CONTENT="1">}
%TCIDATA{BibliographyScheme=Manual}
%TCIDATA{Created=Wednesday, November 25, 2015 15:33:37}
%TCIDATA{LastRevised=Sunday, May 22, 2016 14:51:54}
%TCIDATA{<META NAME="GraphicsSave" CONTENT="32">}
%TCIDATA{<META NAME="DocumentShell" CONTENT="Standard LaTeX\Blank - Standard LaTeX Article">}
%TCIDATA{CSTFile=40 LaTeX article.cst}
%TCIDATA{ComputeDefs=
%$W=\left( 1-\sigma \right) I$
%}


\newtheorem{theorem}{Theorem}
\newtheorem{acknowledgement}[theorem]{Acknowledgement}
\newtheorem{algorithm}[theorem]{Algorithm}
\newtheorem{axiom}[theorem]{Axiom}
\newtheorem{case}[theorem]{Case}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{conclusion}[theorem]{Conclusion}
\newtheorem{condition}[theorem]{Condition}
\newtheorem{conjecture}[theorem]{Conjecture}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{criterion}[theorem]{Criterion}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}
\newtheorem{exercise}[theorem]{Exercise}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{notation}[theorem]{Notation}
\newtheorem{problem}[theorem]{Problem}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{solution}[theorem]{Solution}
\newtheorem{summary}[theorem]{Summary}
\newenvironment{proof}[1][Proof]{\noindent\textbf{#1.} }{\ \rule{0.5em}{0.5em}}
\input{../../tcilatex}
\DeclareMathAccent{\wtilde}{\mathord}{largesymbols}{"65}
\pagestyle{fancy}
\fancyfoot[C]{\thepage}



\begin{document}


\section{Linear Mixed Models}

\subsection{The random effects model}

Consider the model%
\begin{equation*}
Y_{ij}=\mu +a_{i}+\varepsilon _{ij}\qquad 
\begin{array}{c}
i=1,\cdots ,a \\ 
j=1,\cdots ,n%
\end{array}%
\end{equation*}%
where $a_{i}\sim \mathcal{N}\left( 0,\sigma _{a}^{2}\right) $ and $%
\varepsilon _{ij}\sim \mathcal{N}\left( 0,\sigma ^{2}\right) $, and the $%
\varepsilon _{ij}$ and $a_{i}$ are $JI+I$ independent random variables.

\bigskip

\begin{example}
Suppose that we are interested in studying the output in numbers of parts
turned out by the workers in a factory. A large number of workers are
available, and we choose $I$ of them at random, asking each to work $J$
different two-hour time periods.
\end{example}

\begin{example}
Suppose that a pharmaceutical company wishes to test a new experimental
drug. The drug is to be applied to patients with particular disease and the
response is a measure of the improvement in their status. A sample of $I=10$
clinics is selected at random from a large population of clinics and, within
each clinic, a random sample of $J=5$ patients is selected.
\end{example}

\bigskip

The matrix form%
\begin{eqnarray*}
Y_{11} &=&\mu +\alpha _{1}+\varepsilon _{11} \\
Y_{12} &=&\mu +\alpha _{1}+\varepsilon _{12} \\
&&\vdots \\
Y_{1n} &=&\mu +\alpha _{1}+\varepsilon _{1n} \\
&&\vdots \\
Y_{a1} &=&\mu +\alpha _{I}+\varepsilon _{I1} \\
&&\vdots \\
Y_{an} &=&\mu +\alpha _{I}+\varepsilon _{In}
\end{eqnarray*}%
\begin{equation*}
\Leftrightarrow \left[ 
\begin{array}{c}
Y_{11} \\ 
\vdots \\ 
Y_{1n} \\ 
\vdots \\ 
Y_{I1} \\ 
\vdots \\ 
Y_{IJ}%
\end{array}%
\right] =\underset{a+1\text{ colomn}}{\left[ \underbrace{%
\begin{array}{c}
1_{n} \\ 
\\ 
\vdots \\ 
1_{n}%
\end{array}%
}\right. \left. \underbrace{%
\begin{array}{cccc}
1_{n} &  &  &  \\ 
& 1_{n} &  &  \\ 
&  & \ddots &  \\ 
&  &  & 1_{n}%
\end{array}%
}\right] _{a_{n}\times \left( a+1\right) }}\left[ 
\begin{array}{c}
\mu \\ 
\alpha _{1} \\ 
\vdots \\ 
\alpha _{a}%
\end{array}%
\right]
\end{equation*}

\bigskip

\begin{equation*}
\mathbf{Y}=\mu \boldsymbol{1}_{an}+\mathbf{Za+\varepsilon }
\end{equation*}%
where $\mathbf{Z}$ is an $an\times a$ matrix, $\mathbf{a}$ is $a\times 1$
unknown random vector.

\bigskip

\begin{theorem}
Consider the model $\mathbf{Y}=\mathbf{x\beta }+\tsum\limits_{i=1}^{m}%
\mathbf{Z}_{i}\mathbf{a}_{i}+\mathbf{\varepsilon }$, where $\mathbf{x}$ is a
known $n\times p$ matrix, the $\mathbf{Z}_{i}$'s are known $n_{i}\times
r_{i} $ full-rank matrix, $\mathbf{\beta }$ is a $p\times 1$ vector of
unknown parameters, $\mathbf{\varepsilon }$ is an $n\times 1$ unkown random
vector such that $E\left( \mathbf{\varepsilon }\right) =\mathbf{0}$ and $%
V\left( \mathbf{\varepsilon }\right) =\sigma ^{2}I_{n}$, and $\mathbf{a}_{i}$
are $r_{i}\times 1$ unknown random vectors such that $E\left( \mathbf{a}%
_{i}\right) =\mathbf{0}$ and $Var\left( \mathbf{a}_{i}\right) =\sigma
_{i}^{2}I_{r_{i}}$. Furthermore, $Cov\left( \mathbf{a}_{i},\mathbf{a}%
_{j}\right) =\mathbf{0}$ for $i\neq j$ and $Cov\left( \mathbf{a}_{i},\mathbf{%
\varepsilon }\right) =\mathbf{0}\quad \forall i$. Then

\begin{enumerate}
\item $E\left( \mathbf{Y}\right) =\mathbf{x\beta }$

\item $Var\left( \mathbf{Y}\right) =\tsum\limits_{i=1}^{m}\sigma _{i}^{2}%
\mathbf{Z}_{i}\mathbf{Z}_{i}^{\dagger }+\sigma ^{2}I_{n}$
\end{enumerate}

\begin{proof}
\begin{enumerate}
\item 
\begin{eqnarray*}
E\left( \mathbf{Y}\right) &=&E\left( \mathbf{x\beta }+\tsum\limits_{i=1}^{m}%
\mathbf{Z}_{i}\mathbf{a}_{i}+\mathbf{\varepsilon }\right) \\
&=&\mathbf{x\beta }+\tsum\limits_{i=1}^{m}\left[ \mathbf{Z}_{i}E\left( 
\mathbf{a}_{i}\right) +E\left( \mathbf{\varepsilon }\right) \right] \\
&=&\mathbf{x\beta }
\end{eqnarray*}

\item 
\begin{eqnarray*}
Var\left( \mathbf{Y}\right) &=&Var\left( \mathbf{x\beta }+\tsum%
\limits_{i=1}^{m}\mathbf{Z}_{i}\mathbf{a}_{i}+\mathbf{\varepsilon }\right) \\
&=&Var\left( \tsum\limits_{i=1}^{m}\mathbf{Z}_{i}\mathbf{a}_{i}+\mathbf{%
\varepsilon }\right) \\
&=&Var\left( \tsum\limits_{i=1}^{m}\mathbf{Z}_{i}\mathbf{a}_{i}\right)
+Var\left( \mathbf{\varepsilon }\right) +Cov\left( \tsum\limits_{i=1}^{m}%
\mathbf{Z}_{i}\mathbf{a}_{i},\mathbf{\varepsilon }\right) \\
&=&\tsum\limits_{i=1}^{m}Var\left( \mathbf{Z}_{i}\mathbf{a}_{i}\right)
+\tsum\limits_{i\neq j}Cov\left( \mathbf{Z}_{i}\mathbf{a}_{i},\mathbf{Z}_{j}%
\mathbf{a}_{j}\right) +\sigma ^{2}I_{n} \\
&&+\tsum\limits_{i=1}^{m}Cov\left( \mathbf{Z}_{i}\mathbf{a}_{i},\mathbf{%
\varepsilon }\right) +\tsum\limits_{i=1}^{m}Cov\left( \mathbf{\varepsilon ,Z}%
_{i}\mathbf{a}_{i}\right) \\
&=&\tsum\limits_{i=1}^{m}\mathbf{Z}_{i}Var\left( \mathbf{Z}_{i}\mathbf{a}%
_{i}\right) \mathbf{Z}_{i}^{\dagger }+\tsum\limits_{i\neq j}\mathbf{Z}%
_{i}Cov\left( \mathbf{a}_{i},\mathbf{a}_{j}\right) \mathbf{Z}_{j}+\sigma
^{2}I_{n} \\
&&+\tsum\limits_{i=1}^{m}\mathbf{Z}_{i}Cov\left( \mathbf{a}_{i},\mathbf{%
\varepsilon }\right) +\tsum\limits_{i=1}^{m}Cov\left( \mathbf{\varepsilon ,a}%
_{i}\right) \mathbf{Z}_{i}^{\dagger } \\
&=&\tsum\limits_{i=1}^{m}\mathbf{Z}_{i}\sigma _{i}^{2}I_{r_{i}}\mathbf{Z}%
_{i}^{\dagger }+\sigma ^{2}I_{n} \\
&=&\tsum\limits_{i=1}^{m}\sigma _{i}^{2}\mathbf{Z}_{i}\mathbf{Z}%
_{i}^{\dagger }+\sigma ^{2}I_{n}
\end{eqnarray*}
\end{enumerate}
\end{proof}
\end{theorem}

\bigskip

\begin{example}
(Randomized Blocks) An experiment involving three treatments was carried out
by randomly assigning the treatments to experimental units within each of
four blocks of size $3$. The linear model is given by 
\begin{equation*}
y_{ij}=\mu +\tau _{i}+a_{j}+\varepsilon _{ij}\quad 
\begin{array}{c}
i=1,\cdots ,3 \\ 
j=1,\cdots ,4%
\end{array}%
\end{equation*}%
where $a_{j}\sim \mathcal{N}\left( 0,\sigma _{a}^{2}\right) $, $\varepsilon
_{ij}\sim \mathcal{N}\left( 0,\sigma ^{2}\right) $, and $Cov\left(
a_{j},\varepsilon _{ij}\right) =0$.%
\begin{eqnarray*}
Y_{11} &=&\mu +\tau _{1}+a_{1}+\varepsilon _{11} \\
Y_{21} &=&\mu +\tau _{2}+a_{1}+\varepsilon _{12} \\
Y_{31} &=&\mu +\tau _{3}+a_{1}+\varepsilon _{13} \\
&&\vdots \\
Y_{14} &=&\mu +\tau _{1}+a_{4}+\varepsilon _{31} \\
&&\vdots \\
Y_{34} &=&\mu +\tau _{3}+a_{4}+\varepsilon _{34}
\end{eqnarray*}%
\begin{equation*}
\Leftrightarrow \left[ 
\begin{array}{c}
Y_{11} \\ 
Y_{21} \\ 
Y_{31} \\ 
\vdots \\ 
Y_{14} \\ 
\vdots \\ 
Y_{34}%
\end{array}%
\right] =\left[ 
\begin{array}{cc}
\boldsymbol{1}_{3} & I_{3} \\ 
&  \\ 
\boldsymbol{1}_{3} & I_{3} \\ 
\boldsymbol{1}_{3} & I_{3} \\ 
\boldsymbol{1}_{3} & I_{3}%
\end{array}%
\right\vert \left. 
\begin{array}{cccc}
\boldsymbol{1}_{3} & 0 & 0 & 0 \\ 
&  &  &  \\ 
0 & \boldsymbol{1}_{3} & 0 & 0 \\ 
0 & 0 & \boldsymbol{1}_{3} & 0 \\ 
0 & 0 & 0 & \boldsymbol{1}_{3}%
\end{array}%
\right] +\varepsilon
\end{equation*}%
\newline
\newline
\begin{equation*}
\mathbf{Y=x\beta +Za+\varepsilon }
\end{equation*}%
where 
\begin{equation*}
\mathbf{x=}\left[ 
\begin{array}{cc}
\boldsymbol{1}_{3} & I_{3} \\ 
\vdots & \vdots \\ 
\boldsymbol{1}_{3} & I_{3}%
\end{array}%
\right] _{12\times 4},\quad \mathbf{\beta }^{\dagger }=\left[ 
\begin{array}{cccc}
\mu & 1_{1} & 1_{2} & 1_{3}%
\end{array}%
\right] ^{\dagger },
\end{equation*}%
\newline
\begin{equation*}
\mathbf{Z}_{1}=\left[ 
\begin{array}{cccc}
\boldsymbol{1}_{3} &  &  & \mathbf{0} \\ 
& \boldsymbol{1}_{3} &  &  \\ 
&  & \boldsymbol{1}_{3} &  \\ 
\mathbf{0} &  &  & \boldsymbol{1}_{3}%
\end{array}%
\right] _{12\times 4},\quad \mathbf{a}_{1}^{\dagger }=\left[ a_{1},\cdots
,a_{4}\right] _{4\times 1}
\end{equation*}
\end{example}

\bigskip

\begin{itemize}
\item $\tsum =Var\left( \mathbf{Y}\right) =\sigma _{a}^{2}\mathbf{Z}_{1}%
\mathbf{Z}_{1}^{\dagger }+\sigma ^{2}I=\left[ 
\begin{array}{cccc}
\tsum\nolimits_{1} &  &  &  \\ 
& \tsum\nolimits_{1} &  &  \\ 
&  & \tsum\nolimits_{1} &  \\ 
&  &  & \tsum\nolimits_{1}%
\end{array}%
\right] $\newline
where%
\begin{equation*}
\tsum\nolimits_{1}=\left[ 
\begin{array}{ccc}
\sigma _{a}^{2}+\sigma ^{2} & \sigma _{a}^{2} & \sigma _{a}^{2} \\ 
\sigma _{a}^{2} & \sigma _{a}^{2}+\sigma ^{2} & \sigma _{a}^{2} \\ 
\sigma _{a}^{2} & \sigma _{a}^{2} & \sigma _{a}^{2}+\sigma ^{2}%
\end{array}%
\right]
\end{equation*}
\end{itemize}

\bigskip

\subsection{Estimation of Variance Components}

Consider the model%
\begin{equation*}
\mathbf{Y}=\mathbf{x\beta }+\tsum\limits_{i=1}^{m}\mathbf{Z}_{i}\mathbf{a}%
_{i}+\mathbf{\varepsilon }
\end{equation*}%
where $\mathbf{x}$ is $n\times p$ matrix, $\mathbf{Z}_{i}$'s are $n\times
r_{i}$ full-rank matrix, $\mathbf{\beta }$ is a $p\times 1$ vector of
unknown parameters, $\mathbf{\varepsilon \sim }\mathcal{N}_{n}\left(
0,\sigma ^{2}I_{n}\right) $ and $\mathbf{a}_{i}\sim \mathcal{N}\left( 
\mathbf{0},\sigma _{i}^{2}I_{r_{i}}\right) $, $Cov\left( \mathbf{a}_{i},%
\mathbf{a}_{j}\right) =\mathbf{0}\quad \forall i\neq j$ and $Cov\left( 
\mathbf{a}_{i},\mathbf{\varepsilon }\right) =\mathbf{0}\quad \forall i$.

\bigskip

\begin{equation*}
\therefore \mathbf{Y}\sim \mathcal{N}\left( \mathbf{x\beta }%
,\tsum\nolimits_{n\times n}\right)
\end{equation*}%
where $\tsum =\tsum\limits_{i=1}^{m}\sigma _{i}^{2}\mathbf{Z}_{i}\mathbf{Z}%
_{i}^{\dagger }+\sigma ^{2}I=\tsum\limits_{i=0}^{m}\sigma _{i}^{2}\mathbf{Z}%
_{i}\quad $($\sigma _{0}^{2}=\sigma ^{2}$ and $I_{n}=\mathbf{Z}_{0}$)%
\begin{equation*}
E\left( \mathbf{KY}\right) =\mathbf{kx\beta }=0\Leftrightarrow \mathbf{kx}=%
\mathbf{0}\Leftrightarrow \text{REML}
\end{equation*}%
(residual maximum likelihood)

\bigskip

\begin{theorem}
A full-rank matrix $\mathbf{K}$ with maximal number of rows such that $%
\mathbf{kx}=\mathbf{0}$, is an $\left( n-r\right) \times n$ matrix. $\mathbf{%
K}$ must be of the form $\mathbf{K}=\mathbf{C}\left( I-P_{\mathbf{x}}\right)
=\mathbf{C}\left[ I-\mathbf{x}\left( \mathbf{x}^{\dagger }\mathbf{x}\right)
^{-1}\mathbf{x}^{\dagger }\right] $ where $\mathbf{0}$ specifies a full-rank
transformation of the rows of $I-P_{\mathbf{x}}$.
\end{theorem}

\bigskip

Note:

\begin{enumerate}
\item there are infinite number of $\mathbf{K}$

\item $\left( I-P_{\mathbf{x}}\right) \mathbf{Y}$ gives the ordinary
residual vector $\mathbf{\hat{\varepsilon}}$\newline
$\mathbf{KY}=\mathbf{C}\left( I-P_{\mathbf{x}}\right) \mathbf{Y}$ is a
vector of linear combinations of $\mathbf{\hat{\varepsilon}}$

\item $\mathbf{K}_{\left( n-r\right) \times n}\mathbf{Y}_{n\times 1}\sim 
\mathcal{N}_{n-r}\left( \mathbf{0},\mathbf{K}\tsum \mathbf{K}^{\dagger
}\right) $%
\begin{eqnarray*}
E\left( \mathbf{KY}\right) &=&\mathbf{kx\beta }=\mathbf{0} \\
Var\left( \mathbf{KY}\right) &=&\mathbf{K}Var\left( \mathbf{Y}\right) 
\mathbf{K}^{\dagger }=\mathbf{K}\tsum \mathbf{K}^{\dagger }
\end{eqnarray*}
\end{enumerate}

\bigskip

\begin{theorem}
Consider the model in which $\mathbf{Y}\sim \mathcal{N}_{n}\left( \mathbf{%
x\beta },\tsum \right) $, where $\tsum =\tsum\limits_{i=0}^{m}\sigma _{i}^{2}%
\mathbf{Z}_{i}\mathbf{Z}_{i}^{\dagger }$ and let $\mathbf{K}$ be a matrix
such that $\mathbf{kx}=\mathbf{0}$. Then a set of $m+1$ estimating equations
for $\sigma _{0}^{2},\cdots ,\sigma _{m}^{2}$ is given by%
\begin{eqnarray*}
tr\left[ \mathbf{K}^{\dagger }\left( \mathbf{K}\tsum \mathbf{K}^{\dagger
}\right) ^{-1}\mathbf{KZ}_{i}\mathbf{Z}_{i}^{\dagger }\right] &=&\mathbf{Y}%
^{\dagger }\mathbf{K}^{\dagger }\left( \mathbf{K}\tsum \mathbf{K}^{\dagger
}\right) ^{-1}\mathbf{KZ}_{i}\mathbf{Z}_{i}^{\dagger }\times \\
&&\mathbf{K}^{\dagger }\left( \mathbf{K}\tsum \mathbf{K}^{\dagger }\right)
^{-1}\mathbf{KY}
\end{eqnarray*}%
\begin{equation*}
i=0,\cdots ,m
\end{equation*}

\begin{proof}
\begin{equation*}
\because \mathbf{KY}\sim \mathcal{N}_{n-r}\left( \mathbf{0},\mathbf{K}\tsum 
\mathbf{K}^{\dagger }\right)
\end{equation*}%
\begin{equation*}
\therefore \text{the log likelihood of }KT\text{ is}
\end{equation*}%
\begin{eqnarray*}
&&\ln L\left( \sigma _{0}^{2},\sigma _{1}^{2},\cdots ,\sigma _{m}^{2}\right)
\\
&=&\ln \left[ \frac{1}{\left( \sqrt{2\pi }\right) ^{n-r}\left\vert \mathbf{K}%
\tsum \mathbf{K}^{\dagger }\right\vert ^{\frac{1}{2}}}{\LARGE e}^{-\frac{%
\left( \mathbf{y}^{\prime }-0\right) ^{\dagger }\left( \mathbf{K}\tsum 
\mathbf{K}^{\dagger }\right) ^{-1}\left( \mathbf{y}^{\prime }-0\right) }{2}}%
\right] \\
&=&-\frac{n-r}{2}\ln 2\pi -\frac{1}{2}\ln \left\vert \mathbf{K}\tsum \mathbf{%
K}^{\dagger }\right\vert -\frac{1}{2}\left( \mathbf{Ky}\right) ^{\dagger
}\left( \mathbf{K}\tsum \mathbf{K}^{\dagger }\right) ^{-1}\left( \mathbf{Ky}%
\right) \\
&=&-\frac{n-r}{2}\ln 2\pi -\frac{1}{2}\ln \left\vert \mathbf{K}\left(
\tsum\limits_{i=0}^{m}\sigma _{i}^{2}\mathbf{Z}_{i}\mathbf{Z}_{i}^{\dagger
}\right) \mathbf{K}^{\dagger }\right\vert \\
&&-\frac{1}{2}\mathbf{y}^{\dagger }\mathbf{K}^{\dagger }\left[ \mathbf{%
K\left( \tsum\limits_{i=0}^{m}\sigma _{i}^{2}\mathbf{Z}_{i}\mathbf{Z}%
_{i}^{\dagger }\right) K}^{\dagger }\right] ^{-1}\mathbf{Ky}
\end{eqnarray*}

\begin{enumerate}
\item Let $\mathbf{A}$ be an $n\times n$ positive definite matrix. Then%
\begin{equation*}
\frac{\partial \ln \left\vert \mathbf{A}\right\vert }{\partial \chi }%
=tr\left( \mathbf{A}^{-1}\frac{\partial \mathbf{A}}{\partial \chi }\right)
\end{equation*}

\item 
\begin{equation*}
\frac{\partial \mathbf{A}^{-1}}{\partial \chi }=-\mathbf{A}^{-1}\frac{%
\partial \mathbf{A}}{\partial \chi }\mathbf{A}^{-1}
\end{equation*}
\end{enumerate}

\begin{eqnarray*}
&&\frac{\partial \ln L\left( \sigma _{0}^{2},\cdots ,\sigma _{m}^{2}\right) 
}{\partial \sigma _{i}^{2}} \\
&=&-\frac{1}{2}tr\left( \left[ \mathbf{K}\tsum \mathbf{K}^{\dagger }\right]
^{-1}\frac{\partial }{\partial \sigma _{i}^{2}}\left( \mathbf{K}\underset{%
\tsum\limits_{i=0}^{m}\sigma _{i}^{2}\mathbf{Z}_{i}\mathbf{Z}_{i}^{\dagger }}%
{\underbrace{\tsum }}\mathbf{K}^{\dagger }\right) \right) \\
&&+\frac{1}{2}\mathbf{Y}^{\dagger }\mathbf{K}^{\dagger }\left[ \mathbf{K}%
\tsum \mathbf{K}^{\dagger }\right] ^{-1}\left[ \frac{\partial }{\partial
\sigma _{i}^{2}}\left( \mathbf{K}\tsum \mathbf{K}^{\dagger }\right) \right] %
\left[ \mathbf{K}\tsum \mathbf{K}^{\dagger }\right] ^{-1}\mathbf{KY} \\
&=&-\frac{1}{2}tr\left( \left[ \mathbf{K}\tsum \mathbf{K}^{\dagger }\right]
^{-1}\mathbf{KZ}_{i}^{\dagger }\mathbf{Z}_{i}^{\dagger }\mathbf{K}^{\dagger
}\right) \\
&&+\frac{1}{2}\mathbf{Y}^{\dagger }\mathbf{K}^{\dagger }\left[ \mathbf{K}%
\tsum \mathbf{K}^{\dagger }\right] ^{-1}\mathbf{KZ}_{i}^{\dagger }\mathbf{Z}%
_{i}^{\dagger }\mathbf{K}^{\dagger }\left[ \mathbf{K}\tsum \mathbf{K}%
^{\dagger }\right] ^{-1}\mathbf{KY\_\_(\ast )} \\
&=&-\frac{1}{2}tr\left( \mathbf{K}^{\dagger }\left[ \mathbf{K}\tsum \mathbf{K%
}^{\dagger }\right] ^{-1}\mathbf{KZ}_{i}\mathbf{Z}_{i}^{\dagger }\right) +%
\frac{1}{2}\left( \ast \right)
\end{eqnarray*}
\end{proof}
\end{theorem}

\bigskip

\begin{example}
(One-way Random Effects) Consider the model%
\begin{equation*}
y_{ij}=\mu +a_{i}+\mathbf{\varepsilon }_{ij}\quad 
\begin{tabular}{l}
$i=1,\cdots ,3$ \\ 
$j=1,\cdots ,4$%
\end{tabular}%
\end{equation*}%
where $a_{i}\sim \mathcal{N}\left( 0,\sigma _{a}^{2}\right) $ and $%
\varepsilon _{ij}\sim \mathcal{N}\left( 0,\sigma ^{2}\right) $, $Cov\left(
a_{i},a_{j}\right) =0\quad i\neq j$ and $Cov\left( a_{i},\varepsilon \right)
=0$
\end{example}

\bigskip

\begin{equation*}
\mathbf{Y}_{12}=\mu \mathbf{1}_{12}+\mathbf{Z}_{12\times 3}\mathbf{a}%
_{3\times 1}+\mathbf{\varepsilon }_{12}
\end{equation*}%
where $\mathbf{Z}=\left[ 
\begin{array}{ccc}
\boldsymbol{1}_{4} & \boldsymbol{0} & \boldsymbol{0} \\ 
\boldsymbol{0} & \boldsymbol{1}_{4} & \boldsymbol{0} \\ 
\boldsymbol{0} & \boldsymbol{0} & \boldsymbol{1}_{4}%
\end{array}%
\right] _{12\times 3}$

\bigskip

$\therefore $%
\begin{equation*}
\tsum =\sigma _{a}^{2}\underset{\left[ 
\begin{array}{ccc}
J_{4} &  &  \\ 
& J_{4} &  \\ 
&  & J_{4}%
\end{array}%
\right] }{\underbrace{\mathbf{Z}}}\mathbf{Z}^{\dagger }+\sigma ^{2}I_{12}=%
\left[ 
\begin{array}{ccc}
\tsum_{1} & \boldsymbol{0} & \boldsymbol{0} \\ 
\boldsymbol{0} & \tsum_{1} & \boldsymbol{0} \\ 
\boldsymbol{0} & \boldsymbol{0} & \tsum_{1}%
\end{array}%
\right]
\end{equation*}%
where%
\begin{equation*}
\tsum\nolimits_{1}=\left[ 
\begin{array}{cccc}
\sigma _{a}^{2}+\sigma ^{2} & \sigma _{a}^{2} & \sigma _{a}^{2} & \sigma
_{a}^{2} \\ 
& \sigma _{a}^{2}+\sigma ^{2} & \sigma _{a}^{2} & \sigma _{a}^{2} \\ 
&  & \sigma _{a}^{2}+\sigma ^{2} & \sigma _{a}^{2} \\ 
&  &  & \sigma _{a}^{2}+\sigma ^{2}%
\end{array}%
\right]
\end{equation*}

\bigskip

Then $I_{12\times 12}-P_{\mathbf{x}}=I-\boldsymbol{1}_{12}\left( \boldsymbol{%
1}_{12}^{\dagger }\boldsymbol{1}\right) ^{-1}\boldsymbol{1}_{12}^{\dagger
}=I_{12}-\frac{1}{12}J_{12}$

\bigskip 

A suitable $\mathbf{C}$ would be $\mathbf{C}=\left[ I_{12},\mathbf{O}_{12}%
\right] _{12\times 24}$ and\newline
$\mathbf{K}_{11\times 12}=\mathbf{C}_{11\times 12}\left( I-P_{\mathbf{x}%
}\right) _{12\times 12}=\left[ 
\begin{array}{cc}
1_{11} & \mathbf{O}_{11}%
\end{array}%
\right] _{11\times 12}\left( I_{12}-\frac{1}{12}J_{12}\right) $

\bigskip

$K\tsum K^{\dagger }=\left( I_{12}-\frac{1}{12}J_{12}\right) \tsum \left(
I_{12}-\frac{1}{12}J_{12}\right) =\left( \tsum -\frac{1}{12}J\tsum \right)
\left( I_{12}-\frac{1}{12}J_{12}\right) $\newline
$=\tsum -\frac{1}{12}\tsum J_{12}-\frac{1}{12}J\tsum +\frac{1}{14x}J\tsum J$

\bigskip

$tr\left( K^{\prime }\left( K\tsum K^{\dagger }\right) ^{-1}K\mathbf{Z}_{0}%
\mathbf{Z}_{0}^{\dagger }\right) =tr\left( K^{\prime }\left( K\tsum
K^{\dagger }\right) ^{-1}xJ_{12}\right) =$

$\left( K\tsum K^{\dagger }\right) ^{-1}=\left( K\left( \sigma
_{0}^{2}I_{12}+\sigma _{1}^{2}Z_{1}Z_{1}^{\dagger }\right) K\right) ^{-1}$

\bigskip

\bigskip

\section{The General Mixed Model}

\bigskip

\begin{eqnarray*}
\mathbf{Y} &=&\mathbf{x\beta }+\mathbf{Z\mu }+\mathbf{\varepsilon \quad
,\quad \varepsilon }\sim \mathcal{N}\left( 0,\mathbf{R}\right) \mathbf{\quad
,\quad \mu }\sim \mathcal{N}\left( 0,\mathbf{G}\right)  \\
&&\mathbf{X,Z,R,G}\text{ are all known}
\end{eqnarray*}%
where $\mathbf{X}$ and $\mathbf{Z}$ are $n\times p$ and $n\times q$
matrices, and%
\begin{equation*}
\mathbf{\varepsilon }^{\ast }=\left[ 
\begin{array}{c}
\mathbf{\mu } \\ 
\mathbf{\varepsilon }%
\end{array}%
\right] =\left[ 
\begin{array}{cc}
\mathbf{Z} & I%
\end{array}%
\right] \left[ 
\begin{array}{c}
\mathbf{\mu } \\ 
\mathbf{\varepsilon }%
\end{array}%
\right] =\mathbf{A}\left[ 
\begin{array}{c}
\mathbf{\mu } \\ 
\mathbf{\varepsilon }%
\end{array}%
\right] 
\end{equation*}

\bigskip

\begin{equation*}
\mathbf{Y}=\mathbf{x\beta }+\mathbf{\varepsilon }^{\ast }\mathbf{,}\text{
where }\mathbf{\varepsilon }^{\ast }\sim \mathcal{N}_{n}\left( 0,\mathbf{V}%
\right) 
\end{equation*}%
\begin{eqnarray*}
\mathbf{V} &=&Var\left( \mathbf{A}\left[ 
\begin{array}{c}
\mathbf{\mu } \\ 
\mathbf{\varepsilon }%
\end{array}%
\right] \right)  \\
&=&\mathbf{A}\left[ 
\begin{array}{cc}
\mathbf{G} & 0 \\ 
0 & \mathbf{R}%
\end{array}%
\right] \mathbf{A}^{\dagger } \\
&=&\left[ 
\begin{array}{cc}
\mathbf{Z} & I%
\end{array}%
\right] \left[ 
\begin{array}{cc}
\mathbf{G} & 0 \\ 
0 & \mathbf{R}%
\end{array}%
\right] \left[ 
\begin{array}{c}
\mathbf{Z}^{\dagger } \\ 
I%
\end{array}%
\right]  \\
&=&\left[ 
\begin{array}{cc}
\mathbf{ZG} & \mathbf{R}%
\end{array}%
\right] \left[ 
\begin{array}{c}
\mathbf{Z}^{\dagger } \\ 
I%
\end{array}%
\right]  \\
&=&\mathbf{ZGZ}^{\dagger }+I\mathbf{R}
\end{eqnarray*}

\bigskip

Recall: $\mathbf{Y}=\mathbf{x\beta }+\mathbf{\varepsilon }$, $\mathbf{%
\varepsilon }\sim \mathcal{N}_{n}\left( 0,\tsum \right) $, $\tsum $ known, $%
\tsum =\tsum^{\frac{1}{2}}\left( \tsum^{\frac{1}{2}}\right) ^{\dagger }$%
\begin{equation*}
\Rightarrow \tsum\nolimits^{-\frac{1}{2}}\mathbf{Y}\sim \mathcal{N}%
_{n}\left( 0,\underset{I_{n}}{\underbrace{\tsum\nolimits^{-\frac{1}{2}}\tsum
\left( \tsum\nolimits^{-\frac{1}{2}}\right) ^{\dagger }}}\right)
\end{equation*}%
\newline
$\therefore $ the LSE of $\mathbf{\beta }$ is%
\begin{equation*}
\mathbf{\hat{\beta}}=\left( \mathbf{x}^{\dagger }\tsum\nolimits^{-1}\mathbf{x%
}\right) ^{-1}\mathbf{x}^{\dagger }\tsum\nolimits^{-1}\mathbf{Y\quad }\text{%
, the weighted LSE}
\end{equation*}

\bigskip

\begin{itemize}
\item $\mathbf{\hat{\beta}}=\left( \mathbf{x}^{\dagger }V^{-1}\mathbf{x}%
\right) ^{-1}\mathbf{x}^{\dagger }V^{-1}\mathbf{Y}$

\item Estimation of $\mathbf{\mu }$:%
\begin{equation*}
\mathbf{Y}\sim \mathcal{N}_{n}\left( \mathbf{x},V\right) \quad ,\quad 
\mathbf{\mu }=\mathcal{N}\left( 0,\mathbf{G}\right)
\end{equation*}%
\begin{eqnarray*}
Cov\left( \mathbf{Y,\mu }\right) &=&Cov\left( \mathbf{x\beta }+\mathbf{Z\mu }%
+\mathbf{\varepsilon ,\mu }\right) \\
&=&Cov\left( \mathbf{Z\mu ,\mu }\right) +Cov\left( \mathbf{\varepsilon ,\mu }%
\right) \\
&=&\mathbf{Z}Var\left( \mathbf{\mu }\right) +\mathbf{0} \\
&=&\mathbf{ZG}
\end{eqnarray*}%
\begin{equation*}
\Rightarrow \left[ 
\begin{array}{c}
\mathbf{Y} \\ 
\mathbf{\mu }%
\end{array}%
\right] \sim \mathcal{N}_{n+q}\left( \left[ 
\begin{array}{c}
\mathbf{x\beta } \\ 
\boldsymbol{0}%
\end{array}%
\right] ,\left[ 
\begin{array}{cc}
\mathbf{V} & \mathbf{ZG} \\ 
\mathbf{GZ}^{\dagger } & \mathbf{G}%
\end{array}%
\right] \right)
\end{equation*}
\end{itemize}

\bigskip

recall: 
\begin{equation*}
\left[ 
\begin{array}{c}
\mathbf{Y} \\ 
\mathbf{Z}%
\end{array}%
\right] \sim \mathcal{N}\left( \left[ 
\begin{array}{c}
\mathbf{\mu }_{\mathbf{Y}} \\ 
\mathbf{\mu }_{\mathbf{Z}}%
\end{array}%
\right] ,\left[ 
\begin{array}{cc}
\tsum\nolimits_{\mathbf{Y}} & \tsum\nolimits_{\mathbf{YZ}} \\ 
\tsum\nolimits_{\mathbf{ZY}} & \tsum\nolimits_{\mathbf{Z}}%
\end{array}%
\right] \right)
\end{equation*}%
\begin{equation*}
\Rightarrow \mathbf{Z}|\mathbf{Y}\sim \mathcal{N}\left( \mathbf{\mu }_{%
\mathbf{Z}|\mathbf{Y}},\tsum\nolimits_{\mathbf{Z}|\mathbf{Y}}\right)
\end{equation*}%
with%
\begin{eqnarray*}
\mathbf{\mu }_{\mathbf{Z}|\mathbf{Y}} &=&\mathbf{\mu }_{Z}+\tsum%
\nolimits_{ZY}\tsum\nolimits_{Y}^{-1}\left( \mathbf{Y}-\mathbf{\mu }%
_{Y}\right) \\
\tsum\nolimits_{\mathbf{Z}|\mathbf{Y}}
&=&\tsum\nolimits_{Z}-\tsum\nolimits_{ZY}\tsum\nolimits_{Y}^{-1}\tsum%
\nolimits_{YZ}
\end{eqnarray*}%
\begin{eqnarray*}
E\left( \mathbf{\mu }|\mathbf{Y}\right) &=&\mathbf{\mu }_{\mu
}+\tsum\nolimits_{\mathbf{\mu Y}}\tsum\nolimits_{Y}^{-1}\left( \mathbf{%
Y-x\beta }\right) \\
&=&0+\mathbf{GZ}^{\dagger }\mathbf{V}^{-1}\left( \mathbf{Y-x\beta }\right) \\
&=&\mathbf{GZ}^{\dagger }\mathbf{V}^{-1}\left( \mathbf{Y-x\beta }\right)
\end{eqnarray*}%
is the best linear unbiased predictor of $\mathbf{\mu }$ (BLUP).\newline
\newline
Thus, $\mathbf{\tilde{\mu}=GZ}^{\dagger }\mathbf{V}^{-1}\left( \mathbf{Y-x%
\hat{\beta}}\right) $ is the empirical BLUP.

\end{document}
