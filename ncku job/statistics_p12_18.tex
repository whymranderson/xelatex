
\documentclass{article}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{accents}
\usepackage[ignoreall,a4paper]{geometry}
\usepackage{fancyhdr}

\setcounter{MaxMatrixCols}{10}
%TCIDATA{OutputFilter=LATEX.DLL}
%TCIDATA{Version=5.00.0.2606}
%TCIDATA{<META NAME="SaveForMode" CONTENT="1">}
%TCIDATA{BibliographyScheme=Manual}
%TCIDATA{Created=Wednesday, November 25, 2015 15:33:37}
%TCIDATA{LastRevised=Wednesday, December 09, 2015 16:32:51}
%TCIDATA{<META NAME="GraphicsSave" CONTENT="32">}
%TCIDATA{<META NAME="DocumentShell" CONTENT="Standard LaTeX\Blank - Standard LaTeX Article">}
%TCIDATA{CSTFile=40 LaTeX article.cst}
%TCIDATA{ComputeDefs=
%$W=\left( 1-\sigma \right) I$
%}


\newtheorem{theorem}{Theorem}
\newtheorem{acknowledgement}[theorem]{Acknowledgement}
\newtheorem{algorithm}[theorem]{Algorithm}
\newtheorem{axiom}[theorem]{Axiom}
\newtheorem{case}[theorem]{Case}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{conclusion}[theorem]{Conclusion}
\newtheorem{condition}[theorem]{Condition}
\newtheorem{conjecture}[theorem]{Conjecture}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{criterion}[theorem]{Criterion}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}
\newtheorem{exercise}[theorem]{Exercise}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{notation}[theorem]{Notation}
\newtheorem{problem}[theorem]{Problem}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{solution}[theorem]{Solution}
\newtheorem{summary}[theorem]{Summary}
\newenvironment{proof}[1][Proof]{\noindent\textbf{#1.} }{\ \rule{0.5em}{0.5em}}
\input{../tcilatex}
\DeclareMathAccent{\wtilde}{\mathord}{largesymbols}{"65}
\pagestyle{fancy}
\fancyfoot[C]{\thepage}
\input{tcilatex}

\begin{document}


\begin{theorem}[Gauss-Markov-Aitken]
If $Y=x\beta +\varepsilon \quad 
%TCIMACRO{\TeXButton{underaccent_epsilon}{\underaccent{\wtilde}{\epsilon}}}%
%BeginExpansion
\underaccent{\wtilde}{\epsilon}%
%EndExpansion
\symbol{126}N\left( 0,\sigma ^{2}W\right) $, the Generalized least-square
estimator(GLSE) $\hat{\beta}=\left( x^{\dagger }W^{-1}x\right)
^{-1}x^{\dagger }W^{-1}Y$ is BLUE for $\beta ,$ $Var\left( \hat{\beta}%
\right) =\left( x^{\dagger }W^{-1}x\right) ^{-1}\sigma ^{2}$
\end{theorem}

\paragraph{Misspecification of the Dispersion matrix}

\bigskip

If W is matrix as A, then we have%
\begin{equation*}
\hat{\beta}^{\ast }=\left( x^{\dagger }A^{-1}x\right) ^{-1}x^{\dagger
}A^{-1}Y
\end{equation*}%
(missread W as A).

\bigskip

Note: $\hat{\beta}^{\ast }$ is unbiased, since $E\left( \hat{\beta}^{\ast
}\right) =\left( x^{\dagger }A^{-1}x\right) ^{-1}x^{\dagger }A^{-1}x\beta
=\beta $. Meaning missreading W does not affect bias.

\begin{equation*}
Cov\left( \hat{\beta}^{\ast }\right) =\left( x^{\dagger }A^{-1}x\right)
^{-1}x^{\dagger }A^{-1}Cov\left( Y\right) A^{-1}x\left( x^{\dagger
}A^{-1}x\right) ^{-1}
\end{equation*}%
\qquad

\bigskip

Since $Cov\left( \hat{\beta}^{\ast }\right) -Cov\left( \hat{\beta}\right)
=\left( x^{\dagger }A^{-1}x\right) ^{-1}x^{\dagger }A^{-1}Cov\left( Y\right)
A^{-1}x\left( x^{\dagger }A^{-1}x\right) ^{-1}-\left( x^{\dagger
}A^{-1}x\right) ^{-1}\sigma ^{2}$%
\begin{equation*}
\underset{\text{by A.41(ri)}}{=}\left\{ \sigma ^{2}\left( x^{\dagger
}A^{-1}x\right) ^{-1}x^{\dagger }A^{-1}-\left( x^{\dagger }W^{-1}x\right)
^{-1}x^{\dagger }W^{-1}\right\} \times
\end{equation*}%
\begin{equation*}
W\left\{ \left( x^{\dagger }A^{-1}x\right) ^{-1}x^{\dagger }A^{-1}-\left(
x^{\dagger }W^{-1}x\right) ^{-1}x^{\dagger }W^{-1}\right\} ^{\dagger }\geq 0
\end{equation*}

\bigskip

If the OLS(i.e. $\hat{\beta}_{0}=\left( x^{\dagger }x^{-1}\right) x^{\dagger
}Y$) is mistakenly instead of the true GLSE $\hat{\beta}_{1}$

\begin{itemize}
\item If take A=I $\hat{\beta}^{\ast }=\hat{\beta}_{OLS}$%
\begin{equation*}
\Rightarrow Cov\left( \hat{\beta}_{OLS}\right) -Cov\left( \hat{\beta}\right)
=\sigma ^{2}\left\{ \left( x^{\dagger }x\right) ^{-1}x^{\dagger }-\left(
x^{\dagger }W^{-1}x\right) ^{-1}x^{\dagger }W^{-1}\right\}
\end{equation*}%
\begin{equation*}
W\left\{ \left( x^{\dagger }x\right) ^{-1}x^{\dagger }-\left( x^{\dagger
}W^{-1}x\right) ^{-1}x^{\dagger }W^{-1}\right\} ^{\dagger }
\end{equation*}%
Let $\left( x^{\dagger }x\right) ^{-1}=U$%
\begin{equation*}
\Rightarrow Cov\left( \hat{\beta}_{OLS}\right) -Cov\left( \hat{\beta}\right)
=\sigma ^{2}\left\{ Ux^{\dagger }-\left( x^{\dagger }W^{-1}x\right)
^{-1}x^{\dagger }W^{-1}\right\} W\left\{ xU-W^{-1}x\left( x^{\dagger
}W^{-1}x\right) ^{-1}\right\}
\end{equation*}%
\begin{equation*}
\Rightarrow Cov\left( \hat{\beta}_{OLS}\right) -Cov\left( \hat{\beta}\right)
=0
\end{equation*}%
wrongly estimated but accuracy the same%
\begin{equation*}
\Leftrightarrow Ux^{\dagger }=\left( x^{\dagger }W^{-1}x\right)
^{-1}x^{\dagger }W^{-1}\ldots \left( \ast \right)
\end{equation*}%
\begin{equation*}
\Leftrightarrow Ux^{\dagger }W=\left( x^{\dagger }W^{-1}x\right)
^{-1}x^{\dagger }\ldots \left( \ast \ast \right)
\end{equation*}%
Let Z be a matrix of maximum rank s.t. $Z^{\dagger }x=0$
\end{itemize}

from (*) $\ Ux^{\dagger }WZ=\left( x^{\dagger }W^{-1}x\right)
^{-1}x^{\dagger }Z=0\Leftrightarrow \left( x^{\dagger }WZ\right) =0$

from (*) $\ Ux^{\dagger }Z=\left( x^{\dagger }W^{-1}x\right) ^{-1}x^{\dagger
}W^{-1}Z=0\Leftrightarrow \left( x^{\dagger }W^{-1}Z\right) =0$

\begin{itemize}
\item Note that Z$^{\dagger }x=0\Rightarrow $ x and Z span the whole n-space%
\begin{equation*}
\Rightarrow W_{nxn}^{\frac{1}{2}}=xA_{1}+ZB_{1}
\end{equation*}%
\begin{eqnarray*}
W &=&W^{\frac{1}{2}}W^{\frac{1}{2}}=\left( xA_{1}+ZB_{1}\right) \left(
xA_{1}+ZB_{1}\right) ^{\dagger } \\
&=&xA_{1}A_{1}^{\dagger }x^{\dagger }+xA_{1}B_{1}^{\dagger }Z^{\dagger
}+ZB_{1}A_{1}^{\dagger }x^{\dagger }+ZB_{1}B_{1}^{\dagger }Z^{\dagger
}\qquad \left( \triangle \right)
\end{eqnarray*}%
now $x^{\dagger }WZ=0\Rightarrow x^{\dagger }\left[ xA_{1}A_{1}^{\dagger
}x^{\dagger }+xA_{1}B_{1}^{\dagger }Z^{\dagger }+ZB_{1}A_{1}^{\dagger
}x^{\dagger }+ZB_{1}B_{1}^{\dagger }Z^{\dagger }\right] Z=0$%
\begin{equation*}
\because x^{\dagger }Z=0\Rightarrow \underset{\text{non-singular}}{%
\underbrace{x^{\dagger }x}}A_{1}B_{1}^{\dagger }\underset{\text{non-singular}%
}{\underbrace{Z^{\dagger }Z}}=0\Rightarrow A_{1}B_{1}^{\dagger }=0
\end{equation*}%
from ($\triangle $) and $A_{1}B_{1}^{\dagger }\Rightarrow
W=xA_{1}A_{1}^{\dagger }x^{\dagger }+ZB_{1}B_{1}^{\dagger }Z^{\dagger
}=xA_{1}x^{\dagger }+ZB_{1}Z^{\dagger }$, (A,B are nonsingular)
\end{itemize}

\bigskip

\begin{example}
$x=\left[ \boldsymbol{1},x_{1}\right] $ the the choice $W=\left( 1-\sigma
\right) I+\sigma \boldsymbol{11}^{\dagger }\quad \left( 0<\sigma <1\right) $%
\begin{equation*}
x^{\dagger }WZ=x^{\dagger }\left[ \left( 1-\sigma \right) I+\sigma 
\boldsymbol{11}^{\dagger }\right] Z=\left( 1-\sigma \right) \underset{=0}{%
\underbrace{x^{\dagger }Z}}+\sigma \underset{=0}{\underbrace{x^{\dagger }%
\boldsymbol{11}^{\dagger }Z}}=0
\end{equation*}%
\begin{equation*}
\Rightarrow \text{GLSE }\left( W=\left( 1-\sigma \right) I+\sigma 
\boldsymbol{11}^{\dagger }\right) \text{ and OLSE }(W=A=I)\text{ are equal}
\end{equation*}
\end{example}

\bigskip

$Y=x\beta +%
%TCIMACRO{\TeXButton{underaccent_epsilon}{\underaccent{\wtilde}{\epsilon}}}%
%BeginExpansion
\underaccent{\wtilde}{\epsilon}%
%EndExpansion
\quad Cov\left( 
%TCIMACRO{\TeXButton{underaccent_epsilon}{\underaccent{\wtilde}{\epsilon}}}%
%BeginExpansion
\underaccent{\wtilde}{\epsilon}%
%EndExpansion
\right) =\sigma ^{2}W\quad \frac{\left( Y-x\hat{\beta}\right) ^{\dagger
}W^{-1}\left( Y-x\hat{\beta}\right) }{n-k}\leadsto \sigma ^{2}$

Note: missread W makes $Var\left( \hat{\beta}_{i}\right) $ larger, then $%
\frac{\tilde{\beta}}{\sqrt{Var\left( \tilde{\beta}\right) }}$ becomes
smaller, ......??

Note:%
\begin{equation*}
\left( 
\begin{array}{ccc}
1 &  & \sigma \\ 
& \ddots &  \\ 
\sigma &  & 1%
\end{array}%
\right) x=\lambda x\quad x=\left[ 
\begin{array}{c}
1 \\ 
\vdots \\ 
1%
\end{array}%
\right]
\end{equation*}%
eigenvalue and =n$\Rightarrow \tsum \lambda _{i}=n$. $1+\left( n+1\right)
\sigma >0\Rightarrow \sigma >-\frac{1}{n-1}$ (n-1 multiple roots)

\bigskip

\paragraph{Estimating $\protect\sigma ^{2}$ if misspecifity W}

\bigskip

Assume that A is choosen instead of W%
\begin{equation*}
%TCIMACRO{%
%\TeXButton{underaccent_epsilon_hat}{\underaccent{\wtilde}{\hat{\epsilon}}}}%
%BeginExpansion
\underaccent{\wtilde}{\hat{\epsilon}}%
%EndExpansion
=%
%TCIMACRO{\TeXButton{underaccent_e}{\underaccent{\wtilde}{e}}}%
%BeginExpansion
\underaccent{\wtilde}{e}%
%EndExpansion
=Y-x\tilde{\beta}=\left[ I-x\left( x^{\dagger }A^{-1}x\right)
^{-1}x^{\dagger }A^{-1}\right] Y
\end{equation*}%
\begin{eqnarray*}
&=&\left[ I-x\left( x^{\dagger }A^{-1}x\right) ^{-1}x^{\dagger }A^{-1}\right]
\left( x\beta +\varepsilon \right) \\
&=&x\beta -x\left( x^{\dagger }A^{-1}x\right) ^{-1}x^{\dagger }A^{-1}x\beta
+\varepsilon -x\left( x^{\dagger }A^{-1}x\right) ^{-1}x^{\dagger
}A^{-1}\varepsilon \\
&=&\left[ I-x\left( x^{\dagger }A^{-1}x\right) ^{-1}x^{\dagger }A^{-1}\right]
%TCIMACRO{\TeXButton{underaccent_epsilon}{\underaccent{\wtilde}{\epsilon}}}%
%BeginExpansion
\underaccent{\wtilde}{\epsilon}%
%EndExpansion
\end{eqnarray*}%
\begin{equation*}
\because E\left[ \left( Y-x\hat{\beta}\right) ^{\dagger }W^{-1}\left( Y-x%
\hat{\beta}\right) \right] =\left( n-k\right) \sigma ^{2}\Rightarrow \frac{%
\left( Y-x\hat{\beta}\right) ^{\dagger }W^{-1}\left( Y-x\hat{\beta}\right) }{%
n-k}\leadsto \sigma ^{2}
\end{equation*}%
\begin{equation*}
\frac{e^{\dagger }A^{-1}e}{n-k}=\tilde{\sigma}^{2}
\end{equation*}%
\begin{equation*}
E\left( \tilde{\sigma}^{2}\right) =\sigma ^{2}+\frac{1}{n-k}\left\{
tr\left\{ \sigma ^{2}x\left( x^{\dagger }A^{-1}x\right) ^{-1}x^{\dagger
}A^{-1}\left( I-2W\right) \right\} +tr\left\{ xCov\left( \tilde{\beta}%
\right) x^{\dagger }\right\} \right\}
\end{equation*}

If A=I%
\begin{equation*}
E\left( \tilde{\sigma}^{2}\right) =\sigma ^{2}+\frac{\sigma ^{2}}{n-k}%
\left\{ k-tr\left\{ \left( x^{\dagger }x\right) ^{-1}x^{\dagger }Wx\right\}
\right\} <0\quad \text{meaning parameter underestimate}
\end{equation*}%
Even if paramater are obvious, the result is not obvious

\begin{equation*}
E\left( \tilde{\sigma}^{2}\right) =E\left( \frac{e^{\dagger }A^{-1}e}{n-k}%
\right) =\frac{1}{n-k}E\left( 
%TCIMACRO{\TeXButton{underaccent_e}{\underaccent{\wtilde}{e}}}%
%BeginExpansion
\underaccent{\wtilde}{e}%
%EndExpansion
^{\dagger }A^{-1}%
%TCIMACRO{\TeXButton{underaccent_e}{\underaccent{\wtilde}{e}}}%
%BeginExpansion
\underaccent{\wtilde}{e}%
%EndExpansion
\right)
\end{equation*}%
\begin{equation*}
%TCIMACRO{\TeXButton{underaccent_e}{\underaccent{\wtilde}{e}}}%
%BeginExpansion
\underaccent{\wtilde}{e}%
%EndExpansion
=\left[ I-x\left( x^{\dagger }A^{-1}x\right) ^{-1}x^{\dagger }A^{-1}\right] 
%TCIMACRO{\TeXButton{underaccent_epsilon}{\underaccent{\wtilde}{\epsilon}}}%
%BeginExpansion
\underaccent{\wtilde}{\epsilon}%
%EndExpansion
\end{equation*}

\bigskip

\newpage

\paragraph{Heteroccdasticity and Autoregression}

\bigskip

December 8 (monday)

\bigskip

\begin{itemize}
\item $E\left( 
%TCIMACRO{\TeXButton{underaccent_epsilon}{\underaccent{\wtilde}{\epsilon}}}%
%BeginExpansion
\underaccent{\wtilde}{\epsilon}%
%EndExpansion
\right) =%
%TCIMACRO{\TeXButton{underaccent_o}{\underaccent{\wtilde}{o}}}%
%BeginExpansion
\underaccent{\wtilde}{o}%
%EndExpansion
\quad Cov\left( 
%TCIMACRO{\TeXButton{underaccent_epsilon}{\underaccent{\wtilde}{\epsilon}}}%
%BeginExpansion
\underaccent{\wtilde}{\epsilon}%
%EndExpansion
%TCIMACRO{\TeXButton{underaccent_epsilon}{\underaccent{\wtilde}{\epsilon}}}%
%BeginExpansion
\underaccent{\wtilde}{\epsilon}%
%EndExpansion
^{\dagger }\right) =\sigma ^{2}W=\sigma ^{2}\left[ 
\begin{array}{ccc}
k_{1} &  & 0 \\ 
& \ddots &  \\ 
0 &  & k_{n}%
\end{array}%
\right] $%
\begin{equation*}
GLSE\quad \hat{\beta}=\left( x^{\dagger }W^{-1}x\right) ^{-1}x^{\dagger
}W^{-1}Y=\left( \tsum 
%TCIMACRO{\TeXButton{underaccent_x}{\underaccent{\wtilde}{x}}}%
%BeginExpansion
\underaccent{\wtilde}{x}%
%EndExpansion
_{i}%
%TCIMACRO{\TeXButton{underaccent_x}{\underaccent{\wtilde}{x}}}%
%BeginExpansion
\underaccent{\wtilde}{x}%
%EndExpansion
_{i}^{\dagger }\right) ^{-1}\left( \tsum 
%TCIMACRO{\TeXButton{underaccent_x}{\underaccent{\wtilde}{x}}}%
%BeginExpansion
\underaccent{\wtilde}{x}%
%EndExpansion
_{i}%
%TCIMACRO{\TeXButton{underaccent_x}{\underaccent{\wtilde}{x}}}%
%BeginExpansion
\underaccent{\wtilde}{x}%
%EndExpansion
_{i}^{\dagger }\right)
\end{equation*}%
x$^{\dagger }=\left( 
%TCIMACRO{\TeXButton{underaccent_x}{\underaccent{\wtilde}{x}}}%
%BeginExpansion
\underaccent{\wtilde}{x}%
%EndExpansion
_{1},\cdots ,%
%TCIMACRO{\TeXButton{underaccent_x}{\underaccent{\wtilde}{x}}}%
%BeginExpansion
\underaccent{\wtilde}{x}%
%EndExpansion
_{n}\right) $
\end{itemize}

\bigskip

example: $y_{i}=\alpha +\beta x_{i}+\varepsilon _{i}\quad Var\left(
\varepsilon _{i}\right) =\sigma ^{2}x_{i}^{2}\quad W=diag\left\{
x_{1}^{2},\cdots ,x_{n}^{2}\right\} $

\begin{itemize}
\item We don't have the original sample $%
%TCIMACRO{\TeXButton{underaccent_y}{\underaccent{\wtilde}{y}}}%
%BeginExpansion
\underaccent{\wtilde}{y}%
%EndExpansion
$ and x, but we have the sample mean $\bar{Y}_{i}=\frac{1}{n_{i}}%
\tsum\limits_{j=1}^{n}y_{j}\quad \bar{x}_{ik}=\frac{1}{n_{i}}%
\tsum\limits_{j=1}^{n_{i}}x_{jk}$%
\begin{equation*}
\bar{Y}_{i}=\beta _{0}+\beta _{1}\bar{x}+\varepsilon _{i}\quad Var\left( 
\bar{Y}_{i}\right) =\frac{\sigma ^{2}}{n_{i}}
\end{equation*}
\end{itemize}

\bigskip

More general situation%
\begin{equation*}
\begin{array}{ccc}
\text{Y}_{i}\text{ continuous:} & \text{normal regression} &  \\ 
Y_{i}\text{ discrete:} & \text{poission regression} & \ast \\ 
& \text{logistic regression} & \ast%
\end{array}%
\end{equation*}%
* $link\left( 
%TCIMACRO{\TeXButton{underaccent_mu}{\underaccent{\wtilde}{\mu}}}%
%BeginExpansion
\underaccent{\wtilde}{\mu}%
%EndExpansion
\right) =x\beta $ is a ?? function

\bigskip

\begin{itemize}
\item The disturbance of followers the so-called process of intral class
correllation $\varepsilon _{j}=V_{j}+u_{tj}$, t=1\symbol{126}m, j=1\symbol{%
126}n
\end{itemize}

\bigskip

Note: The disturbance V$_{j}$ are identical for the m realization modeling $%
\varepsilon _{i}\quad E\left( V_{j}\right) =\sigma _{v}^{2}$, j=1\symbol{126}%
n, $Cov\left( V_{j},V_{j^{\prime }}\right) =0$, $j\neq j^{\prime }$ of each
of the n individ%
\begin{equation*}
E\left( u_{tj}\right) =0\quad Var\left( u_{tj}\right) =\sigma _{u}^{2}\quad
Cov\left( u_{tj},u_{tj^{\prime }}\right) =0\quad \left( t,j\right) \neq
\left( t,j^{\prime }\right) 
\end{equation*}%
\begin{equation*}
%TCIMACRO{\TeXButton{underaccent_epsilon}{\underaccent{\wtilde}{\epsilon}}}%
%BeginExpansion
\underaccent{\wtilde}{\epsilon}%
%EndExpansion
_{mn\times 1}=\left[ 
\begin{array}{c}
%TCIMACRO{\TeXButton{underaccent_epsilon}{\underaccent{\wtilde}{\epsilon}}}%
%BeginExpansion
\underaccent{\wtilde}{\epsilon}%
%EndExpansion
_{1} \\ 
\vdots  \\ 
%TCIMACRO{\TeXButton{underaccent_epsilon}{\underaccent{\wtilde}{\epsilon}}}%
%BeginExpansion
\underaccent{\wtilde}{\epsilon}%
%EndExpansion
_{n}%
\end{array}%
\right] 
\end{equation*}%
\begin{equation*}
E\left( 
%TCIMACRO{\TeXButton{underaccent_epsilon}{\underaccent{\wtilde}{\epsilon}}}%
%BeginExpansion
\underaccent{\wtilde}{\epsilon}%
%EndExpansion
%TCIMACRO{\TeXButton{underaccent_epsilon}{\underaccent{\wtilde}{\epsilon}}}%
%BeginExpansion
\underaccent{\wtilde}{\epsilon}%
%EndExpansion
^{\dagger }\right) =diag\left( \Phi ,\cdots ,\Phi \right) =\left[ 
\begin{array}{ccc}
\Phi  &  &  \\ 
& \ddots  &  \\ 
&  & \Phi 
\end{array}%
\right] 
\end{equation*}%
\begin{equation*}
\Phi _{mxm}=E\left( 
%TCIMACRO{\TeXButton{underaccent_epsilon}{\underaccent{\wtilde}{\epsilon}}}%
%BeginExpansion
\underaccent{\wtilde}{\epsilon}%
%EndExpansion
_{j}%
%TCIMACRO{\TeXButton{underaccent_epsilon}{\underaccent{\wtilde}{\epsilon}}}%
%BeginExpansion
\underaccent{\wtilde}{\epsilon}%
%EndExpansion
_{j}^{\dagger }\right) =\left\{ E\left( 
%TCIMACRO{\TeXButton{underaccent_epsilon}{\underaccent{\wtilde}{\epsilon}}}%
%BeginExpansion
\underaccent{\wtilde}{\epsilon}%
%EndExpansion
_{tj}%
%TCIMACRO{\TeXButton{underaccent_epsilon}{\underaccent{\wtilde}{\epsilon}}}%
%BeginExpansion
\underaccent{\wtilde}{\epsilon}%
%EndExpansion
_{t^{\prime }j}^{\dagger }\right) \right\} 
\end{equation*}%
\begin{equation*}
=\left\{ E\left( V_{j}+u_{tj}\right) \left( V_{j}+u_{t^{\prime }j}\right)
^{\dagger }\right\} =\left\{ E\left( V_{j}^{2}\right) \right\} 
\end{equation*}%
\begin{equation*}
=\left\{ 
\begin{array}{c}
\sigma _{v}^{2},\quad t\neq t^{\prime } \\ 
E\left( V_{j}^{2}\right) +E\left( u_{tj}^{2}\right) =\sigma _{v}^{2}+\sigma
_{u}^{2},\quad t\neq t^{\prime }%
\end{array}%
\right. 
\end{equation*}%
\begin{equation*}
=\left[ 
\begin{array}{ccc}
\sigma _{v}^{2}+\sigma _{u}^{2} &  & \sigma _{v}^{2} \\ 
& \ddots  &  \\ 
\sigma _{v}^{2} &  & \sigma _{v}^{2}+\sigma _{u}^{2}%
\end{array}%
\right] =\sigma ^{2}\left[ 
\begin{array}{ccc}
1 &  & \rho  \\ 
& \ddots  &  \\ 
\rho  &  & 1%
\end{array}%
\right] 
\end{equation*}%
(equal correlation)%
\begin{equation*}
\sigma ^{2}=\sigma _{v}^{2}+\sigma _{u}^{2}\quad \rho =\frac{\sigma _{v}^{2}%
}{\sigma _{v}^{2}+\sigma _{u}^{2}}
\end{equation*}

\bigskip 

\paragraph{Autoregressive Disturbance}

\quad 

\bigskip 

Assume $\left\{ u_{t}\right\} \left( t=\cdots ,-2,-1,0,1,2,\cdots \right) $
be an random process $E\left( u_{t}\right) =0\quad E\left( u_{t}^{2}\right)
=\sigma _{u}^{2}\quad Cov\left( u_{t},u_{t^{\prime }}\right) =0\quad \forall
t\neq t^{\prime }$%
\begin{equation*}
V_{t}-\mu =\rho \left( V_{t-1}-\mu \right) \quad \left\vert \rho \right\vert
<1\quad \left( \mu =0,\quad V_{t}=V_{t-1}+u_{t}\right) 
\end{equation*}%
repeated subsitute%
\begin{eqnarray*}
V_{t}-\mu  &=&\rho \left( V_{t-1}-\mu \right) +u_{t} \\
&=&\rho \left( \rho \left( V_{t-2}-\mu \right) +u_{t-1}\right) +u_{t} \\
&=&\cdots =\tsum\limits_{s=0}^{\infty }\rho ^{s}u_{t-s}
\end{eqnarray*}%
\begin{equation*}
E\left( V_{t}\right) =\mu +E\left( \tsum\limits_{s=0}^{\infty }\rho
^{s}u_{t-s}\right) =\mu +\tsum\limits_{s=0}^{\infty }\rho ^{s}E\left(
u_{t-s}\right) =\mu 
\end{equation*}%
\begin{equation*}
E\left( V_{t}-\mu \right) ^{2}=E\left( \tsum\limits_{s=0}^{\infty }\rho
^{s}u_{t-s}\right) ^{2}=\tsum\limits_{s=0}^{\infty
}\tsum\limits_{r=0}^{\infty }\rho ^{s+r}E\left( u_{t-s}u_{t-r}\right) 
\end{equation*}%
\begin{eqnarray*}
&=&\tsum\limits_{s=0}^{\infty }\rho ^{2s}E\left( u_{t-s}^{2}\right)  \\
&=&\sigma _{u}^{2}\tsum\limits_{s=0}^{\infty }\rho ^{2s} \\
&=&\sigma _{u}^{2}\frac{1}{1-\rho ^{2}}=\sigma ^{2}
\end{eqnarray*}%
\begin{equation*}
E\left[ \left( V_{t}-\mu \right) \left( V_{t-k}-\mu \right) \right] =E\left[
\left( \tsum\limits_{s=0}^{\infty }\rho ^{s}E\left( u_{t-s}\right) \right)
\left( \tsum\limits_{r=0}^{\infty }\rho ^{r}E\left( u_{t-k-r}\right) \right) %
\right] 
\end{equation*}%
\begin{equation*}
\begin{array}{c}
\text{if }k+r=S\text{ then }E\left[ u_{t-s}u_{t-k-r}\right] =E\left(
u_{t-s}^{2}\right)  \\ 
\text{if }k+r\neq S\text{ then }E\left[ u_{t-s}u_{t-k-r}\right] =0%
\end{array}%
\end{equation*}%
\begin{eqnarray*}
&\therefore &E\left[ \left( V_{t}-\mu \right) \left( V_{t-k}-\mu \right) %
\right] =\left( \tsum\limits_{r=0}^{\infty }\rho ^{2r+k}E\left(
u_{t-k-r}^{2}\right) \right)  \\
&=&\sigma _{u}^{2}\tsum\limits_{r=0}^{\infty }\rho ^{2r+k} \\
&=&\sigma _{u}^{2}\frac{\rho ^{k}}{1-\rho ^{2}}=\sigma ^{2}\rho ^{k}
\end{eqnarray*}%
\begin{equation*}
E\left[ \left( V_{t}-\mu \right) ^{2}\right] =\sigma ^{2}=Var\left(
V_{t-k}\right) 
\end{equation*}%
\begin{equation*}
Cov\left( 
%TCIMACRO{\TeXButton{underaccent_V}{\underaccent{\wtilde}{V}}}%
%BeginExpansion
\underaccent{\wtilde}{V}%
%EndExpansion
\right) =\sigma ^{2}\left[ 
\begin{array}{ccccc}
1 & \sigma  & \sigma ^{2} & \cdots  & \sigma ^{n-1} \\ 
& 1 & \sigma  &  &  \\ 
&  & \ddots  & \sigma  & \vdots  \\ 
&  &  & 1 & \sigma  \\ 
&  &  &  & 1%
\end{array}%
\right] =\sigma ^{2}W
\end{equation*}%
\begin{equation*}
W^{-1}=\frac{1}{1-\rho ^{2}}\left[ 
\begin{array}{ccccc}
1 & -\rho  &  &  &  \\ 
& 1+\rho ^{2} & -\rho  &  &  \\ 
&  & \ddots  & -\rho  &  \\ 
&  &  & 1+\rho ^{2} & -\rho  \\ 
&  &  &  & 1%
\end{array}%
\right] 
\end{equation*}

If $V_{t}-\mu =\rho \left( V_{t-1}-\mu \right) +\mu _{t}$ how to examine the
existence of $\rho $, how to estimate?

\bigskip 

\paragraph{Testing Autoregression}

\begin{equation*}
H_{0}=\rho =0
\end{equation*}

\end{document}
